{"./":{"url":"./","title":"Introduction","keywords":"","body":" 好大的板栗， 🌰 好小的板栗，(●'◡'●) 文档主要记录 numpy pandas scipy 的常用方法。 Update time： 2020-06-12 "},"Chapter1/":{"url":"Chapter1/","title":"Numpy","keywords":"","body":"Numpy 主要参考： NumPy参考 Update time： 2020-05-25 "},"Chapter1/Numpy 创建数组.html":{"url":"Chapter1/Numpy 创建数组.html","title":"Numpy 创建数组","keywords":"","body":"Numpy 创建数组 numpy.empty numpy.empty方法用来创建一个指定 形状（shape）、数据类型（dtype）且未初始化的数组： numpy.empty(shape, dtype = float, order = 'C') hape：int或tuple 空数组的形状 dtype：数据类型，可选 所需的输出数据类型。 order：{'C'，'F'}，可选 是否在存储器中以行主（C风格）或列主（Fortran风格）顺序存储多维数据。 返回 给定形状，dtype和顺序的未初始化（任意）数据的数组。对象数组将初始化为无。 #创建三行两列的数组 import numpy as np x = np.empty([3,2], dtype = int) print (x) # 注意 − 数组元素为随机值，因为它们未初始化。 #[[ 6917529027641081856 5764616291768666155] # [ 6917529027641081859 -5764598754299804209] # [ 4497473538 844429428932120]] numpy.zeros 创建指定大小的数组，数组元素以 0 来填充： numpy.zeros(shape, dtype = float, order = 'C') shape：int或ints序列 新数组的形状，例如（2， 3）或2。 dtype：数据类型，可选 数组的所需数据类型，例如numpy.int8。默认值为numpy.float64。 order：{'C'，'F'}，可选 是否在存储器中以C或Fortran连续（按行或列方式）存储多维数据。 import numpy as np # 默认为浮点数 x = np.zeros(5) print(x) # [0. 0. 0. 0. 0.] # 设置类型为整数 y = np.zeros((5,), dtype = np.int) print(y) # [0 0 0 0 0] # 自定义类型 z = np.zeros((2,2), dtype = [('x', 'i4'), ('y', 'i4')]) print(z) #[[(0, 0) (0, 0)] # [(0, 0) (0, 0)]] numpy.ones 创建指定形状的数组，数组元素以 1 来填充： numpy.ones(shape, dtype = None, order = 'C') shape：int或ints序列 新数组的形状，例如（2， 3）或2。 dtype：数据类型，可选 数组的所需数据类型，例如numpy.int8。默认值为numpy.float64。 order：{'C'，'F'}，可选 是否在存储器中以C或Fortran连续（按行或列方式）存储多维数据。 import numpy as np # 默认为浮点数 x = np.ones(5) print(x) # [1. 1. 1. 1. 1.] # 自定义类型 x = np.ones([2,2], dtype = int) print(x) #[[1 1] # [1 1]] Numerical ranges numpy.arange() numpy.arange([start,]stop,[step,]dtype=None) 在给定间隔内返回均匀间隔的值。 在半开区间[开始， 停止）（换句话说，包括开始 t3 >但不包括\\停止*）。*对于整数参数，该函数等效于Python内置的range函数，但返回一个ndarray而不是一个列表。 >>> np.arange(3) array([0, 1, 2]) >>> np.arange(3.0) array([ 0., 1., 2.]) >>> np.arange(3,7) array([3, 4, 5, 6]) >>> np.arange(3,7,2) array([3, 5]) numpy.linspace() numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None) 在指定的间隔内返回均匀间隔的数字。 返回num均匀分布的样本，在 [start, stop]。 这个区间的端点可以任意的被排除在外。 参数： start : scalar(标量) 序列的起始值 stop : scala 序列的结束点，除非endpoint被设置为False，在这种情况下, the sequence consists of all but the last of num + 1 evenly spaced samples(该序列包括所有除了最后的num+1上均匀分布的样本(感觉这样翻译有点坑)), 以致于stop被排除.当endpoint is False的时候注意步长的大小(下面有例子). num : int, optional(可选) 生成的样本数，默认是50。必须是非负。 endpoint : bool, optional 如果是真，则一定包括stop，如果为False，一定不会有stop retstep : bool, optional 如果为真，返回（样本，步骤），其中步长是样本之间的间距。 dtype : dtype, optional 返回： samples：ndarray step ：float 仅在retstep为True时返回 样本之间的间距大小。 >>> np.linspace(2.0, 3.0, num=5) array([ 2. , 2.25, 2.5 , 2.75, 3. ]) >>> np.linspace(2.0, 3.0, num=5, endpoint=False) array([ 2. , 2.2, 2.4, 2.6, 2.8]) >>> np.linspace(2.0, 3.0, num=5, retstep=True) (array([ 2. , 2.25, 2.5 , 2.75, 3. ]), 0.25) Building matrices numpy.diag() 提取对角线或构造对角数组。 numpy.diag(v,k=0) v 是一个1维数组时，结果形成一个以一维数组为对角线元素的矩阵 v 是一个二维矩阵时，结果输出矩阵的对角线元素 k : int, optional 对角线的位置，大于零位于对角线上面，小于零则在下面。 >>> x = np.arange(9).reshape((3,3)) >>> x array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) >>> np.diag(x) array([0, 4, 8]) >>> np.diag(x, k=1) array([1, 5]) >>> np.diag(x, k=-1) array([3, 7]) >>> np.diag(np.diag(x)) array([[0, 0, 0], [0, 4, 0], [0, 0, 8]]) Update time： 2020-05-25 "},"Chapter1/Numpy 数组遍历.html":{"url":"Chapter1/Numpy 数组遍历.html","title":"Numpy 数组遍历","keywords":"","body":"Numpy 数组遍历 NumPy 迭代器对象 numpy.nditer提供了一种灵活访问一个或者多个数组元素的方式。 迭代器最基本的任务的可以完成对数组元素的访问。 import numpy as np a = np.arange(6).reshape(2,3) print ('原始数组是：') print (a) print ('\\n') print ('迭代输出元素：') for x in np.nditer(a): print (x, end=\", \" ) print ('\\n') 原始数组是： [[0 1 2] [3 4 5]] 迭代输出元素： 0, 1, 2, 3, 4, 5, 以上实例不是使用标准 C 或者 Fortran 顺序，选择的顺序是和数组内存布局一致的，这样做是为了提升访问的效率，默认是行序优先（row-major order，或者说是 C-order）。 控制遍历顺序 for x in np.nditer(a, order='F'):Fortran order，即是列序优先； for x in np.nditer(a.T, order='C'):C order，即是行序优先； import numpy as np a = np.arange(0,60,5) a = a.reshape(3,4) print ('原始数组是：') print (a) 原始数组是： [[ 0 5 10 15] [20 25 30 35] [40 45 50 55]] print ('\\n') print ('原始数组的转置是：') b = a.T print (b) 原始数组的转置是： [[ 0 20 40] [ 5 25 45] [10 30 50] [15 35 55]] print ('\\n') print ('以 C 风格顺序排序：') c = b.copy(order='C') print (c) for x in np.nditer(c): print (x, end=\", \" ) 以 C 风格顺序排序： [[ 0 20 40] [ 5 25 45] [10 30 50] [15 35 55]] 0, 20, 40, 5, 25, 45, 10, 30, 50, 15, 35, 55, print ('\\n') print ('以 F 风格顺序排序：') c = b.copy(order='F') print (c) for x in np.nditer(c): print (x, end=\", \" ) 以 F 风格顺序排序： [[ 0 20 40] [ 5 25 45] [10 30 50] [15 35 55]] 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 可以通过显式设置，来强制 nditer 对象使用某种顺序： import numpy as np a = np.arange(0,60,5) a = a.reshape(3,4) print ('原始数组是：') print (a) print ('\\n') print ('以 C 风格顺序排序：') for x in np.nditer(a, order = 'C'): print (x, end=\", \" ) print ('\\n') print ('以 F 风格顺序排序：') for x in np.nditer(a, order = 'F'): print (x, end=\", \" ) Update time： 2020-05-25 "},"Chapter1/Numpy 数组操作.html":{"url":"Chapter1/Numpy 数组操作.html","title":"Numpy 数组操作","keywords":"","body":"Numpy 数组操作 连接数组 np.c_和 np.r_ np.r_是按列连接两个矩阵，就 是把两矩阵上下相加，要求列数相等，类似于pandas中的concat()。 np.c_是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等，类似于pandas中的merge()。 import numpy as np a = np.array([1, 2, 3]) b = np.array([4, 5, 6]) np.c_[a,b] array([[1, 4], [2, 5], [3, 6]]) np.r_[a,b] array([1, 2, 3, 4, 5, 6]) numpy.concatenate numpy.concatenate 函数用于沿指定轴连接相同形状的两个或多个数组，格式如下： numpy.concatenate((a1, a2, ...), axis) 数说明： a1, a2, …：相同类型的数组 axis：沿着它连接数组的轴，默认为 0 import numpy as np a = np.array([[1,2],[3,4]]) print ('第一个数组：') print (a) print ('\\n') b = np.array([[5,6],[7,8]]) print ('第二个数组：') print (b) print ('\\n') # 两个数组的维度相同 print ('沿轴 0 连接两个数组：') print (np.concatenate((a,b))) print ('\\n') print ('沿轴 1 连接两个数组：') print (np.concatenate((a,b),axis = 1)) 第一个数组： [[1 2] [3 4]] 第二个数组： [[5 6] [7 8]] 沿轴 0 连接两个数组： [[1 2] [3 4] [5 6] [7 8]] 沿轴 1 连接两个数组： [[1 2 5 6] [3 4 7 8]] numpy.stack numpy.stack函数用于沿新轴连接数组序列，格式如下： numpy.stack(arrays, axis) 参数说明： arrays相同形状的数组序列 axis：返回数组中的轴，输入数组沿着它来堆叠 import numpy as np a = np.array([[1,2],[3,4]]) print ('第一个数组：') print (a) print ('\\n') b = np.array([[5,6],[7,8]]) print ('第二个数组：') print (b) print ('\\n') print ('沿轴 0 堆叠两个数组：') print (np.stack((a,b),0)) print ('\\n') print ('沿轴 1 堆叠两个数组：') print (np.stack((a,b),1)) 第一个数组： [[1 2] [3 4]] 第二个数组： [[5 6] [7 8]] 沿轴 0 堆叠两个数组： [[[1 2] [3 4]] [[5 6] [7 8]]] 沿轴 1 堆叠两个数组： [[[1 2] [5 6]] [[3 4] [7 8]]] numpy.hstack numpy.hstack 是 numpy.stack 函数的变体，它通过水平堆叠来生成数组。 import numpy as np a = np.array([[1,2],[3,4]]) print ('第一个数组：') print (a) print ('\\n') b = np.array([[5,6],[7,8]]) print ('第二个数组：') print (b) print ('\\n') print ('水平堆叠：') c = np.hstack((a,b)) print (c) print ('\\n') 第一个数组： [[1 2] [3 4]] 第二个数组： [[5 6] [7 8]] 水平堆叠： [[1 2 5 6] [3 4 7 8]] numpy.vstack numpy.vstack是numpy.stack函数的变体，它通过垂直堆叠来生成数组。 import numpy as np a = np.array([[1,2],[3,4]]) print ('第一个数组：') print (a) print ('\\n') b = np.array([[5,6],[7,8]]) print ('第二个数组：') print (b) print ('\\n') print ('竖直堆叠：') c = np.vstack((a,b)) print (c) 第一个数组： [[1 2] [3 4]] 第二个数组： [[5 6] [7 8]] 竖直堆叠： [[1 2] [3 4] [5 6] [7 8]] 分割数组 numpy.split numpy.split函数沿特定的轴将数组分割为子数组，格式如下： numpy.split(ary, indices_or_sections, axis) 数说明： ary：被分割的数组 indices_or_sections：如果是一个整数，就用该数平均切分，如果是一个数组，为沿轴切分的位置（左开右闭） axis：沿着哪个维度进行切向，默认为0，横向切分。为1时，纵向切分 import numpy as np a = np.arange(9) print ('第一个数组：') print (a) print ('\\n') print ('将数组分为三个大小相等的子数组：') b = np.split(a,3) print (b) print ('\\n') print ('将数组在一维数组中表明的位置分割：') b = np.split(a,[4,7]) print (b) 第一个数组： [0 1 2 3 4 5 6 7 8] 将数组分为三个大小相等的子数组： [array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8])] 将数组在一维数组中表明的位置分割： [array([0, 1, 2, 3]), array([4, 5, 6]), array([7, 8])] numpy.hsplit numpy.hsplit 函数用于水平分割数组，通过指定要返回的相同形状的数组数量来拆分原数组。 import numpy as np harr = np.floor(10 * np.random.random((2, 6))) print ('原array：') print(harr) print ('拆分后：') print(np.hsplit(harr, 3)) 原array： [[4. 7. 6. 3. 2. 6.] [6. 3. 6. 7. 9. 7.]] 拆分后： [array([[4., 7.], [6., 3.]]), array([[6., 3.], [6., 7.]]), array([[2., 6.], [9., 7.]])] numpy.vsplit numpy.vsplit沿着垂直轴分割，其分割方式与hsplit用法相同。 import numpy as np a = np.arange(16).reshape(4,4) print ('第一个数组：') print (a) print ('\\n') print ('竖直分割：') b = np.vsplit(a,2) print (b) 第一个数组： [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]] 竖直分割： [array([[0, 1, 2, 3], [4, 5, 6, 7]]), array([[ 8, 9, 10, 11], [12, 13, 14, 15]])] 数组元素的添加 numpy.resize numpy.resize 函数返回指定大小的新数组。 如果新数组大小大于原始大小，则包含原始数组中的元素的副本。 numpy.resize(arr, shape) 参数说明： arr：要修改大小的数组 shape：返回数组的新形状 import numpy as np a = np.array([[1,2,3],[4,5,6]]) print ('第一个数组：') print (a) 第一个数组： [[1 2 3] [4 5 6]] print ('\\n') print ('第一个数组的形状：') print (a.shape) 第一个数组的形状： (2, 3) print ('\\n') b = np.resize(a, (3,2)) print ('第二个数组：') print (b) 第二个数组： [[1 2] [3 4] [5 6]] print ('\\n') print ('第二个数组的形状：') print (b.shape) 第二个数组的形状： (3, 2) print ('\\n') # 要注意 a 的第一行在 b 中重复出现，因为尺寸变大了 print ('修改第二个数组的大小：') b = np.resize(a,(3,3)) print (b) 修改第二个数组的大小： [[1 2 3] [4 5 6] [1 2 3]] numpy.append numpy.append函数在数组的末尾添加值。 追加操作会分配整个数组，并把原来的数组复制到新数组中。 此外，输入数组的维度必须匹配否则将生成ValueError。 append 函数返回的始终是一个一维数组。 numpy.append(arr, values, axis=None) 参数说明： arr：输入数组 values：要向arr添加的值，需要和arr形状相同（除了要添加的轴） axis：默认为 None。当axis无定义时，是横向加成，返回总是为一维数组！当axis有定义的时候，分别为0和1的时候。当axis有定义的时候，分别为0和1的时候（列数要相同）。当axis为1时，数组是加在右边（行数要相同）。 numpy.insert numpy.insert函数在给定索引之前，沿给定轴在输入数组中插入值。 如果值的类型转换为要插入，则它与输入数组不同。 插入没有原地的，函数会返回一个新数组。 此外，如果未提供轴，则输入数组会被展开。 numpy.insert(arr, obj, values, axis) 参数说明： arr：输入数组 obj：在其之前插入值的索引 values：要插入的值 axis：沿着它插入的轴，如果未提供，则输入数组会被展开 import numpy as np a = np.array([[1,2],[3,4],[5,6]]) print ('第一个数组：') print (a) 第一个数组： [[1 2] [3 4] [5 6]] print ('\\n') print ('未传递 Axis 参数。 在插入之前输入数组会被展开。') print (np.insert(a,3,[11,12])) 未传递 Axis 参数。 在插入之前输入数组会被展开。 [ 1 2 3 11 12 4 5 6] print ('\\n') print ('传递了 Axis 参数。 会广播值数组来配输入数组。') print ('沿轴 0 广播：') print (np.insert(a,1,[11],axis = 0)) 传递了 Axis 参数。 会广播值数组来配输入数组。 沿轴 0 广播： [[ 1 2] [11 11] [ 3 4] [ 5 6]] print ('\\n') print ('沿轴 1 广播：') print (np.insert(a,1,11,axis = 1)) 沿轴 1 广播： [[ 1 11 2] [ 3 11 4] [ 5 11 6]] numpy.unique numpy.unique函数用于去除数组中的重复元素。 numpy.unique 函数用于去除数组中的重复元素。 numpy.unique(arr, return_index, return_inverse, return_counts) arr：输入数组，如果不是一维数组则会展开 return_index：如果为true，返回新列表元素在旧列表中的位置（下标），并以列表形式储 return_inverse：如果为true，返回旧列表元素在新列表中的位置（下标），并以列表形式储 return_counts：如果为true，返回去重数组中的元素在原数组中的出现次数 import numpy as np a = np.array([5,2,6,2,7,5,6,8,2,9]) print ('第一个数组：') print (a) 第一个数组： [5 2 6 2 7 5 6 8 2 9] print ('\\n') print ('第一个数组的去重值：') u = np.unique(a) print (u) 第一个数组的去重值： [2 5 6 7 8 9] print ('\\n') print ('去重数组的索引数组：') # 返回值为两个 u,indices = np.unique(a, return_index = True) print (indices) 去重数组在原数组中的索引，构成的数组： [1 0 2 4 7 9] print ('\\n') print ('我们可以看到每个和原数组下标对应的数值：') print (a) 我们可以看到每个和原数组下标对应的数值： [5 2 6 2 7 5 6 8 2 9] print ('\\n') print ('去重数组的下标：') # 返回值为两个 indices 旧数组中的元素在去重后的数组中的索引，构成的数组 u,indices = np.unique(a,return_inverse = True) print (u) 去重数组的下标： [2 5 6 7 8 9] print ('\\n') print ('下标为：') print (indices) 下标为： [1 0 2 0 3 1 2 4 0 5] print ('\\n') print ('使用下标重构原数组：') print (u[indices]) 使用下标重构原数组： [5 2 6 2 7 5 6 8 2 9] print ('\\n') print ('返回去重元素的重复数量：') #u去重后的数组，indices 去重后的数组中的元素在原来数组中出现的次数 u,indices = np.unique(a,return_counts = True) print (u) print (indices) 返回去重元素的重复数量： [2 5 6 7 8 9] [3 2 2 1 1 1] numpy.delete numpy.delete 函数返回从输入数组中删除指定子数组的新数组。 与 insert() 函数的情况一样，如果未提供轴参数，则输入数组将展开。 Numpy.delete(arr, obj, axis) 参数说明： arr：输入数组 obj：可以被切片，整数或者整数数组，表明要从输入数组删除的子数组 axis：沿着它删除给定子数组的轴，如果未提供，则输入数组会被展开 import numpy as np a = np.arange(12).reshape(3,4) print ('第一个数组：') print (a) 第一个数组： [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] print ('\\n') print ('未传递 Axis 参数。 在插入之前输入数组会被展开。') print (np.delete(a,5)) 未传递 Axis 参数。 在插入之前输入数组会被展开。 [ 0 1 2 3 4 6 7 8 9 10 11] print ('\\n') print ('删除第二列：') print (np.delete(a,1,axis = 1)) 未传递 Axis 参数。 在插入之前输入数组会被展开。 [ 0 1 2 3 4 6 7 8 9 10 11] print ('\\n') print ('包含从数组中删除的替代值的切片：') a = np.array([1,2,3,4,5,6,7,8,9,10]) print (np.delete(a, np.s_[::2])) 包含从数组中删除的替代值的切片： [ 2 4 6 8 10] Update time： 2020-05-25 "},"Chapter1/axis=0 axis=1的理解.html":{"url":"Chapter1/axis=0 axis=1的理解.html","title":"axis=0 axis=1的理解","keywords":"","body":"axis=0 axis=1的理解 轴用来为超过一维的数组定义的属性，二维数据拥有两个轴：第 0 轴沿着行的垂直往下，第 1 轴沿着列的方向水平延伸。 官方对于 0 和 1 的解释是轴，也就是坐标轴。而坐标轴是有方向的，所以千万不要用行和列的思维去想 axis，因为行和列是没有方向的，这样想会在遇到不同的例子时感到困惑。 根据官方的说法，0 表示纵轴，方向从上到下；1 表示横轴，方向从左到右。当 axis = 1 时，数组的变化是横向的，而体现出来的是列的增加或者减少。 其实 axis 的重点在于方向，而不是行和列。具体到各种用法而言也是如此。当 axis = 1 时，如果是求平均，那么是从左到右横向求平均；如果是拼接，那么也是左右横向拼接；如果是drop，那么也是横向发生变化，体现为列的减少。 当考虑了方向，即 axis=0为纵向，axis=1为横向，而不是行和列，那么所有的例子就都统一了。 Update time： 2020-05-25 "},"Chapter1/NumPy 排序、条件刷选函数.html":{"url":"Chapter1/NumPy 排序、条件刷选函数.html","title":"NumPy 排序、条件刷选函数","keywords":"","body":"NumPy 排序、条件刷选函数 NumPy 提供了多种排序的方法。 这些排序函数实现不同的排序算法，每个排序算法的特征在于执行速度，最坏情况性能，所需的工作空间和算法的稳定性。 下表显示了三种排序算法的比较。 numpy.sort() numpy.sort() 函数返回输入数组的排序副本。函数格式如下： 参数说明： a: 要排序的数组 axis: 沿着它排序数组的轴，如果没有数组会被展开，沿着最后的轴排序， axis=0 按列排序，axis=1 按行排序 kind: 默认为’quicksort’（快速排序） order: 如果数组包含字段，则是要排序的字段 import numpy as np a = np.array([[3, 7], [9, 1]]) print(a) # [[3 7] # [9 1]] print('调用 sort() 函数：') print(np.sort(a)) # [[3 7] # [1 9]] print('按列排序：') print(np.sort(a, axis=0)) # [[3 1] # [9 7]] # 在 sort 函数中排序字段 dt = np.dtype([('name', 'S10'), ('age', int)]) a = np.array([(\"raju\", 21), (\"anil\", 25), (\"ravi\", 17), (\"amar\", 27)], dtype=dt) print('数组是：') print(a) #[(b'raju', 21) (b'anil', 25) (b'ravi', 17) (b'amar', 27)] print('按 name 排序：') print(np.sort(a, order='name')) #[(b'amar', 27) (b'anil', 25) (b'raju', 21) (b'ravi', 17)] sorted() sorted(iterable[, cmp[, key[, reverse]]]) sorted() 函数对所有可迭代的对象进行排序操作。 sort 与 sorted 区别： sort 是应用在 list 上的方法，sorted 可以对所有可迭代的对象进行排序操作。 list 的 sort 方法返回的是对已经存在的列表进行操作，而内建函数 sorted 方法返回的是一个 新的 list，而不是在原来的基础上进行的操作。 # sorted()可以利用参数reverse=True进行反向排序 >>>list=[3,4,2,6,1] >>>sorted(list) [1, 2, 3, 4, 6] >>>sorted(list, reverse=True) [6, 4, 3, 2, 1] numpy.argsort() numpy.argsort() 函数返回的是数组值从小到大的索引值。 import numpy as np x = np.array([3, 1, 2]) print('数组是：') print(x) # [3 1 2] print('对 x 调用 argsort() 函数：') y = np.argsort(x) print(y) #[1 2 0] print('以排序后的顺序重构原数组：') print(x[y]) # [1 2 3] print('使用循环重构原数组：') for i in y: print(x[i], end=\" \") # 1 2 3 numpy.lexsort() numpy.lexsort() 用于对多个序列进行排序。把它想象成对电子表格进行排序，每一列代表一个序列，排序时优先照顾靠后的列。 这里举一个应用场景：小升初考试，重点班录取学生按照总成绩录取。在总成绩相同时，数学成绩高的优先录取，在总成绩和数学成绩都相同时，按照英语成绩录取…… 这里，总成绩排在电子表格的最后一列，数学成绩在倒数第二列，英语成绩在倒数第三列。 import numpy as np nm = ('raju', 'anil', 'ravi', 'amar') dv = ('f.y.', 's.y.', 's.y.', 'f.y.') ind = np.lexsort((dv, nm)) print('调用 lexsort() 函数：') print(ind) print('\\n') print('使用这个索引来获取排序后的数据：') print([nm[i] + \", \" + dv[i] for i in ind]) 调用 lexsort() 函数： [3 1 0 2] 使用这个索引来获取排序后的数据： ['amar, f.y.', 'anil, s.y.', 'raju, f.y.', 'ravi, s.y.'] 上面传入 np.lexsort 的是一个tuple，排序时首先排 nm， 顺序为：amar、anil、raju、ravi 。 综上排序结果为 [3 1 0 2]。 numpy.partition() >>> a = np.array([3, 4, 2, 1]) >>> np.partition(a, 3) # 将数组 a 中所有元素（包括重复元素）从小到大排列， # 3 表示的是排序数组索引为 3 的数字，比该数字小的排在该数字前面， # 比该数字大的排在该数字的后面 array([2, 1, 3, 4]) >>> >>> np.partition(a, (1, 3)) # 小于 1 的在前面，大于 3 的在后面，1和3之间的在中间 array([1, 2, 3, 4]) numpy.nonzero() numpy.nonzero() 函数返回输入数组中非零元素的索引。 import numpy as np a = np.array([[30, 40, 0], [0, 20, 10], [50, 0, 60]]) print(a) # [[30 40 0] # [ 0 20 10] # [50 0 60]] print('调用 nonzero() 函数：') print(np.nonzero(a)) m=np.nonzero(a) # (array([0, 0, 1, 1, 2, 2], dtype=int64), array([0, 1, 1, 2, 0, 2], dtype=int64)) #返回值的前后两部分数组，对应位置组合（0，0），（0，1）表示非0元素在原数组中的位置 #输出非0元素 print(a[m]) # [30 40 20 10 50 60] numpy.where() numpy.where(condition[, x, y]) 该函数可以接受一个必选参数 condition，注意该参数必须是 array 型的，只不过元素是 true 或者是 false . x,y是可选参数：如果条件为真，则返回x,如果条件为false，则返回y，注意condition、x、y三者必须要能够“广播”到相同的形状 返回结果：返回的是数组array或者是元素为array的tuple元组，如果只有一个condition，则返回包含array的tuple，如果是有三个参数，则返回一个array。 numpy.where() 函数返回输入数组中满足给定条件的元素的索引。 import numpy as np x = np.arange(9.).reshape(3, 3) print ('我们的数组是：') print (x) print ( '大于 3 的元素的索引：') y = np.where(x > 3) print (y) print ('使用这些索引来获取满足条件的元素：') print (x[y]) 我们的数组是： [[0. 1. 2.] [3. 4. 5.] [6. 7. 8.]] 大于 3 的元素的索引： (array([1, 1, 2, 2, 2]), array([1, 2, 0, 1, 2])) 使用这些索引来获取满足条件的元素： [4. 5. 6. 7. 8.] np.piecewise() numpy.piecewise(x, condlist, funclist, *args, **kw) 参数一 x:表示要进行操作的对象 参数二：condlist，表示要满足的条件列表，可以是多个条件构成的列表 参数三：funclist，执行的操作列表，参数二与参数三是对应的，当参数二为true的时候，则执行相对应的操作函数。 返回值：返回一个array对象，和原始操作对象x具有完全相同的维度和形状 x = np.arange(0,10) print(x) xx=np.piecewise(x, [x = 6], [-1, 1]) print(xx) [0 1 2 3 4 5 6 7 8 9] [-1 -1 -1 -1 0 0 1 1 1 1] 即将元素中小于4的用-1替换掉，大于等于6的用1替换掉，其余的默认以0填充。其实这里的替换和填充就是function，这不过这里的function跟简单粗暴，都用同一个数替换了. 使用lambda表达式 x = np.arange(0,10) xxxx=np.piecewise(x, [x = 6], [lambda x:x**2, lambda x:x*100]) print(xxxx) [ 0 1 4 9 0 0 600 700 800 900] numpy.extract() numpy.extract() 函数根据某个条件从数组中抽取 元素，返回满条件的元素。 numpy.extract(条件，数组)：如果满足某些指定条件，则返回input_array的元素。 import numpy as np # (1) 使用arange函数创建数组 a = np.arange(7) # (2) 生成选择偶数元素的条件变量 condition = (a % 2) == 0 # (3) 使用extract函数基于生成的条件从数组中抽取元素 print \"Even numbers\", np.extract(condition, a) # (4) 使用nonzero函数抽取数组中的非零元素 print \"Non zero\", np.nonzero(a) Update time： 2020-05-25 "},"Chapter1/常用统计函数.html":{"url":"Chapter1/常用统计函数.html","title":"常用统计函数","keywords":"","body":"常用统计函数 NumPy 提供了很多统计函数，用于从数组中查找最小元素，最大元素，百分位标准差和方差等。 numpy.cumsum() numpy.cumsum(a, axis=None, dtype=None, out=None) Parameters: a : array_like Input array. axis : int, optional Axis along which the cumulative sum is computed. The default (None) is to compute the cumsum over the flattened array. dtype : dtype, optional 按照所给定的轴参数返回元素的梯形累计和，axis=0，按照行累加。axis=1，按照列累加。axis不给定具体值，就把 numpy 数组当成一个一维数组。 >>> a = np.array([[1,2,3], [4,5,6]]) >>> a array([[1, 2, 3], [4, 5, 6]]) >>> np.cumsum(a) array([ 1, 3, 6, 10, 15, 21]) >>> np.cumsum(a, dtype=float) # specifies type of output value(s) array([ 1., 3., 6., 10., 15., 21.]) >>> >>> np.cumsum(a,axis=0) # sum over rows for each of the 3 columns array([[1, 2, 3], [5, 7, 9]]) >>> np.cumsum(a,axis=1) # sum over columns for each of the 2 rows array([[ 1, 3, 6], [ 4, 9, 15]]) numpy.sum() numpy.sum(a, axis=None, dtype=None, out=None, keepdims=, initial=, where=) a：array_like 类型 待求和的数组。 axis ：可选择None 或int类型 或 整型的tuple类型 >>> np.sum([0.5, 1.5]) 2.0 >>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32) 1 >>> np.sum([[0, 1], [0, 5]]) 6 >>> np.sum([[0, 1], [0, 5]], axis=0) array([0, 6]) >>> np.sum([[0, 1], [0, 5]], axis=1) array([1, 5]) amin()和 amax() numpy.amin()用于计算数组中的元素沿指定轴的最小值。 numpy.amax()用于计算数组中的元素沿指定轴的最大值。 import numpy as np a = np.array([[3, 7, 5], [8, 4, 3], [2, 4, 9]]) print(a) # [[3 7 5] # [8 4 3] # [2 4 9]] print(np.amin(a)) #2 print(np.amin(a, 1)) # 行最小值 # [3 3 2] print('再次调用 amin() 函数：') print(np.amin(a, 0)) # 列最小值 # [2 4 3] print('调用 amax() 函数：') print(np.amax(a)) # 9 print(np.amax(a, axis=1)) # 行最大值 # [7 8 9] print('再次调用 amax() 函数：') print(np.amax(a, axis=0)) # 列最大值 # [8 7 9] numpy.ptp() numpy.ptp()函数计算数组中元素最大值与最小值的差（最大值 - 最小值）。 import numpy as np a = np.array([[3, 7, 5], [8, 4, 3], [2, 4, 9]]) print(a) # [[3 7 5] # [8 4 3] # [2 4 9]] print('调用 ptp() 函数：') print(np.ptp(a)) # 7 print('沿轴 1 调用 ptp() 函数：') print(np.ptp(a, axis=1)) # [4 5 7] print('沿轴 0 调用 ptp() 函数：') print(np.ptp(a, axis=0)) # [6 3 6] numpy.median() 计算a在指定轴上的中位数（如果有两个，则取这两个的平均值） numpy.median(a[, axis, out, overwrite_input, keepdims]) numpy.median() 函数用于计算数组 a 中元素的中位数（中值） import numpy as np a = np.array([[30, 65, 70], [80, 95, 10], [50, 90, 60]]) print(a) # [[30 65 70] # [80 95 10] # [50 90 60]] print('调用 median() 函数：') print(np.median(a)) # 65.0 print('沿轴 0(列) 调用 median() 函数：') print(np.median(a, axis=0)) # [50. 90. 60.] print('沿轴 1（行） 调用 median() 函数：') print(np.median(a, axis=1)) # [65. 80. 60.] numpy.nanmedian(a[, axis, out, overwrite_input, ...]) :计算a在指定轴上的中位数，忽略NaN numpy.mean() numpy.mean()函数返回数组中元素的算术平均值。 如果提供了轴，则沿其计算。 算术平均值是沿轴的元素的总和除以元素的数量。 import numpy as np a = np.array([[1, 2, 3], [3, 4, 5], [4, 5, 6]]) print(a) # [[1 2 3] # [3 4 5] # [4 5 6]] print('调用 mean() 函数：') print(np.mean(a)) #3.6666666666666665 print('沿轴 0 调用 mean() 函数：') print(np.mean(a, axis=0)) #[2.66666667 3.66666667 4.66666667] print('沿轴 1 调用 mean() 函数：') print(np.mean(a, axis=1)) #[2. 4. 5.] numpy.nanmean(a[, axis, dtype, out, keepdims]) :计算a在指定轴上的算术均值，忽略NaN numpy.average() numpy.average(a[, axis, weights, returned]) numpy.average()函数根据在另一个数组中给出的各自的权重计算数组中元素的加权平均值。 该函数可以接受一个轴参数。 如果没有指定轴，则数组会被展开。 加权平均值即将各数值乘以相应的权数，然后加总求和得到总体值，再除以总的单位数。 考虑数组[1,2,3,4]和相应的权重[4,3,2,1]，通过将相应元素的乘积相加，并将和除以权重的和，来计算加权平均值。 import numpy as np a = np.array([1, 2, 3, 4]) print(a) #[1 2 3 4] print('调用 average() 函数：') print(np.average(a)) #2.5 # 不指定权重时相当于 mean 函数 wts = np.array([4, 3, 2, 1]) print('再次调用 average() 函数：') print(np.average(a, weights=wts)) #2.0 # 如果 returned 参数设为 true，则返回权重的和 print('权重的和：') print(np.average([1, 2, 3, 4], weights=[4, 3, 2, 1], returned=True)) #(2.0, 10.0) #第二个参数为权重的和 numpy.std()标准差 标准差是一组数据平均值分散程度的一种度量。 标准差是方差的算术平方根。 标准差公式如下： std = sqrt(mean((x - x.mean())**2)) 如果数组是 [1，2，3，4]，则其平均值为 2.5。 因此，差的平方是 [2.25,0.25,0.25,2.25]，并且其平均值的平方根除以 4，即 sqrt(5/4) ，结果为 1.1180339887498949。 import numpy as np print (np.std([1,2,3,4])) 1.1180339887498949 numpy.nanstd(a[, axis, dtype, out, ddof, keepdims]):计算a在指定轴上的标准差，忽略NaN numpy.var()方差 统计中的方差（样本方差）是每个样本值与全体样本值的平均数之差的平方值的平均数，即 mean((x - x.mean())** 2)。 偏样本方差biased sample variance。计算公式为 （ x¯\\bar{x}​x​¯​​为均值）： var=1N∑i=1N(xi−x¯)2\r v a r=\\frac{1}{N} \\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2}\r var=​N​​1​​∑​i=1​N​​(x​i​​−​x​¯​​)​2​​ 无偏样本方差unbiased sample variance。计算公式为 （ x¯\\bar{x}​x​¯​​为均值）： var=1N−1∑i=1N(xi−x¯)2\r v a r=\\frac{1}{N-1} \\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2}\r var=​N−1​​1​​∑​i=1​N​​(x​i​​−​x​¯​​)​2​​ 当ddof=0时，计算偏样本方差；当ddof=1时，计算无偏样本方差。默认值为 0。当ddof为其他整数时，分母就是N-ddof。 换句话说，标准差是方差的平方根。 import numpy as np print (np.var([1,2,3,4])) 1.25 numpy.nanvar(a[, axis, dtype, out, ddof, keepdims]) :计算a在指定轴上的方差，忽略 NaN NumPy 算术函数包含简单的加减乘除: add()，subtract()，multiply()和 divide()。 需要注意的是数组必须具有相同的形状或符合数组广播规则。 numpy.cov() 估计协方差矩阵，给定数据和权重。 numpy.cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None) 参数 m :array_like 包含多个变量和观察值的1-D或2-D数组。M的每一行代表一个变量（即特征），每一列都是对所有这些变量的单一观察（即每一列代表一个样本） y :array_like, optional 另外一组变量和观察结果。 y具有与m相同的形式。 rowvar : bool,optional 如果 rowvar 为 True（默认值），则每行代表一个变量，并在列中显示（即每一列为一个样本）。 否则，关系被转置：每列代表变量，而行包含观察值。 bias : bool,optional 默认归一化（False）为（N-1），其中N为给定观测次数（无偏估计）。如果bias为True，则归一化为N. 这些值可以通过使用numpy版本> = 1.5中的关键字ddof来覆盖。 ddof : int,optional 如果不是，偏移所隐含的默认值将被覆盖。请注意，ddof = 1将返回无偏估计，即使指定了权重和权重，ddof = 0将返回简单平均值。详见附注。 默认值为None。 fweights :array_like, int, optional 整数频率权重组成的1-D数组; 代表每个观察向量应重复的次数。 考虑两个变量，和，它们完全相关，但方向相反： >>> x = np.array([[0, 2], [1, 1], [2, 0]]).T >>> x array([[0, 1, 2], [2, 1, 0]]) 注意增加而减少。协方差矩阵清楚地表明： >>> np.cov(x) array([[ 1., -1.], [-1., 1.]]) 注意，示出和之间的相关性的元素是负的。 此外，请注意x和y是如何组合的： >>> x = [-2.1, -1, 4.3] >>> y = [3, 1.1, 0.12] >>> X = np.vstack((x,y)) >>> print(np.cov(X)) [[ 11.71 -4.286 ] [ -4.286 2.14413333]] >>> print(np.cov(x, y)) [[ 11.71 -4.286 ] [ -4.286 2.14413333]] >>> print(np.cov(x)) 11.71 协方差矩阵计算的是不同维度之间的协方差，而不是不同样本之间。 numpy.corrcoef(x[, y, rowvar, bias, ddof]) : 返回皮尔逊积差相关 numpy.correlate(a, v[, mode]) ：返回两个一维数组的互相关系数 numpy.cov(m[, y, rowvar, bias, ddof, fweights, ...])：返回协方差矩阵 multiply() import numpy as np a = np.arange(9, dtype = np.float_).reshape(3,3) print ('第一个数组：') print (a) 第一个数组： [[0. 1. 2.] [3. 4. 5.] [6. 7. 8.]] print ('第二个数组：') b = np.array([10,10,10]) print (b) 第二个数组： [10 10 10] print ('两个数组相加：') print (np.add(a,b)) 两个数组相加： [[10. 11. 12.] [13. 14. 15.] [16. 17. 18.]] print ('两个数组相减：') print (np.subtract(a,b)) 两个数组相减： [[-10. -9. -8.] [ -7. -6. -5.] [ -4. -3. -2.]] print ('两个数组相乘：') print (np.multiply(a,b)) 两个数组相乘： [[ 0. 10. 20.] [30. 40. 50.] [60. 70. 80.]] print ('两个数组相除：') print (np.divide(a,b)) 两个数组相除： [[0. 0.1 0.2] [0.3 0.4 0.5] [0.6 0.7 0.8]] numpy.reciprocal() numpy.reciprocal() 函数返回参数逐元素的倒数。如 1/4 倒数为 4/1。 import numpy as np a = np.array([0.25, 1.33, 1, 100]) print ('我们的数组是：') print (a) print ('\\n') print ('调用 reciprocal 函数：') print (np.reciprocal(a)) 我们的数组是： [ 0.25 1.33 1. 100. ] 调用 reciprocal 函数： [4. 0.7518797 1. 0.01 ] numpy.power() numpy.power()函数将第一个输入数组中的元素作为底数，计算它与第二个输入数组中相应元素的幂。 import numpy as np a = np.array([10,100,1000]) print ('我们的数组是；') print (a) print ('\\n') print ('调用 power 函数：') print (np.power(a,2)) print ('\\n') print ('第二个数组：') b = np.array([1,2,3]) print (b) print ('\\n') print ('再次调用 power 函数：') print (np.power(a,b)) 我们的数组是； [ 10 100 1000] 调用 power 函数： [ 100 10000 1000000] 第二个数组： [1 2 3] 再次调用 power 函数： [ 10 10000 1000000000] numpy.mod() numpy.mod() 计算输入数组中相应元素的相除后的余数。 函数 numpy.remainder()也产生相同的结果。 import numpy as np a = np.array([10,20,30]) b = np.array([3,5,7]) print ('第一个数组：') print (a) print ('\\n') print ('第二个数组：') print (b) print ('\\n') print ('调用 mod() 函数：') print (np.mod(a,b)) a = np.array([10, 20, 30]) print(a) print(np.mod(a, 3)) #[1 2 0] print ('调用 remainder() 函数：') print (np.remainder(a,b)) 第一个数组： [10 20 30] 第二个数组： [3 5 7] 调用 mod() 函数： [1 0 2] 调用 remainder() 函数： [1 0 2] 参考 统计 Update time： 2020-07-05 "},"Chapter1/NumPy 常用函数.html":{"url":"Chapter1/NumPy 常用函数.html","title":"NumPy 常用函数","keywords":"","body":"NumPy 常用函数 numpy.argpartition numpy.argpartition(a, kth, axis=-1, kind='introselect', order=None) 在快排算法中，有一个典型 的操作：partition。这个操作指：根据一个数值x，把数组中的元素划分成两半，使得index前面的元素都不大于x，index后面的元素都不小于x。 numpy 中的 argpartition() 函数就是起的这个作用。对于传入的数组a，先用O(n)复杂度求出第 k 大的数字，然后利用这个第 k 大的数字将数组 a 划分成两半。 (第 k 大的数字, 就是排序（从小到大）后的数组，第k个位置的元素，自己理解的) 此函数不对原数组进行操作，它只返回分区之后的下标。一般 numpy 中以 arg 开头的函数都是返回下标而不改变原数组。 此函数还有另外两个参数： kind：用于指定partition的算法 order：表示排序的key，也就是按哪些字段进行排序 当我们只关心 TopK 时，我们不需要使用np.sort()对数组进行全量排序，np.argpartition()已经够用了。 $ import numpy as np $ a = np.array([9, 4, 4, 3, 3, 9, 0, 4, 6, 0]) $ print(np.argpartition(a, 4)) #将数组a中所有元素（包括重复元素）从小到大排列，比第5大的元素小的放在前面，大的放在后面，输出新数组索引 >> [6 9 4 3 7 2 1 5 8 0] $ a[np.argpartition(a, 4)] #输出新数组索引对应的数组 >> array([0, 0, 3, 3, 4, 4, 4, 9, 6, 9]) 注意，排序规则是从0开始排序，即，第0大的元素为“0”，第1大的元素还为“0”，第2大的元素为“3” >>> arr = np.array([8,7,6,5,4,3,2,1]) >>> np.argpartition(arr, 0) array([7, 1, 2, 3, 4, 5, 6, 0], dtype=int32) >>> np.argpartition(arr, 1) array([7, 6, 2, 3, 4, 5, 1, 0], dtype=int32) >>> np.argpartition(arr, 2) array([7, 6, 5, 3, 4, 2, 1, 0], dtype=int32) >>> np.argpartition(arr, 3) array([6, 7, 5, 4, 3, 1, 2, 0], dtype=int32) >>> np.argpartition(arr, 4) array([4, 7, 6, 5, 3, 1, 2, 0], dtype=int32) >>> np.argpartition(arr, 5) array([4, 7, 6, 5, 3, 2, 1, 0], dtype=int32) >>> np.argpartition(arr, 6) array([4, 7, 6, 5, 3, 2, 1, 0], dtype=int32) >>> np.argpartition(arr, 7) array([4, 7, 6, 5, 3, 2, 1, 0], dtype=int32) 第一次调用，给第二个参数传了0，说明我需要返回最小值得索引index。得到的返回值是array([7, 1, 2, 3, 4, 5, 6, 0], dtype=int32)，在这个返回的array中，我关心的是第0个值（7），它是原数组arr的索引，arr[7]就是我要找的最小值。请注意返回值中的其他几个索引值，和原数组的索引比起来，他们基本上没有什么变化。接下来的几次调用也是这种情况，其实这也就说明argpartition没有对他不关心的数据做太大的改动或者操作。 $ import numpy as np $ a = np.array([9, 4, 4, 3, 3, 9, 0, 4, 6, 0]) $ a[np.argpartition(a, -5)[-5:]] >> array([4, 4, 9, 6, 9]) numpy.clip numpy.clip(a, a_min, a_max, out=None)[source] 在很多数据处理和算法中（比如强化学习中的 PPO），我们需要使得所有的值保持在一个上下限区间内。Numpy 内置的 Clip 函数可以解决这个问题。Numpy clip () 函数用于对数组中的值进行限制。给定一个区间范围，区间范围外的值将被截断到区间的边界上。例如，如果指定的区间是 [-1,1]，小于-1 的值将变为-1，而大于 1 的值将变为 1 Clip示例：限制数组中的最小值为 2，最大值为 6。 array = np.array([10, 7, 4, 3, 2, 2, 5, 9, 0, 4, 6, 0]) np.clip(array,2,5) # array([5, 5, 4, 3, 2, 2, 5, 5, 2, 4, 5, 2]) array = np.array([10, -1, 4, -3, 2, 7, 5, 9, 0, 4, 6, 0]) # array([6, 2, 4, 2, 2, 2, 5, 6, 2, 4, 6, 2]) 高维数组也是一样的 x=np.array([[1,2,3,5,6,7,8,9],[1,2,3,5,6,7,8,9]]) np.clip(x,3,8) Out[90]: array([[3, 3, 3, 5, 6, 7, 8, 8], [3, 3, 3, 5, 6, 7, 8, 8]]) numpy.where() numpy.where()函数，此函数返回数组中满足某个条件的元素的索引 np.where(condition, x, y) 三目运算满足condition，为x；不满足condition，则为y score = np.array([[80, 88], [82, 81], [84, 75], [86, 83], [75, 81]]) # 如果数值小于80，替换为0，如果大于等于80，替换为90 re_score = np.where(score np.where(condition) 只有条件 (condition)，没有x和y，则输出满足条件 (即非0) 元素的坐标 (等价于numpy.nonzero)。这里的坐标以tuple的形式给出，通常原数组有多少维，输出的tuple中就包含几个数组，分别对应符合条件元素的各维坐标。 //这里我们输出x数据中大于5的元素的索引 y=np.where(x>5) print(y) 输出结果： (array([1, 2, 2, 2], dtype=int64), array([2, 0, 1, 2], dtype=int64)) //使用索引取出元素\" print(x[y]) 输出结果： [6 7 8 9] numpy.extract() 从数组中提取符合条件的元素 numpy.extract()函数，和where函数有一点相，不过extract函数是返回满足条件的元素，而不是元素索引 arr = np.arange(10) arrarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])# Define the codition, here we take MOD 3 if zero condition = np.mod(arr, 3)==0 #conditionarray([ True, False, False, True, False, False, True, False, False,True]) np.extract(condition, arr) #array([0, 3, 6, 9]) 同样地，如果有需要，我们可以用 AND 和 OR 组合的直接条件，如下所示： np.extract(((arr > 2) & (arr numpy.setdiff1d 如何找到仅在 A 数组中有而 B 数组没有的元素 setdiff1d(ar1, ar2, assume_unique=False) 1.功能：找到2个数组中集合元素的差异。 2.返回值：在ar1中但不在ar2中的'已排序'的'唯一值'。 3.参数： ar1：array_like 输入数组。 ar2：array_like 输入比较数组。 assume_unique：bool。如果为True，则假定输入数组是唯一的，即可以加快计算速度。 默认值为False。 返回数组中不在另一个数组中的独有元素。这等价于两个数组元素集合的差集。 a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]) b = np.array([3,4,7,6,7,8,11,12,14]) c = np.setdiff1d(a,b) carray([1, 2, 5, 9]) numpy.intersect1d() 求两个数组的交集 >>> np.intersect1d([1, 3, 4, 3], [3, 1, 2, 1]) array([1, 3]) 要求交集的数组多于两个, 可使用functools.reduce: >>> from functools import reduce >>> reduce(np.intersect1d, ([1, 3, 4, 3], [3, 1, 2, 1], [6, 3, 4, 2])) array([3]) numpy.nonzero() numpy.nonzero(a) 返回数组 a 中非零元素的索引值数组。 （1）只有 a 中非零元素才会有索引值，那些零值元素没有索引值； （2）返回的索引值数组是一个2维 tuple 数组，该 tuple 数组中包含一维的array数组。其中，一维 array 向量的个数与 a 的维数是一致的。 （3）索引值数组的每一个array均是从一个维度上来描述其索引值。比如，如果a是一个二维数组，则索引值数组有两个 array，第一个 array 从行维度来描述索引值；第二个 array 从列维度来描述索引值。 （4） 该np.transpose(np.nonzero(x)) 函数能够描述出每一个非零元素在不同维度的索引值。 （5）通过 a[nonzero(a)]得到所有a中的非零值 nz = np.nonzero([1,2,0,0,4,0])​ nz #(array([0, 1, 4], dtype=int64),) nz[0] #array([0, 1, 4], dtype=int64) nz[0] #array([0, 1, 4], dtype=int64) numpy.pad() np.pad（array，pad_width，mode，**kwars） 解释： 第一个参数是待填充数组 第二个参数是填充的形状，（2，3）表示前面两个，后面三个 第三个参数是填充的方法 填充方法： constant连续一样的值填充，有关于其填充值的参数。 constant_values=（x, y）时前面用x填充，后面用y填充。缺参数是为0000。。。 edge用边缘值填充 linear_ramp边缘递减的填充方式 maximum, mean, median, minimum分别用最大值、均值、中位数和最小值填充 reflect, symmetric都是对称填充。前一个是关于边缘对称，后一个是关于边缘外的空气对称╮(╯▽╰)╭ wrap用原数组后面的值填充前面，前面的值填充后面 也可以有其他自定义的填充方法 numpy.percentile（） 分位数函数 在 python 中计算一个多维数组的任意百分比分位数，此处的百分位是从小到大排列，只需用np.percentile即可…… numpy.percentile(a, q, axis=None, out=None, overwrite_input=False, interpolation='linear', keepdims=False) a : np数组 q : float in range of [0,100] (or sequence of floats) Percentile to compute。 要计算的q分位数。 axis : 那个轴上运算。 keepdims :bool是否保持维度不变。 import numpy as np a = np.array([1,2,3,4,5]) p = np.percentile(a, 50) # 求中位数 # 3 >>> a = np.array([[10, 7, 4], [3, 2, 1]]) >>> a array([[10, 7, 4], [ 3, 2, 1]]) >>> np.percentile(a, 50) #50%的分位数，就是a里排序之后的中位数 3.5 >>> np.percentile(a, 50, axis=0) #axis为0，在纵列上求 array([[ 6.5, 4.5, 2.5]]) >>> np.percentile(a, 50, axis=1) #axis为1，在横行上求 array([ 7., 2.]) >>> np.percentile(a, 50, axis=1, keepdims=True) #keepdims=True保持维度不变 array([[ 7.], [ 2.]]) numpy.nanpercentile 忽略空值的百分位数 numpy.count_nonzero() numpy.count_nonzero(a) 计算数组a中非零值的数量。 >>> np.count_nonzero(np.eye(4)) 4 >>> np.count_nonzero([[0,1,7,0,0],[3,0,0,2,19]]) 5 numpy.nonzero() 返回非零元素的索引。 返回数组的元组，每个维度a一个，包含该维度中非零元素的索引。a中的值总是以行主，C风格顺序测试和返回。相应的非零值可以用下式获得： >>> x array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]) >>> np.nonzero(x) (array([0, 1, 2]), array([0, 1, 2])) >>> x[np.nonzero(x)] array([ 1., 1., 1.]) >>> np.transpose(np.nonzero(x)) array([[0, 0], [1, 1], [2, 2]]) nonzero的常见用法是找到数组的索引，其中条件为True。 >>> a = np.array([[1,2,3],[4,5,6],[7,8,9]]) >>> a > 3 array([[False, False, False], [ True, True, True], [ True, True, True]], dtype=bool) >>> np.nonzero(a > 3) (array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2])) 参考： Sorting, searching, and counting Update time： 2020-05-25 "},"Chapter1/NumPy 协方差cov与相关系数corrcoef的使用.html":{"url":"Chapter1/NumPy 协方差cov与相关系数corrcoef的使用.html","title":"NumPy 协方差cov与相关系数corrcoef的使用","keywords":"","body":"NumPy 协方差cov与相关系数corrcoef的使用 协方差 如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值时另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值；如果两个变量的变化趋势相反，即其中一个变量大于自身的期望值时另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。 可以通俗的理解为：两个变量在变化过程中是同方向变化？还是反方向变化？同向或反向程度如何？ 你变大，同时我也变大，说明两个变量是同向变化的，这时协方差就是正的; 你变大，同时我变小，说明两个变量是反向变化的，这时协方差就是负的; 从数值来看，协方差的数值越大，两个变量同向程度也就越大。反之亦然。 import numpy as np a = np.array([1,2,3]) b = np.array([4,3,4]) x = np.vstack((a,b)) >>> np.cov(a, b) array([[ 1. , 0. ], [ 0. , 0.33333333]]) >>> np.cov(a, b, bias=True) array([[ 0.66666667, 0. ], [ 0. , 0.22222222]]) >>> np.cov(x) array([[ 1. , 0. ], [ 0. , 0.33333333]]) bais 参数的意义，默认为False，那么计算均值的时候除以 n - 1，如果设为True，那么计算均值的时候除以n，其中n为随机变量的维度数 numpy 的 cov 函数使用的三点： 变量矩阵的一行表示一个随机变量； bais参数控制计算时除以n-1还是n, True表示除以n，False表示除以n-1； 输出结果是一个协方差矩阵, results[i][j]表示第i个随机变量与第j个随机变量的协方差. 相关系数 相关系数是用以反映变量之间相关关系密切程度的统计指标。相关系数也可以看成协方差：一种剔除了两个变量量纲影响、标准化后的特殊协方差,它消除了两个变量变化幅度的影响，而只是单纯反应两个变量每单位变化时的相似程度。 需要注意的是, np.corrcoef()接受的参数是一个矩阵,返回的结果也是一个矩阵 import numpy as np a = np.array([1,2,3]) b = np.array([2,5,8]) x = np.vstack((a,b)) np.corrcoef(a, b) np.corrcoef(x) >>> np.corrcoef(a, b) array([[ 1., 1.], [ 1., 1.]]) >>> np.corrcoef(x) array([[ 1., 1.], [ 1., 1.]]) numpy 中函数 corrcoef 的用法与函数 cov 的用法相似，只是corrcoef中bais参数不起作用。 numpy的corrcoef 函数使用的两点： 变量矩阵的一行表示一个随机变量； 输出结果是一个相关系数矩阵, results[i][j]表示第i个随机变量与第j个随机变量的相关系数. 参考 协方差与相关系数 numpy中cov与corrcoef的使用 Update time： 2020-08-16 "},"Chapter1/numpy.set_printoptions.html":{"url":"Chapter1/numpy.set_printoptions.html","title":"numpy.set_printoptions()","keywords":"","body":"numpy.set_printoptions() set_printoptions() 设置输出样式 numpy.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, suppress=None, nanstr=None, infstr=None, formatter=None )[source] precision: int, optional，float输出的精度，即小数点后维数，默认8（ Number of digits of precision for floating point output (default 8)） threshold : int, optional，当数组数目过大时，设置显示几个数字，其余用省略号（Total number of array elements which trigger summarization rather than full repr (default 1000).） edgeitems: int, optional，边缘数目（Number of array items in summary at beginning and end of each dimension (default 3)）. linewidth : int, optional，The number of characters per line for the purpose of inserting line breaks (default 75). suppress : bool, optional，是否压缩由科学计数法表示的浮点数（Whether or not suppress printing of small floating point values using scientific notation (default False).） nanstr: str, optional，String representation of floating point not-a-number (default nan). infstr : str, optional，String representation of floating point infinity (default inf). 浮点精度可设置： >>> np.set_printoptions(precision=4) >>> print(np.array([1.123456789])) [ 1.1235] 长数组可概括为： >>> np.set_printoptions(threshold=5) >>> print(np.arange(10)) [0 1 2 ..., 7 8 9] np.set_printoptions(threshold=np.nan) 设置打印时显示方式, threshold=np.nan 意思是输出数组的时候完全输出，不需要省略号将中间数据省略 参考： http://doc.codingdict.com/NumPy_v111/reference/generated/numpy.set_printoptions.html#numpy.set_printoptions Update time： 2020-05-25 "},"Chapter1/numpy.datetime64日期函数.html":{"url":"Chapter1/numpy.datetime64日期函数.html","title":"numpy.datetime64()日期函数","keywords":"","body":"numpy.datetime64()日期函数 Basic Datetimes 创建数据时间的最基本的方法是使用ISO 8601日期或日期时间格式的字符串。内部存储单元是从字符串的形式自动选择的，可以是date unit或time unit。 日期单位是年（‘Y’），月（‘M’），星期（‘W’）和日（‘D’），而时间单位是小时（h），秒（‘s’），毫秒（‘ms’）和一些附加的基于秒前缀的单位。 简单的ISO日期 >>> np.datetime64('2005-02-25') numpy.datetime64('2005-02-25') str(np.datetime64('2005-02-25')) #'2005-02-25' 使用月份为单位 >>> np.datetime64('2005-02') numpy.datetime64('2005-02') 仅指定月份，但强制使用“天”单位： >>> np.datetime64('2005-02', 'D') numpy.datetime64('2005-02-01') 从字符串创建数据集的数组时，仍然可以使用带有通用单位的日期时间类型从输入中自动选择单位。 >>> np.array(['2007-07-13', '2006-01-13', '2010-08-13'], dtype='datetime64') array(['2007-07-13', '2006-01-13', '2010-08-13'], dtype='datetime64[D]') 生成日期范围 datetime类型适用于许多常见的NumPy函数，例如arange可用于生成日期范围。 所有日期一个月： 指定格式dtype=datetime64[D] 天 Z = np.arange('2016-07', '2016-08', dtype='datetime64[D]') ''' array(['2016-07-01', '2016-07-02', '2016-07-03', '2016-07-04', '2016-07-05', '2016-07-06', '2016-07-07', '2016-07-08', '2016-07-09', '2016-07-10', '2016-07-11', '2016-07-12', '2016-07-13', '2016-07-14', '2016-07-15', '2016-07-16', '2016-07-17', '2016-07-18', '2016-07-19', '2016-07-20', '2016-07-21', '2016-07-22', '2016-07-23', '2016-07-24', '2016-07-25', '2016-07-26', '2016-07-27', '2016-07-28', '2016-07-29', '2016-07-30', '2016-07-31'], dtype='datetime64[D]') ''' 日期和时间的增量算法 timedelta64 NumPy 允许减去两个日期时间值，一个产生具有时间单位的数字的操作。由于 NumPy 在其核心中没有物理量系统，因此创建了timedelta64数据类型以补充datetime64。 Datetimes 和Timedeltas 一起工作，为简单的日期时间计算提供方法。 >>> np.datetime64('2009-01-01') - np.datetime64('2008-01-01') #numpy.timedelta64(366,'D') str(np.datetime64('2009-01-01') - np.datetime64('2008-01-01')) '366 days' str(np.datetime64('2009-01-01') - np.datetime64('2008-01-01')).split(\" \")[0] #'366' np.busday_count() 要查找datetime64日期的指定范围内有多少有效天数，请使用busday_count： >>> np.busday_count(np.datetime64('2011-07-11'), np.datetime64('2011-07-18')) 5 >>> np.busday_count(np.datetime64('2011-07-18'), np.datetime64('2011-07-11')) -5 如果你有一个datetime64 day值的数组，并且你想要一个有多少是有效日期的计数，你可以这样做： >>> a = np.arange(np.datetime64('2011-07-11'), np.datetime64('2011-07-18')) >>> np.count_nonzero(np.is_busday(a)) 5 参考： Datetimes and Timedeltas Update time： 2020-05-25 "},"Chapter1/numpy.split函数.html":{"url":"Chapter1/numpy.split函数.html","title":"numpy.split()函数","keywords":"","body":"numpy.split()函数 np.split(ary, indices_or_sections, axis=0) 函数功能： 把一个数组从左到右按顺序切分 参数： ary：要切分的数组 indices_or_sections：如果是一个整数，就用该数平均切分，如果是一个数组，为沿轴切分的位置 axis：沿着哪个维度进行切向，默认为0，横向切分 一维数组 x = np.array([0,1,2,3,4,5,6,7,8]) print (np.split(x,3)) # [array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8])] print (np.split(x,[3,5,6,9])) # [array([0, 1, 2]), array([3, 4]), array([5]), array([6, 7, 8]), # array([], dtype=int32)] print(np.split(x,[3,5,6,8])) # [array([0, 1, 2]), array([3, 4]), array([5]), array([6, 7]), array([8])] 当 indices_or_sections 为数组时，为[) 的情况： 很明显，因为一维数组只有一个维度，所以切分只会在这一个维度进行。第一行输出对应indices_or_sections参数为一个整数，将数组平均分成了三份，第二行输出对应indices_or_sections参数为一个数组，此时每一次切分都会将要切分数组的前n（n=3,5，6,9）个元素切分出来，第一次n=3，进行数组切分得到array([0, 1, 2])，第二次n=5，进行数组切分得到array([3, 4])，此时数组前5个元素已经切分完毕，后续同理，最后一次n=9，切分完毕后数组所有元素已经被切分，所以最后一个array为array([], dtype=int32)，对比第三行输出可以看出区别。 二维数组 import numpy as np a = np.array([[1,2,3], [1,2,5], [4,6,7]]) print (np.split(a, [2, 3],axis = 0)) [array([[1, 2, 3], [1, 2, 5]]), array([[4, 6, 7]]), array([], shape=(0, 3), dtype=int32)] print (np.split(a, [1, 2],axis = 1)) [array([[1], [1], [4]]), array([[2], [2], [6]]), array([[3], [5], [7]])] Update time： 2020-05-25 "},"Chapter1/Numpy np.count_nonzero.html":{"url":"Chapter1/Numpy np.count_nonzero.html","title":"Numpy np.count_nonzero","keywords":"","body":"Numpy np.count_nonzero numpy.count_nonzero(a) 计算数组a中非零值的数量。 参数： a：array_like要为其计数非零的数组。 返回： count：int或数组int数组中的非零值数。 >>> np.count_nonzero(np.eye(4)) 4 >>> np.count_nonzero([[0,1,7,0,0],[3,0,0,2,19]]) 5 Update time： 2020-05-25 "},"Chapter1/Numpy np.zeros_like函数.html":{"url":"Chapter1/Numpy np.zeros_like函数.html","title":"Numpy np.zeros_like()函数","keywords":"","body":"Numpy np.zeros_like()函数 numpy.zeros_like(a, dtype = None, order ='K', subok = True ) 返回与指定数组具有相同形状和数据类型的数组，并且数组中的值都为0。 参数 a ： array_like 用 `a`的形状和数据类型，来定义返回数组的属性 dtype： 数据类型，可选 覆盖结果的数据类型。 order 顺序 ： {'C'，'F'，'A'或'K'}，可选 覆盖结果的内存布局。'C'表示C顺序，'F'表示F顺序，'A'表示如果a是Fortran连续，则表示'F'，否则'C'。“K”表示匹配的布局一个尽可能接近。 subok： bool，可选。 值为True是使用a的内部数据类型，值为False是使用a数组的数据类型，默认为True Update time： 2020-05-25 "},"Chapter1/Numpy 上三角矩阵triu和下三角矩阵tril.html":{"url":"Chapter1/Numpy 上三角矩阵triu和下三角矩阵tril.html","title":"Numpy 上三角矩阵triu和下三角矩阵tril","keywords":"","body":"Numpy 上三角矩阵triu和下三角矩阵tril numpy.tril 下三角矩阵 >>> np.tril([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], -1) array([[ 0, 0, 0], [ 4, 0, 0], [ 7, 8, 0], [10, 11, 12]]) numpy.triu >>> np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], -1) array([[ 1, 2, 3], [ 4, 5, 6], [ 0, 8, 9], [ 0, 0, 12]]) numpy.triu_indices numpy.triu_indices(n, k=0, m=None) 参数： n : 返回的索引将有效的数组的大小。 k : int 可选， 对角线偏移 m : int 可选 返回的数组将有效的数组的列维度。默认情况下，m等于n。 返回 inds：元组，形状（2）的数组，形状（n） 三角形的索引。返回的元组包含两个数组，每个数组的索引沿着数组的一个维度。可用于切割形状的阵列（n，n）。 In [29]: iu1 = np.triu_indices(3) In [30]: iu2 = np.triu_indices(4) In [31]: iu3 = np.triu_indices(4, 2) In [32]: a = np.arange(16).reshape(4, 4) In [33]: a Out[33]: array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) In [34]: a[iu1] Out[34]: array([ 0, 1, 2, 5, 6, 10]) In [35]: a[iu2] Out[35]: array([ 0, 1, 2, 3, 5, 6, 7, 10, 11, 15]) In [36]: a[iu1]=-1 In [37]: a Out[37]: array([[-1, -1, -1, 3], [ 4, -1, -1, 7], [ 8, 9, -1, 11], [12, 13, 14, 15]]) In [38]: a[iu2] = -10 In [39]: a Out[39]: array([[-10, -10, -10, -10], [ 4, -10, -10, -10], [ 8, 9, -10, -10], [ 12, 13, 14, -10]]) In [40]: a = np.arange(16).reshape(4, 4) In [41]: a Out[41]: array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) In [42]: a[iu3] Out[42]: array([2, 3, 7]) In [43]: a[iu3] = 100 In [44]: a Out[44]: array([[ 0, 1, 100, 100], [ 4, 5, 6, 100], [ 8, 9, 10, 11], [ 12, 13, 14, 15]]) numpy.tril_indices 同理 numpy.triu_indices_from 返回上三角矩阵的索引 data = np.random.randn(5,5) mask = np.zeros_like(data) mask array([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) np.triu_indices_from(mask) (array([0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4], dtype=int64), array([0, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 3, 4, 4], dtype=int64)) numpy.tril_indices_from 同理 参考： 官方文档 官方文档(中) Update time： 2020-05-25 "},"Chapter1/NumPy 矩阵库Matrix.html":{"url":"Chapter1/NumPy 矩阵库Matrix.html","title":"NumPy 矩阵库(Matrix)","keywords":"","body":"NumPy 矩阵库(Matrix) NumPy 中包含了一个矩阵库 numpy.matlib，该模块中的函数返回的是一个矩阵，而不是 ndarray 对象。 一个m*n 的矩阵是一个由行（mrow）列n（column）元素排列成的矩形阵列。 矩阵里的元素可以是数字、符号或数学式。以下是一个由 6 个数字元素构成的 2 行 3 列的矩阵： matlib.empty() matlib.empty() 函数返回一个新的矩阵，语法格式为： numpy.matlib.empty(shape, dtype, order) 返回给定形状和类型的新矩阵，而不初始化条目。 参数说明： shape: 定义新矩阵形状的整数或整数元组 Dtype: 可选，数据类型 order: C（行序优先） 或者 F（列序优先） >>> import numpy.matlib >>> np.matlib.empty((2, 2)) # filled with random data matrix([[ 6.76425276e-320, 9.79033856e-307], [ 7.39337286e-309, 3.22135945e-309]]) #random >>> np.matlib.empty((2, 2), dtype=int) matrix([[ 6600475, 0], [ 6586976, 22740995]]) #random numpy.matlib.zeros() numpy.matlib.zeros() 函数创建一个以 0 填充的矩阵。 shape：int或ints序列矩阵的形状 dtype：数据类型，可选矩阵的所需数据类型，默认为float。 order：{'C'，'F'}，可选是否以C或Fortran连续顺序存储结果，默认值为“C”。 如果shape具有长度一即(N,)或者是标量N，则out形状矩阵(1,N)。 >>> import numpy.matlib >>> np.matlib.zeros((2, 3)) matrix([[ 0., 0., 0.], [ 0., 0., 0.]]) >>> np.matlib.zeros(2) matrix([[ 0., 0.]]) numpy.matlib.ones() 矩阵的一。 返回给定形状和类型的矩阵，用一个填充。 shape：{sequence of ints，int} 矩阵的形状 dtype：数据类型，可选 矩阵的所需数据类型，默认为np.float64。 order：{'C'，'F'}，可选是否以C或Fortran连续顺序存储矩阵，默认值为“C”。 >>> np.matlib.ones((2,3)) matrix([[ 1., 1., 1.], [ 1., 1., 1.]]) >>> np.matlib.ones(2) matrix([[ 1., 1.]]) numpy.matlib.eye() numpy.matlib.eye() 函数返回一个矩阵，对角线元素为 1，其他位置为零。 numpy.matlib.eye(n, M,k, dtype) 参数说明： n: 返回矩阵的行数 M: 返回矩阵的列数，默认为 n k: 对角线的索引 , 0表示主对角线，正值表示上对角线，负值表示下对角线。 dtype: 数据类型 >>> import numpy.matlib >>> np.matlib.eye(3, k=1, dtype=float) matrix([[ 0., 1., 0.], [ 0., 0., 1.], [ 0., 0., 0.]]) numpy.eye 等效数组功能。 numpy.matlib.identity() numpy.matlib.identity(n,dtype=None)函数返回给定大小的单位矩阵。 单位矩阵是个方阵，从左上角到右下角的对角线（称为主对角线）上的元素均为 1，除此以外全都为 0。 n：int 返回的单位矩阵的大小。 dtype：数据类型，可选 输出的数据类型。默认为float。 >>> import numpy.matlib >>> np.matlib.identity(3, dtype=int) matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) numpy.identity 等效数组功能。 numpy.matlib.rand() numpy.matlib.rand(*args)函数创建一个给定大小的矩阵，数据是随机填充的。 返回具有给定形状的随机值矩阵。 创建给定形状的矩阵，并通过[0， 1）的均匀分布的随机样本传播它。 * args：参数 输出形状。如果给定为N个整数，每个整数指定一个维度的大小。如果给出一个元组，这个元组给出完整的形状。 >>> import numpy.matlib >>> np.matlib.rand(2, 3) matrix([[ 0.68340382, 0.67926887, 0.83271405], [ 0.00793551, 0.20468222, 0.95253525]]) #random >>> np.matlib.rand((2, 3)) matrix([[ 0.84682055, 0.73626594, 0.11308016], [ 0.85429008, 0.3294825 , 0.89139555]]) #random 如果第一个参数是元组，则忽略其他参数： >>> np.matlib.rand((2, 3), 4) matrix([[ 0.46898646, 0.15163588, 0.95188261], [ 0.59208621, 0.09561818, 0.00583606]]) #random nump.mat() 将输入解释为矩阵。 numpy.mat(data, dtype=None) data：array_like 输入数据。 dtype：数据类型 输出矩阵的数据类型。 >>> from numpy import * >>> a1=array([1,2,3]) >>> a1 array([1, 2, 3]) >>> a1=mat(a1) >>> a1 matrix([[1, 2, 3]]) >>> shape(a1) (1, 3) >>> b=matrix([1,2,3]) >>> shape(b) (1, 3) 创建常见的矩阵 >>>data1=mat(zeros((3,3))) #创建一个3*3的零矩阵，矩阵这里zeros函数的参数是一个tuple类型(3,3) >>> data1 matrix([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]]) >>>data2=mat(ones((2,4))) #创建一个2*4的1矩阵，默认是浮点型的数据，如果需要时int类型，可以使用dtype=int >>> data2 matrix([[ 1., 1., 1., 1.], [ 1., 1., 1., 1.]]) >>>data3=mat(random.rand(2,2)) #这里的random模块使用的是numpy中的random模块，random.rand(2,2)创建的是一个二维数组，需要将其转换成#matrix >>> data3 matrix([[ 0.57341802, 0.51016034], [ 0.56438599, 0.70515605]]) >>>data4=mat(random.randint(10,size=(3,3))) #生成一个3*3的0-10之间的随机整数矩阵，如果需要指定下界则可以多加一个参数 >>> data4 matrix([[9, 5, 6], [3, 0, 4], [6, 0, 7]]) >>>data5=mat(random.randint(2,8,size=(2,5))) #产生一个2-8之间的随机整数矩阵 >>> data5 matrix([[5, 4, 6, 3, 7], [5, 3, 3, 4, 6]]) >>>data6=mat(eye(2,2,dtype=int)) #产生一个2*2的对角矩阵 >>> data6 matrix([[1, 0], [0, 1]]) a1=[1,2,3] a2=mat(diag(a1)) #生成一个对角线为1、2、3的对角矩阵 >>> a2 matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]]) 矩阵运算 矩阵相乘 a1=mat([1,2]); a2=mat([[1],[2]]); a3=a1*a2; #1*2的矩阵乘以2*1的矩阵，得到1*1的矩阵 矩阵点乘(矩阵对应元素相乘) 矩阵对应元素相乘 a1=mat([3,1]) a2=mat([2,2]) a3=multiply(a1,a2) # matrix([[6, 2]]) 矩阵求逆，转置 矩阵求逆 >>>a1=mat(eye(2,2)*0.5) >>> a1 matrix([[ 0.5, 0. ], [ 0. , 0.5]]) >>>a2=a1.I #求矩阵matrix([[0.5,0],[0,0.5]])的逆矩阵 >>> a2 matrix([[ 2., 0.], [ 0., 2.]]) 矩阵转置 >>> a1=mat([[1,1],[0,0]]) >>> a1 matrix([[1, 1], [0, 0]]) >>> a2=a1.T >>> a2 matrix([[1, 0], [1, 0]]) 矩阵的分隔和合并 矩阵的分隔，同列表和数组的分隔一致。 >>>a=mat(ones((3,3))) >>> a matrix([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]]) >>>b=a[1:,1:] #分割出第二行以后的行和第二列以后的列的所有元素 >>> b matrix([[ 1., 1.], [ 1., 1.]]) 矩阵的合并 >>>a=mat(ones((2,2))) >>> a matrix([[ 1., 1.], [ 1., 1.]]) >>>b=mat(eye(2)) >>> b matrix([[ 1., 0.], [ 0., 1.]]) >>>c=vstack((a,b)) #按列合并，即增加行数 >>> c matrix([[ 1., 1.], [ 1., 1.], [ 1., 0.], [ 0., 1.]]) >>>d=hstack((a,b)) #按行合并，即行数不变，扩展列数 >>> d matrix([[ 1., 1., 1., 0.], [ 1., 1., 0., 1.]]) 扩展矩阵函数tile() tile(inX, (i,j)) ;i是扩展个数，j是扩展长度 >>>x=mat([0,0,0]) >>> x matrix([[0, 0, 0]]) >>> tile(x,(3,1)) #即将x扩展3个，j=1,表示其列数不变 matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) >>> tile(x,(2,2)) #x扩展2次，j=2,横向扩展 matrix([[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]) Update time： 2020-05-25 "},"Chapter1/NumPy 线性代数中的运算.html":{"url":"Chapter1/NumPy 线性代数中的运算.html","title":"NumPy 线性代数中的运算","keywords":"","body":"NumPy 线性代数中的运算 NumPy 提供了线性代数函数库 linalg，该库包含了线性代 数所需的所有功能 。 矩阵和向量积 numpy.dot() numpy.dot()对于两个一维的数组，计算的是这两个数组对应下标元素的乘积和(数学上称之为内积)；对于二维数组，计算的是两个数组的矩阵乘积；对于多维数组，它的通用计算公式如下，即结果数组中的每个元素都是：数组a的最后一维上的所有元素与数组b的倒数第二位上的所有元素的乘积和： numpy.dot(a, b, out=None) 参数说明： a : ndarray 数组 b : ndarray 数组 out : ndarray, 可选，用来保存dot()的计算结果 import numpy.matlib import numpy as np a = np.array([[1,2],[3,4]]) b = np.array([[11,12],[13,14]]) print(np.dot(a,b)) [[37 40] [85 92]] [[1*11+2*13, 1*12+2*14],[3*11+4*13, 3*12+4*14]] numpy.vdot() numpy.vdot() 函数是两个向量的点积。 如果第一个参数是复数，那么它的共轭复数会用于计算。 如果参数是多维数组，它会被展开。 请注意，vdot处理多维数组的方式与dot不同：不是执行矩阵乘积，而是先将输入参数平铺到1-D向量。因此，它应该只用于向量。 >>> a = np.array([1+2j,3+4j]) >>> b = np.array([5+6j,7+8j]) >>> np.vdot(a, b) (70-8j) >>> np.vdot(b, a) (70+8j) numpy.inner() 两个数组的内积。 用于1-D数组（没有复共轭）的向量的普通内积，在较高维度上是最后轴上的和积。 import numpy as np print (np.inner(np.array([1,2,3]),np.array([0,1,0]))) # 等价于 1*0+2*1+3*0 多维数组实例 import numpy as np a = np.array([[1,2], [3,4]]) print ('数组 a：') print (a) 数组 a： [[1 2] [3 4]] b = np.array([[11, 12], [13, 14]]) print ('数组 b：') print (b) 数组 b： [[11 12] [13 14]] print ('内积：') print (np.inner(a,b)) 内积： [[35 41] [81 95]] 内积计算式为： 1*11+2*12, 1*13+2*14 3*11+4*12, 3*13+4*14 矩阵特征值 numpy.linalg.eig numpy.linalg.eig(a) 计算正方形数组的特征值和右特征向量。 参数： a：（...，M，M）数组 将计算特征值和右特征向量的矩阵 返回： w：（...，M）数组 特征值，每个根据其多重性重复。特征值不必是有序的。结果数组将是复杂类型，除非虚部为零，在这种情况下，它将被转换为实数类型。当a是实数时，得到的特征值将是实数（0虚数部分）或出现在共轭对 v：（...，M，M）数组 归一化（单位“长度”）特征向量，使得列v[:,i]是对应于特征值w[i] 当我们想要求解一个非方阵的奇异值之前，我们需要先把这个矩阵转换为方阵。 >>> from numpy import * >>> import numpy as np >>> A = mat([[4,5,6],[1,2,3]]) >>> U = A*A.T >>> lamda,hU=linalg.eig(U) >>> sigma=sqrt(lamda) >>> print sigma [9.508032 0.77286964] 在开头先进行矩阵的乘法，把矩阵和矩阵的转置相乘，得到一个方阵，然后这个方阵作为参数，可以得到特征值和特征向量。 其中返回的第一个值w进行开根号就是data这个矩阵的奇异值。 可以用svd函数来验证一下。 >>> Q,S,VT=linalg.svd(A) >>> print S [9.508032 0.77286964] 参考： numpy.linalg,eig(a)函数 numpy.linalg.solve() numpy.linalg.solve(a,b)函数给出了矩阵形式的线性方程的解。 计算良好确定的，即满秩线性矩阵方程ax = b的“精确”解，x。 a：（...，M，M）array_like 系数矩阵。 b：{（...，M，），（...，M，K）}，array_like 纵坐标或“因变量”值。 返回： x：{（...，M，），（...，M，K）} ndarray a x = b。返回形状与b相同。 求解方程 3 * x0 + x1 = 9和 x0 + 2 x1 = 8 >>> a = np.array([[3,1], [1,2]]) >>> b = np.array([9,8]) >>> x = np.linalg.solve(a, b) >>> x array([ 2., 3.]) numpy.linalg.inv() numpy.linalg.inv(a) 函数计算矩阵的乘法逆矩阵。 逆矩阵（inverse matrix）：设A是数域上的一个n阶矩阵，若在相同数域上存在另一个n阶矩阵B，使得： AB=BA=E ，则我们称B是A的逆矩阵，而A则被称为可逆矩阵。注：E为单位矩阵。 import numpy as np a = np.array([[1,1,1],[0,2,5],[2,5,-1]]) print ('数组 a：') print (a) 数组 a： [[ 1 1 1] [ 0 2 5] [ 2 5 -1]] ainv = np.linalg.inv(a) print ('a 的逆：') print (ainv) a 的逆： [[ 1.28571429 -0.28571429 -0.14285714] [-0.47619048 0.14285714 0.23809524] [ 0.19047619 0.14285714 -0.0952381 ]] print ('矩阵 b：') b = np.array([[6],[-4],[27]]) print (b) 矩阵 b： [[ 6] [-4] [27]] print ('计算：A^(-1)B：') x = np.linalg.solve(a,b) print (x) 计算：A^(-1)B： [[ 5.] [ 3.] [-2.]] # 这就是线性方向 x = 5, y = 3, z = -2 的解 矩阵分解 numpy.linalg.svd函数 奇异值分解 函数：np.linalg.svd(a,full_matrices=1,compute_uv=1) 参数： a 是一个形如(M,N)矩阵 full_matrices 的取值是为0或者1，默认值为1，这时u的大小为(M,M)，v的大小为(N,N) 。否则u的大小为(M,K)，v的大小为(K,N) ，K=min(M,N)。 compute_uv 的取值是为0或者1，默认值为1，表示计算u,s,v。为0的时候只计算s。 返回 总共有三个返回值u,s,v u大小为(M,M)，s大小为(M,N)，v大小为(N,N)。 A = usv 其中s是对矩阵a的奇异值分解。s除了对角元素不为0，其他元素都为0，并且对角元素从大到小排列。s中有n个奇异值，一般排在后面的比较接近0，所以仅保留比较大的r个奇异值。 >>> from numpy import * >>> data = mat([[1,2,3],[4,5,6]]) >>> U,sigma,VT = np.linalg.svd(data) >>> print U [[-0.3863177 -0.92236578] [-0.92236578 0.3863177 ]] >>> print sigma [9.508032 0.77286964] >>> print VT [[-0.42866713 -0.56630692 -0.7039467 ] [ 0.80596391 0.11238241 -0.58119908] [ 0.40824829 -0.81649658 0.40824829]] 有几点需要注意的地方： python 中的 svd 分解得到的 VT 就是 V 的转置，这一点与matlab中不一样，matlab中svd后得到的是V，如果要还原的话还需要将V转置一次，而Python中不需要。 Python 中 svd 后得到的sigma是一个行向量，Python中为了节省空间只保留了A的奇异值，所以我们需要将它还原为奇异值矩阵。同时需要注意的是，比如一个5*5大小的矩阵的奇异值只有两个，但是他的奇异值矩阵应该是5*5的，所以后面的我们需要手动补零，并不能直接使用diag将sigma对角化。 关于奇异值的解释： numpy.linalg.det() numpy.linalg.det() 函数计算输入矩阵的行列式。 行列式在线性代数中是非常有用的值。 它从方阵的对角元素计算。 对于 2×2 矩阵，它是左上和右下元素的乘积与其他两个的乘积的差。 换句话说，对于矩阵[[a，b]，[c，d]]，行列式计算为 ad-bc。 较大的方阵被认为是 2×2 矩阵的组合。 import numpy as np a = np.array([[1,2], [3,4]]) print (np.linalg.det(a)) -2.0 import numpy as np b = np.array([[6,1,1], [4, -2, 5], [2,8,7]]) print (b) print (np.linalg.det(b)) print (6*(-2*7 - 5*8) - 1*(4*7 - 5*2) + 1*(4*8 - -2*2)) [[ 6 1 1] [ 4 -2 5] [ 2 8 7]] -306.0 -306 Update time： 2020-05-25 "},"Chapter1/NumPy 降维和增维.html":{"url":"Chapter1/NumPy 降维和增维.html","title":"NumPy 降维和增维","keywords":"","body":"NumPy 降维和增维 np.newaxis newaxis 表示增加一个新的坐 标轴 import numpy as np a = np.array([1,2,3]) print (a.shape,'\\n',a) 结果为： (3,) [1 2 3] a = np.array([1,2,3])[:,np.newaxis] print (a.shape,'\\n',a) (3, 1) [[1] [2] [3]] 和第一个程序相比，a的shape为（3，）现在为（3，1）变为二维数组了 a = np.array([1,2,3])[np.newaxis,:] print (a.shape,'\\n',a) 输出结果为： (1, 3) [[1 2 3]] 这个和第二个相比，好像和他是反的，相当于转置了，这是因为和[np.newaxis,:] 这个地方np.newaxis 放的位置有关，第二个程序放在[:,]的后面，相当于在原来的后面增加一个维度，所以变为(3,1)，而第三个则放在前面，则为(1,3)。放在前面是先逗号，再冒号，而放在后面是先冒号在逗号，不要弄错了，同时记得是中括号扩起来，不是小括号。 ravel()、flatten()、squeeze() numpy中的ravel()、flatten()、squeeze()都有将多维数组转换为一维数组的功能，区别： ravel()：如果没有必要，不会产生源数据的副本 flatten()：返回源数据的副本 squeeze()：只能对维数为1的维度降维 numpy.ravel(a, order='C') numpy.ravel() vs numpy.flatten() 首先声明两者所要实现的功能是一致的（将多维数组降位一维），两者的区别在于返回拷贝（copy）还是返回视图（view）. numpy.flatten() 返回一份拷贝，对拷贝所做的修改不会影响（reflects）原始矩阵，而numpy.ravel() 返回的是视图（view，也颇有几分C/C++引用reference的意味），会影响（reflects）原始矩阵。 Update time： 2020-05-25 "},"Chapter1/Numpy中reshape函数.html":{"url":"Chapter1/Numpy中reshape函数.html","title":"Numpy中reshape函数","keywords":"","body":"Numpy中reshape函数 一般用法：numpy.arange(n).reshape(a, b); 依次生成n个自然数，并且以a行b列的数组形式显示: In [1]: np.arange(16).reshape(2,8) #生成16个自然数，以2行8列的形式显示 Out[1]: array([[ 0, 1, 2, 3, 4, 5, 6, 7], [ 8, 9, 10, 11, 12, 13, 14, 15]]) 特殊用法: mat (or array).reshape(c, -1); 必须是矩阵格式或者数组格式，才能使用.reshape(c, -1)函数， 表示将此矩阵或者数组重组，以 c行d列的形式表示（-1的作用就在此，自动计算d：d=数组或者矩阵里面所有的元素个数/c, d必须是整数，不然报错）（reshape(-1, e) 即列数固定，行数需要计算）： In [2]: arr=np.arange(16).reshape(2,8) out[2]: In [3]: arr out[3]: array([[ 0, 1, 2, 3, 4, 5, 6, 7], [ 8, 9, 10, 11, 12, 13, 14, 15]]) In [4]: arr.reshape(4,-1) #将arr变成4行的格式，列数自动计算的(c=4, d=16/4=4) out[4]: array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) In [5]: arr.reshape(8,-1) #将arr变成8行的格式，列数自动计算的(c=8, d=16/8=2) out[5]: array([[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7], [ 8, 9], [10, 11], [12, 13], [14, 15]]) In [6]: arr.reshape(10,-1) #将arr变成10行的格式，列数自动计算的(c=10, d=16/10=1.6 != Int) out[6]: ValueError: cannot reshape array of size 16 into shape (10,newaxis) Update time： 2020-05-25 "},"Chapter1/Numpy squeeze函数.html":{"url":"Chapter1/Numpy squeeze函数.html","title":"Numpy squeeze()函数","keywords":"","body":"Numpy squeeze()函数 语法：numpy.squeeze(a,axis = None) 1）a 表示输入的数组； 2）axis 用于指定需要删除的维度，但是指定的维度必须为单维度，否则将会报错； 3）axis的取值可为None 或 int 或 tuple of ints, 可选。若axis为空，则删除所有单维度的条目； 4）返回值： 数组 5) 不会修改原数组； 作用：从数组的形状中删除单维度条目，即把shape中为1的维度去掉 场景：在机器学习和深度学习中，通常算法的结果是可以表示向量的数组（即包含两对或以上的方括号形式[[]]），如果直接利用这个数组进行画图可能显示界面为空（见后面的示例）。我们可以利用squeeze（）函数将表示向量的数组转换为秩为1的数组，这样利用matplotlib库函数画图时，就可以正常的显示结果了。 示例 In [16]: import numpy as np In [17]: a = np.arange(10).reshape(1,10) In [18]: a Out[18]: array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) In [19]: a.shape Out[19]: (1, 10) In [20]: b = np.squeeze(a)^M ...: b Out[20]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) In [21]: b.shape Out[21]: (10,) In [22]: c = np.arange(10).reshape(2,5)^M ...: c Out[22]: array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) In [23]: np.squeeze(c) Out[23]: array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) In [24]: d = np.arange(10).reshape(1,2,5)^M ...: d Out[24]: array([[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]]) In [25]: d.shape Out[25]: (1, 2, 5) In [26]: np.squeeze(d) Out[26]: array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) In [27]: np.squeeze(d).shape Out[27]: (2, 5) np.squeeze()函数可以删除数组形状中的单维度条目，即把shape中为1的维度去掉，但是对非单维的维度不起作用。 matplotlib画图示例 参考 squeeze()函数 Update time： 2020-05-25 "},"Chapter1/Numpy random随机函数.html":{"url":"Chapter1/Numpy random随机函数.html","title":"Numpy random函数","keywords":"","body":"Numpy random函数 numpy.random.RandomState() numpy.random.RandomState() 是一个伪随机数生成器。 伪随机数是用确定性的算法计算出来的似来自[0,1]均匀分布的随机数序列。并不真正的随机，但具有类似于随机数的统计特 征，如均匀性、独立性等。 import numpy as np rng = np.random.RandomState(0) rng.rand(4) # Out[377]: array([0.5488135 , 0.71518937, 0.60276338, 0.54488318]) rng = np.random.RandomState(0) rng.rand(4) # Out[379]: array([0.5488135 , 0.71518937, 0.60276338, 0.54488318]) 0为随机种子，只要随机种子seed相同，产生的随机数序列就相同 因为是伪随机数，所以必须在 rng 这个变量下使用，如果不这样做，那么就得不到相同的随机数组了,即便你再次输入了numpy.random.RandomState()： prng = np.random.RandomState(123456789) # 定义局部种子 prng.rand(2, 4) prng.chisquare(1, size=(2, 2)) # 卡方分布 prng.standard_t(1, size=(2, 3)) # t 分布 prng.poisson(5, size=10) # 泊松分布 numpy.random.seed() 生成随机数的种子，使得每次生成随机数相同 np.random.seed() 的作用：使得随机数据可预测。 当我们设置相同的seed，每次生成的随机数相同。如果不设置seed，则每次会生成不同的 随机数seed( ) 用于指定随机数生成时所用算法开始的整数值，如果使用相同的seed( )值， 则每次生成的随即数都相同，如果不设置这个值，则系统根据时间来自己选择这个值， 此时每次生成的随机数因时间差异而不同。 In [26]: np.random.seed(0) ...: np.random.rand(5) ...: Out[26]: array([ 0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ]) #不同的随机数种子下的rand生成的随机数组的值不等 In [27]: np.random.seed(1000) ...: np.random.rand(5) ...: Out[27]: array([ 0.65358959, 0.11500694, 0.95028286, 0.4821914 , 0.87247454]) #继续输入随机种子为1000下生成的随机数组 In [29]: np.random.seed(1000) ...: np.random.rand(5) ...: Out[29]: array([ 0.65358959, 0.11500694, 0.95028286, 0.4821914 , 0.87247454]) from numpy import * num=0 while(num random.seed(something) 只能是一次有效。 numpy.random.rand() numpy.random.rand(d0,d1...dn)\\ rand 函数根据给定维度生成半开区间 [0,1)之间的数据，包含0，不包含1 dn 表示每个维度 返回值为指定纬度的 numpy.ndarray >>> np.random.rand(3, 3) # shape: 3*3 array([[0.94340617, 0.96183216, 0.88510322], [0.44543261, 0.74930098, 0.73372814], [0.29233667, 0.3940114 , 0.7167332 ]]) from numpy import random x = random.rand(2, 3) print(x) [[ 0.1169922 0.08614147 0.17997144] [ 0.5694889 0.43067372 0.62135592]] x, y = random.rand(2, 3) print(x) print(y) [ 0.60527337 0.78765269 0.71884661] [ 0.67420571 0.946359 0.7632273 ] numpy.random.randint() randint(low[, high, size, dtype]) 从区间[low,high）返回随机整形 参数：low为最小值，high为最大值，size为数组维度大小，dtype为数据类型，默认的- 数据类型是np.int high没有填写时，默认生成随机数的范围是[0，low) raw_user_item_mat = np.random.randint(0, 10, size=(3,4)) #指定生成随机数范围和生成的多维数组大小 print(raw_user_item_mat) [[2 1 3 6] [8 9 1 6] [7 0 1 8]] >>> np.random.randint(1, size = 10) # 返回[0, 1)之间的整数，所以只有0 array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) >>> np.random.randint(1, 5) # 返回[1, 5)之间随机的一个数字 2 np.random.randn() numpy.random.randn(d0,d1,…,dn) randn 函数返回一个或一组样本，具有标准正态分布。 dn 表示每个维度 返回值为指定维度的 numpy.ndarray 数组元素来符合标准正态分布N(0,1) >>> np.random.randn() # 当没有输入参数时，仅返回一个值 -0.7377941002942127 >>> np.random.randn(3, 3) array([[-0.20565666, 1.23580939, -0.27814622], [ 0.53923344, -2.7092927 , 1.27514363], [ 0.38570597, -1.90564739, -0.10438987]]) >>> np.random.randn(3, 3, 3) array([[[ 0.64235451, -1.64327647, -1.27366899], [ 0.69706885, 0.75246699, 2.16235763], [ 1.01141338, -0.19188666, 0.07684428]], [[ 1.34367043, -0.76837057, 0.27803575], [ 0.97007349, 0.41297538, -1.65008923], [-3.78282033, 0.67567421, -0.0753552 ]], [[-0.86540385, 0.14603592, 0.29318291], [-0.8167798 , -0.25492782, -0.58758 ], [ 0.02612474, 0.17882535, -0.95483945]]]) 标准正态分布—-standard normal distribution；标准正态分布又称为u分布，是以0为均值、以1为标准差的正态分布，记为N（0，1）。 numpy.random.choice() numpy.random.choice(a, size=None, replace=True, p=None) 从给定的一位数组中生成一个随机样本 a要求输入一维数组类似数据或者是一个int；size是生成的数组纬度，要求数字或元组；replace为布尔型，决定样本是否有替换；p为样本出现概率,p中概率和必须为1。 #a为整数时，对应的一维数组是np.arange(a) #第一个参数值5对应的a,即传入的数据组 #第二个参数3就是数组的size，传入单值时，数据维度是一维的 #此处将生成一个一维数据包含3个小于5的整数的数组 In [18]: np.random.choice(5,3) Out[18]: array([2, 0, 3]) #给数组中每个数据出现的概率赋值 In [19]: np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0]) Out[19]: array([3, 3, 0], dtype=int64) #repalce参数为是否可以重复，当设置为FALSE时，不能出现重复的数据 In [21]: np.random.choice(5, 4, replace=False) Out[21]: array([3, 2, 1, 4]) In [22]: np.random.choice(5, 5, replace=False) Out[22]: array([4, 0, 3, 1, 2]) In [23]: np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0]) Out[23]: array([3, 0, 2]) #也可以传入非数字、字符串的数组 In [24]: demo_list = ['lenovo', 'sansumg','moto','xiaomi', 'iphone'] In [25]: np.random.choice(demo_list,size=(3,3)) Out[25]: array([['lenovo', 'sansumg', 'sansumg'], ['sansumg', 'iphone', 'iphone'], ['lenovo', 'lenovo', 'xiaomi']], dtype=' 打乱随机排列 np.random.shuffle(x) 在原数组上进行，改变自身序列，无返回值。 import numpy as np arr = np.arange(10) print(arr) np.random.shuffle(arr) print(arr) [0 1 2 3 4 5 6 7 8 9] [5 6 9 7 3 2 8 4 1 0] 对多维数组进行打乱排列时，默认是列维度。 arr=np.arange(12).reshape(3,4) print(arr) print('******') np.random.shuffle(arr) print(arr) [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] ****** [[ 8 9 10 11] [ 0 1 2 3] [ 4 5 6 7]] np.random.permutation(x) 可直接生成一个随机排列的数组 ar=np.random.permutation(10) print(ar) #[9 6 5 8 4 1 7 2 3 0] 一维数组 ar=np.random.permutation([1, 4, 9, 12, 15]) print(ar) #[15 9 12 4 1] 多维数组 arr=np.arange(9).reshape(3,3) print(arr) print('***********') arr2=np.random.permutation(arr) print(arr2) [[0 1 2] [3 4 5] [6 7 8]] *********** [[3 4 5] [6 7 8] [0 1 2]] 特定分布 numpy.random 能产生特定分布的随机数，如normal分布、uniform分布、poisson分布等 这些函数中前面几个参数是分布函数的参数，最后一个参数是shape 如正态分布normal就是均值和方差，uniform 就是上下界，泊松分布就是λ np.random.uniform np.random.uniform(low=0.0, high=1.0, size=None) 作用：可以生成 [low,high) 中的随机数，可以是单个值，也可以是一维数组，也可以是多维数组 参数介绍： low : float 型，或者是数组类型的，默认为 0 high: float 型，或者是数组类型的，默认为 1 size: int 型，或元组，默认为空 In[1]: import numpy as np In[2]: np.random.uniform() # 默认为0到1 Out[2]: 0.827455693512018 In[3]: np.random.uniform(1,5) Out[3]: 2.93533586182789 In[4]: np.random.uniform(1,5,4) #生成一维数组 Out[4]: array([ 3.18487512, 1.40233721, 3.17543152, 4.06933042]) In[5]: np.random.uniform(1,5,(4,3)) #生成4x3的数组 Out[5]: array([[ 2.33083328, 1.592934 , 2.38072 ], [ 1.07485686, 4.93224857, 1.42584919], [ 3.2667912 , 4.57868281, 1.53218578], [ 4.17965117, 3.63912616, 2.83516143]]) In[6]: np.random.uniform([1,5],[5,10]) Out[6]: array([ 2.74315143, 9.4701426 ]) numpy.random.normal 从正态（高斯）分布绘制随机样本。 numpy.random.normal(loc=0.0, scale=1.0, size=None) loc：float此概率分布的均值（对应着整个分布的中心centre） scale：float此概率分布的标准差（对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高） size：int or tuple of ints输出的shape，默认为None，只输出一个值 参考 随机抽样 从np.random.normal()到正态分布的拟 Update time： 2020-06-11 "},"Chapter1/Numpy 字符串操作.html":{"url":"Chapter1/Numpy 字符串操作.html","title":"Numpy 字符串操作","keywords":"","body":"Numpy 字符串操作 参考： 字符串操作 Update time： 2020-05-25 "},"Chapter1/Numpy mgrid和meshgrid的区别和使用.html":{"url":"Chapter1/Numpy mgrid和meshgrid的区别和使用.html","title":"Numpy mgrid()和meshgrid()的区别和使用","keywords":"","body":"Numpy mgrid()和meshgrid()的区别和使用 mgrid 用法：返回多维结构，常见的如2D图形，3D图形。对比np.meshgrid，在处理大数据时速度更快，且能处理多维（np.meshgrid只能处理2维） ret = np.mgrid[ 第1维，第2维 ，第3维 ， …] 返回多值，以多个矩阵的形式返回，第1返回值为第1维数据在最终结构中的分布，第2返回值为第2维数据在最终结构中的分布，以此类推。（分布以矩阵形式呈现） 例如 np.mgrid[X , Y] 样本（i，j）的坐标为 （X[i，j] ,Y[i，j]）,X代表第1维，Y代表第2维，在此例中分别为横纵坐标。 mgrid[[1:3:3j, 4:5:2j]] 3j：3个点 步长为复数表示点数，左闭右闭 步长为实数表示间隔，左闭右开 例如1D结构（array），如下： In [2]: import numpy as np In [3]: pp=np.mgrid[-5:5:5j] In [4]: pp Out[4]: array([-5. , -2.5, 0. , 2.5, 5. ]) 例如2D结构 (2D矩阵)，如下： >>> pp = np.mgrid[-1:1:2j,-2:2:3j] >>> x , y = pp >>> x array([[-1., -1., -1.], [ 1., 1., 1.]]) >>> y array([[-2., 0., 2.], [-2., 0., 2.]]) 例如3D结构 (3D立方体)，如下： >>> pp = np.mgrid[-1:1:2j,-2:2:3j,-3:3:5j] >>> print pp [[[[-1. -1. -1. -1. -1. ] [-1. -1. -1. -1. -1. ] [-1. -1. -1. -1. -1. ]] [[ 1. 1. 1. 1. 1. ] [ 1. 1. 1. 1. 1. ] [ 1. 1. 1. 1. 1. ]]] [[[-2. -2. -2. -2. -2. ] [ 0. 0. 0. 0. 0. ] [ 2. 2. 2. 2. 2. ]] [[-2. -2. -2. -2. -2. ] [ 0. 0. 0. 0. 0. ] [ 2. 2. 2. 2. 2. ]]] [[[-3. -1.5 0. 1.5 3. ] [-3. -1.5 0. 1.5 3. ] [-3. -1.5 0. 1.5 3. ]] [[-3. -1.5 0. 1.5 3. ] [-3. -1.5 0. 1.5 3. ] [-3. -1.5 0. 1.5 3. ]]]] meshgrid meshgrid函数通常使用在数据的矢量化上。 它适用于生成网格型数据，可以接受两个一维数组生成两个二维矩阵，对应两个数组中所有的 (x,y) 对。 In [4]: import numpy as np In [5]: xnums = np.arange(4) In [6]: ynums = np.arange(5) In [7]: xnums Out[7]: array([0, 1, 2, 3]) In [8]: ynums Out[8]: array([0, 1, 2, 3, 4]) In [9]: data_list = np.meshgrid(xnums, ynums) In [10]: data_list Out[10]: [array([[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]), array([[0, 0, 0, 0], [1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]])] In [11]: x, y = data_list In [12]: x.shape Out[12]: (5, 4) In [13]: y.shape Out[13]: (5, 4) In [14]: x Out[14]: array([[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]) In [15]: y Out[15]: array([[0, 0, 0, 0], [1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]]) meshgrid的作用是： 根据传入的两个一维数组参数生成两个数组元素的列表。 如果 第一个参数是 xarray，维度是 xdimesion， 第二个参数是 yarray，维度是y dimesion。 那么生成的第一个二维数组是以xarray为行，共 ydimesion 行的向量； 而第二个二维数组是以 yarray 的转置为列，共 xdimesion 列的向量。 meshgrid 和 mgrid 的区别 参考 Python的 numpy中 meshgrid 和 mgrid 的区别和使用 Update time： 2020-05-25 "},"Chapter1/Numpy 数组填充np.pad函数.html":{"url":"Chapter1/Numpy 数组填充np.pad函数.html","title":"Numpy 数组填充np.pad()函数","keywords":"","body":"Numpy 数组填充np.pad()函数 np.pad()函数 pad(array, pad_width, mode, **kwargs) 返回值：数组 参数 array——表示需要填充的数组； pad_width——表示每个轴（axis）边缘需要填充的数值数目。 参数输入方式为：（(before_1, after_1), … (before_N, after_N)），其中(before_1, after_1)表示第1轴两边缘分别填充before_1个和after_1个数值。取值为：{sequence, array_like, int} mode——表示填充的方式（取值：str字符串或用户提供的函数）,总共有11种填充模式； ‘constant’——表示连续填充相同的值，每个轴可以分别指定填充值，constant_values=（x, y）时前面用x填充，后面用y填充，缺省值填充0 ‘edge’——表示用边缘值填充 ‘linear_ramp’——表示用边缘递减的方式填充 ‘maximum’——表示最大值填充 ‘mean’——表示均值填充 ‘median’——表示中位数填充 ‘minimum’——表示最小值填充 ‘reflect’——表示对称填充 ‘symmetric’——表示对称填充 ‘wrap’——表示用原数组后面的值填充前面，前面的值填充后面 常数填充模式constant A = np.arange(95,99).reshape(2,2) #原始输入数组 A array([[95, 96], [97, 98]]) 用例1 在数组 A 的边缘填充 constant_values 指定的数值 （3,2）表示在A的第[0]轴填充（二维数组中，0轴表示行），即在0轴前面填充3个宽度的0，比如数组A中的95,96两个元素前面各填充了3个0；在后面填充2个0，比如数组A中的97,98两个元素后面各填充了2个0 （2,3）表示在A的第[1]轴填充（二维数组中，1轴表示列），即在1轴前面填充2个宽度的0，后面填充3个宽度的0 np.pad(A,((3,2),(2,3)),'constant',constant_values = (0,0)) #constant_values表示填充值，且(before，after)的填充值等于（0,0） array([[ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 95, 96, 0, 0, 0], [ 0, 0, 97, 98, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0]]) #填充时，从前面轴，往后面轴依次填充 np.pad(A,((3,2),(2,3)),'constant',constant_values = (-2,2)) #填充值，前面填充改为-2，后面填充改为2 array([[-2, -2, -2, -2, 2, 2, 2], [-2, -2, -2, -2, 2, 2, 2], [-2, -2, -2, -2, 2, 2, 2], [-2, -2, 95, 96, 2, 2, 2], [-2, -2, 97, 98, 2, 2, 2], [-2, -2, 2, 2, 2, 2, 2], [-2, -2, 2, 2, 2, 2, 2]]) np.pad(A,((3,2),(2,3)),'constant',constant_values = ((0,0),(1,2))) #0轴和1轴分别填充不同的值，先填充0轴，后填充1轴，存在1轴填充覆盖0轴填充的情形 array([[ 1, 1, 0, 0, 2, 2, 2], [ 1, 1, 0, 0, 2, 2, 2], [ 1, 1, 0, 0, 2, 2, 2], [ 1, 1, 95, 96, 2, 2, 2], [ 1, 1, 97, 98, 2, 2, 2], [ 1, 1, 0, 0, 2, 2, 2], [ 1, 1, 0, 0, 2, 2, 2]]) np.pad(A,((3,2),(2,3)),'constant') #,constant_values 缺省，则默认填充均为0 array([[ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 95, 96, 0, 0, 0], [ 0, 0, 97, 98, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0]]) 边缘值填充模式——’edge’ B = np.arange(1,5).reshape(2,2) #原始输入数组 B array([[1, 2], [3, 4]]) np.pad(B,((1,2),(2,1)),'edge') #注意先填充0轴，后面填充1轴，依次填充 array([[1, 1, 1, 2, 2], [1, 1, 1, 2, 2], [3, 3, 3, 4, 4], [3, 3, 3, 4, 4], [3, 3, 3, 4, 4]]) 边缘最大值填充模式——’maximum’ B = np.arange(1,5).reshape(2,2) #原始输入数组 B array([[1, 2], [3, 4]]) np.pad(B,((1,2),(2,1)),'maximum') #maximum填充模式还有其他控制参数，比如stat_length，详细见numpy库 array([[4, 4, 3, 4, 4], [2, 2, 1, 2, 2], [4, 4, 3, 4, 4], [4, 4, 3, 4, 4], [4, 4, 3, 4, 4]]) C = np.arange(0,9).reshape(3,3) #原始输入数组 C array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) np.pad(C,((3,2),(2,1)),'maximum') array([[8, 8, 6, 7, 8, 8], [8, 8, 6, 7, 8, 8], [8, 8, 6, 7, 8, 8], [2, 2, 0, 1, 2, 2], [5, 5, 3, 4, 5, 5], [8, 8, 6, 7, 8, 8], [8, 8, 6, 7, 8, 8], [8, 8, 6, 7, 8, 8]]) 参考 Numpy学习——数组填充np.pad()函数的应用 Update time： 2020-05-25 "},"Chapter1/Numpy 数学函数.html":{"url":"Chapter1/Numpy 数学函数.html","title":"Numpy 数学函数","keywords":"","body":"Numpy 数学函数 一元的数学函数 abs/fabs：计算整数、浮点数或者复数的绝对值。对于非复数值，可以使用更快的fabs sqrt ：计算平方根，相当于a**0.5 square：计算平方，相当于a**2 exp：计算指数 log/log10/log2/log1p：分别为 sign：计算 ceil：计算各元素的ceiling值：大于等于该值的最小整数 floor：计算个元素的floor值：小于等于该值的最大整数 rint：将各元素四舍五入到最接近的整数，保留dtype modf：将数组的小数和整数部分以两个独立数组的形式返回 isnan：返回一个布尔数组，该数组指示那些是NaN isfinite/isinf：返回一个布尔数组，该数组指示哪些是有限的/无限数 cos/cosh/sin/sinh/tan/tanh：普通和双曲型三角函数 arccos/arcsosh/arcsin/arcsinh/arctan/arctanh:反三角函数 Update time： 2020-07-06 "},"Chapter1/numpy nan_to_num数.html":{"url":"Chapter1/numpy nan_to_num数.html","title":"numpy.nan_to_num","keywords":"","body":"numpy.nan_to_num numpy.nan_to_num(x, copy=True, nan=0.0, posinf=None, neginf=None) 将NaN替换为零，并用大的有限数（默认行为）或用户使用NaN、posinf和/或neginf关键字定义的数字替换NaN。 Parameters x :scalar or array_like Input data. copybool, optional Whether to create a copy of x (True) or to replace values in-place (False). The in-place operation only occurs if casting to an array does not require a copy. Default is True. New in version 1.13. nan: int, float, optional Value to be used to fill NaN values. If no value is passed then NaN values will be replaced with 0.0. posinf: int, float, optional Value to be used to fill positive infinity values. If no value is passed then positive infinity values will be replaced with a very large number. neginf: int, float, optional Value to be used to fill negative infinity values. If no value is passed then negative infinity values will be replaced with a very small (or negative) number. Examples np.nan_to_num(np.inf) 1.7976931348623157e+308 np.nan_to_num(-np.inf) -1.7976931348623157e+308 np.nan_to_num(np.nan) 0.0 x = np.array([np.inf, -np.inf, np.nan, -128, 128]) np.nan_to_num(x) array([ 1.79769313e+308, -1.79769313e+308, 0.00000000e+000, # may vary -1.28000000e+002, 1.28000000e+002]) np.nan_to_num(x, nan=-9999, posinf=33333333, neginf=33333333) array([ 3.3333333e+07, 3.3333333e+07, -9.9990000e+03, -1.2800000e+02, 1.2800000e+02]) y = np.array([complex(np.inf, np.nan), np.nan, complex(np.nan, np.inf)]) array([ 1.79769313e+308, -1.79769313e+308, 0.00000000e+000, # may vary -1.28000000e+002, 1.28000000e+002]) np.nan_to_num(y) array([ 1.79769313e+308 +0.00000000e+000j, # may vary 0.00000000e+000 +0.00000000e+000j, 0.00000000e+000 +1.79769313e+308j]) np.nan_to_num(y, nan=111111, posinf=222222) array([222222.+111111.j, 111111. +0.j, 111111.+222222.j]) Update time： 2020-09-07 "},"Chapter2/":{"url":"Chapter2/","title":"Pandas","keywords":"","body":" 主要参考： [x] pandas documentation Update time： 2020-08-06 "},"Chapter2/Series cat.html":{"url":"Chapter2/Series cat.html","title":"Series.cat","keywords":"","body":"Series.cat Series.cat() Series数据类型：Category Examples 转化为 Category 类型 s = pd.Series(list(\"abbccc\")).astype(\"category\") s 0 a 1 b 2 b 3 c 4 c 5 c dtype: category Categories (3, object): ['a', 'b', 'c'] 查看类别 s.cat.categories Index(['a', 'b', 'c'], dtype='object') 按照先后顺序，重新命名 s.cat.rename_categories(list(\"cba\")) 0 c 1 b 2 b 3 a 4 a 5 a dtype: category Categories (3, object): ['c', 'b', 'a'] 增加类别： s.cat.add_categories([\"d\", \"e\"]) 0 a 1 b 2 b 3 c 4 c 5 c dtype: category Categories (5, object): ['a', 'b', 'c', 'd', 'e'] 删除列别 s.cat.remove_categories([\"a\", \"c\"]) 0 NaN 1 b 2 b 3 NaN 4 NaN 5 NaN dtype: category Categories (1, object): ['b'] 删除无用得类别： s1 = s.cat.add_categories([\"d\", \"e\"]) s1.cat.remove_unused_categories() 0 a 1 b 2 b 3 c 4 c 5 c dtype: category Categories (3, object): ['a', 'b', 'c'] 案例 增加类别，并将缺失值填充为新得类别 if Train_data[c].isnull().any(): Train_data[c] = Train_data[c].cat.add_categories([\"MISSING\"]) Train_data[c] = Train_data[c].fillna(\"MISSING\") Update time： 2020-09-07 "},"Chapter2/Pandas melt.html":{"url":"Chapter2/Pandas melt.html","title":"Pandas.melt","keywords":"","body":"Pandas.melt 数据分析的时候经常要把宽数据--->>长数据，有点像你们用excel 做透视跟逆透视的过程，直接看下面例子，希望有助于理解. pandas.melt 使用参数： pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None) 参数解释： frame:要处理的数据集。 id_vars:不需要被转换的列名。 value_vars:需要转换的列名，如果剩下的列全部都要转换，就不用写了。 var_name和value_name是自定义设置对应的列名。 col_level :如果列是MultiIndex，则使用此级别。 （问题来了：如果某些列没有包含在id_vars和value_vars中会怎么样呢？ 答:这些列的内容会被忽略） Returns DataFrame Examples df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'}, 'B': {0: 1, 1: 3, 2: 5}, 'C': {0: 2, 1: 4, 2: 6}}) df A B C 0 a 1 2 1 b 3 4 2 c 5 6 转化 B 列 pd.melt(df, id_vars=['A'], value_vars=['B']) A variable value 0 a B 1 1 b B 3 2 c B 5 转化 B，C 列 pd.melt(df, id_vars=['A'], value_vars=['B', 'C']) A variable value 0 a B 1 1 b B 3 2 c B 5 3 a C 2 4 b C 4 5 c C 6 The names of ‘variable’ and ‘value’ columns can be customized: pd.melt(df, id_vars=['A'], value_vars=['B'], var_name='myVarname', value_name='myValname') A myVarname myValname 0 a B 1 1 b B 3 2 c B 5 将数据转化，结合 seaborn 的 FacetGrid() 绘制组合图 categorical_features = [\"bodyType\", \"fuelType\", \"gearbox\", \"notRepairedDamage\"] data = Train_data[categorical_features] f = pd.melt(data, value_vars=categorical_features) f 将其转化为两列，根据不同的标签绘制柱状图（分类，数量统计） def count_plot(x, **kwargs): sns.countplot(x=x) x = plt.xticks(rotation=90) g = sns.FacetGrid(f, col=\"variable\", col_wrap=2, size=6, sharex=False, sharey=False) g = g.map(count_plot, \"value\") Update time： 2020-09-07 "},"Chapter2/Pandas set_option.html":{"url":"Chapter2/Pandas set_option.html","title":"Pandas set_option","keywords":"","body":"Pandas set_option pandas.set_option(pat, value) = 设置指定选项的值。 set_option需要两个参数，并将该值设置为指定的参数值，如下所示： 参数 display.max_categories ：int This sets the maximum number of categories pandas should output when printing out a Categorical or a Series of dtype “category”. [default: 8] [currently: 8] display.max_columns ：int 显示的最大列数 display.max_rows ：int 显示的最大行数 display.min_rows ：int display.precision ： int Floating point output precision (number of significant digits). This is only a suggestion [default: 6] [currently: 6] 设置十进制数显示的精度 display.float_format ：callable 设置浮点数显示的格式 display.max_colwidth ：int or None 显示最大列宽设置 用pandas展示数据输出时列名不能对齐 列名用了中文的缘故，设置pandas的参数即可，代码如下： import pandas as pd #这两个参数的默认设置都是False pd.set_option('display.unicode.ambiguous_as_wide', True) pd.set_option('display.unicode.east_asian_width', True) pd.set_option(\"display.max_rows\",80) pd.set_option(\"display.max_columns\",32) 设置浮点数显示的精度 你也可以重置任何一个选项为其默认值： 对于其它的选项也是类似的使用方法。 Update time： 2020-08-18 "},"Chapter2/Pandas 数据结构Series,DataFrame.html":{"url":"Chapter2/Pandas 数据结构Series,DataFrame.html","title":"Pandas 数据结构Series,DataFrame","keywords":"","body":"Pandas 数据结构Series,DataFrame pandas 是基于 numpy 构建的，为时间序列分析提供了很好的支持。pandas中有两个主要的数据结构，一个是Series，另一个是DataFrame。 Series Series 类似于一维数组与字典(map)数据结构的结合。它由一组数据和一组与数据相对应的数据标签（索引index）组成。这组数据和索引标签的基础都是一个一维ndarray数组。可将index索引理解为行索引。 Series 的表现形式为：索引在左，数据在右。 pd.Series(data, index=index) data 支持以下数据类型： Python 字典 多维数组 标量值（如，5） index 是轴标签列表。不同数据可分为以下几种情况： 多维数组 data 是多维数组时，index 长度必须与 data 长度一致。没有指定 index 参数时，创建数值型索引，即 [0, ..., len(data) - 1]。 In [3]: s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e']) In [4]: s Out[4]: a 0.469112 b -0.282863 c -1.509059 d -1.135632 e 1.212112 dtype: float64 In [5]: s.index Out[5]: Index(['a', 'b', 'c', 'd', 'e'], dtype='object') In [6]: pd.Series(np.random.randn(5)) Out[6]: 0 -0.173215 1 0.119209 2 -1.044236 3 -0.861849 4 -2.104569 dtype: float64 字典 Series 可以用字典实例化： In [7]: d = {'b': 1, 'a': 0, 'c': 2} In [8]: pd.Series(d) Out[8]: b 1 a 0 c 2 dtype: int64 data 为字典，且未设置 index 参数时，如果 Python 版本 >= 3.6 且 Pandas 版本 >= 0.23，Series 按字典的插入顺序排序索引。 Python index 参数时，Series 按字母顺序排序字典的键（key）列表。 Pandas 用 NaN（Not a Number）表示缺失数据。 标量值 data 是标量值时，必须提供索引。Series 按索引长度重复该标量值。 In [12]: pd.Series(5., index=['a', 'b', 'c', 'd', 'e']) Out[12]: a 5.0 b 5.0 c 5.0 d 5.0 e 5.0 dtype: float64 Series 类似多维数组 Series 操作与 ndarray 类似，支持大多数 NumPy 函数，还支持索引切片 In [13]: s[0] Out[13]: 0.4691122999071863 In [14]: s[:3] Out[14]: a 0.469112 b -0.282863 c -1.509059 dtype: float64 In [15]: s[s > s.median()] Out[15]: a 0.469112 e 1.212112 dtype: float64 In [16]: s[[4, 3, 1]] Out[16]: e 1.212112 d -1.135632 b -0.282863 dtype: float64 In [17]: np.exp(s) Out[17]: a 1.598575 b 0.753623 c 0.221118 d 0.321219 e 3.360575 dtype: float64 获取索引和数据：ser_obj.index, ser_obj.values 预览数据：ser_obj.head(n), ser_obj.tail(n) 使用一维数组生成 Series DataFrame DataFrame 是由多种类型的列构成的二维标签数据结构，类似于 Excel 、SQL 表，或 Series 对象构成的字典。DataFrame 是最常用的 Pandas 对象，与 Series 一样，DataFrame 支持多种类型的输入数据： 一维 ndarray、列表、字典、Series 字典 二维 numpy.ndarray 结构多维数组或记录多维数组 Series DataFrame 除了数据，还可以有选择地传递 index（行标签）和 columns（列标签）参数。传递了索引或列，就可以确保生成的 DataFrame 里包含索引或列。Series 字典加上指定索引时，会丢弃与传递的索引不匹配的所有数据。 Python > = 3.6，且 Pandas > = 0.23，数据是字典，且未指定 columns 参数时，DataFrame 的列按字典的插入顺序排序. 用 Series 字典或字典生成 DataFrame 生成的索引是每个 Series 索引的并集。先把嵌套字典转换为 Series。如果没有指定列，DataFrame 的列就是字典键的有序列表。 In [37]: d = {'one': pd.Series([1., 2., 3.], index=['a', 'b', 'c']), ....: 'two': pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])} ....: In [38]: df = pd.DataFrame(d) In [39]: df Out[39]: one two a 1.0 1.0 b 2.0 2.0 c 3.0 3.0 d NaN 4.0 In [40]: pd.DataFrame(d, index=['d', 'b', 'a']) Out[40]: one two d NaN 4.0 b 2.0 2.0 a 1.0 1.0 In [41]: pd.DataFrame(d, index=['d', 'b', 'a'], columns=['two', 'three']) Out[41]: two three d 4.0 NaN b 2.0 NaN a 1.0 NaN index 和 columns 属性分别用于访问行、列标签. In [42]: df.index Out[42]: Index(['a', 'b', 'c', 'd'], dtype='object') In [43]: df.columns Out[43]: Index(['one', 'two'], dtype='object') 用多维数组字典、列表字典生成 DataFrame 多维数组的长度必须相同。如果传递了索引参数，index 的长度必须与数组一致。如果没有传递索引参数，生成的结果是 range(n)，n 为数组长度。 In [44]: d = {'one': [1., 2., 3., 4.], ....: 'two': [4., 3., 2., 1.]} ....: In [45]: pd.DataFrame(d) Out[45]: one two 0 1.0 4.0 1 2.0 3.0 2 3.0 2.0 3 4.0 1.0 In [46]: pd.DataFrame(d, index=['a', 'b', 'c', 'd']) Out[46]: one two a 1.0 4.0 b 2.0 3.0 c 3.0 2.0 d 4.0 1.0 用结构多维数组或记录多维数组生成 DataFrame In [47]: data = np.zeros((2, ), dtype=[('A', 'i4'), ('B', 'f4'), ('C', 'a10')]) In [48]: data[:] = [(1, 2., 'Hello'), (2, 3., \"World\")] In [49]: pd.DataFrame(data) Out[49]: A B C 0 1 2.0 b'Hello' 1 2 3.0 b'World' In [50]: pd.DataFrame(data, index=['first', 'second']) Out[50]: A B C first 1 2.0 b'Hello' second 2 3.0 b'World' In [51]: pd.DataFrame(data, columns=['C', 'A', 'B']) Out[51]: C A B 0 b'Hello' 1 2.0 1 b'World' 2 3.0 用列表字典生成 DataFrame In [52]: data2 = [{'a': 1, 'b': 2}, {'a': 5, 'b': 10, 'c': 20}] In [53]: pd.DataFrame(data2) Out[53]: a b c 0 1 2 NaN 1 5 10 20.0 In [54]: pd.DataFrame(data2, index=['first', 'second']) Out[54]: a b c first 1 2 NaN second 5 10 20.0 In [55]: pd.DataFrame(data2, columns=['a', 'b']) Out[55]: a b 0 1 2 1 5 10 备选构建器 DataFrame.from_dict DataFrame.from_dict( data, orient ='columns', dtype = None, columns = None ) data:字典形式为{field：array-like}或{field：dict}。 orient:{‘columns’，‘index’}，默认’列’数据的“方向”。 如果传递的dict的键应该是结果DataFrame的列，则传递’columns’（默认值）。 否则，如果键应该是行，则传递’index’。 dtype：dtype:默认无数据类型强制，否则推断。 columns：list默认无要使用的列标签orient=‘index’。 如果使用，则引发ValueError orient=‘columns’。 DataFrame.from_dict 接收字典组成的字典或数组序列字典，并生成 DataFrame。除了 orient 参数默认为 columns，本构建器的操作与 DataFrame 构建器类似。把 orient 参数设置为 'index'， 即可把字典的键作为行标签。 In [57]: pd.DataFrame.from_dict(dict([('A', [1, 2, 3]), ('B', [4, 5, 6])])) Out[57]: A B 0 1 4 1 2 5 2 3 6 orient='index' 时，键是行标签。本例还传递了列名： In [58]: pd.DataFrame.from_dict(dict([('A', [1, 2, 3]), ('B', [4, 5, 6])]), ....: orient='index', columns=['one', 'two', 'three']) ....: Out[58]: one two three A 1 2 3 B 4 5 6 Update time： 2020-05-25 "},"Chapter2/Pandas 数据获取与查询.html":{"url":"Chapter2/Pandas 数据获取与查询.html","title":"Pandas 数据获取与查询","keywords":"","body":"Pandas 数据获取与查询 数据的查看 import numpy as np import pandas as pd from pandas import Sereis, DataFrame ser = Series(np.arange(3.)) data = DataFrame(np.arange(16).reshape(4,4),index=list('abcd'),columns=list('wxyz')) data['w'] #选择表格中的'w'列，使用类字典属性,返回的是Series类型 data.w #选择表格中的'w'列，使用点属性,返回的是Series类型 data[['w']] #选择表格中的'w'列，返回的是DataFrame类型 data[['w','z']] #选择表格中的'w'、'z'列 data[0:2] #返回第1行到第2行的所有行，前闭后开，包括前不包括后 data[1:2] #返回第2行，从0计，返回的是单行，通过有前后值的索引形式， #如果采用data[1]则报错 data.ix[1:2] #返回第2行的第三种方法，返回的是DataFrame，跟data[1:2]同 data['a':'b'] #利用index值进行切片，返回的是**前闭后闭**的DataFrame, #即末端是包含的 data.head() #返回data的前几行数据，默认为前五行，需要前十行则data.head(10) data.tail() #返回data的后几行数据，默认为后五行，需要后十行则data.tail(10) data.iloc[-1] #选取DataFrame最后一行，返回的是Series data.iloc[-1:] #选取DataFrame最后一行，返回的是DataFrame data.loc['a',['w','x']] #返回‘a’行'w'、'x'列，这种用于选取行索引列索引已知 data.iat[1,1] #选取第二行第二列，用于已知行、列位置的选取。 以属性方式访问，对DataFrame是列名，对Series是键。例如df.A, df.A.a .at,.iat和.get_value，获取单个值，注意两者的区别。df.at[‘a’,‘A’], df.get_value(‘a’,‘A’)。他们等价于df.loc[‘a’,‘A’]。这个比采用[]速度要快，遍历的时候推荐用这个。 df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]], columns=['A', 'B', 'C']) print(df) ''' A B C 0 0 2 3 1 0 4 1 2 10 20 30 ''' print(df.at[2, 'B']) #20 print(df.at[2, 2]) #ValueError: At based indexing on an non-integer index can only have non-integer indexers print(df.get_value(2,'B')) #20 print(df.get_value(2,2)) #只能根据标签索引 Update time： 2020-05-25 "},"Chapter2/Pandas isin索引和~反向索引.html":{"url":"Chapter2/Pandas isin索引和~反向索引.html","title":"Pandas isin索引和~反向索引","keywords":"","body":"Pandas isin索引和~反向索引 isin 1、直接根据条件进行索引，isin()接受一个列表，判断该列中元素是否在列表中 DataFrame.isin(values) df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]}, index=['falcon', 'dog']) df num_legs num_wings falcon 2 2 dog 4 0 # 当values是一个列表时，检查DataFrame中的每个值是否都存在于列表中 df.isin([0, 2]) num_legs num_wings falcon True True dog False True 2、通过字典的形式传递多个条件{‘某列’:[条件],‘某列’:[条件],} 当values是dict时，我们可以传递值来分别检查每个列： df.isin({'num_wings': [0, 3]}) num_legs num_wings falcon False False dog False True 3、When values is a Series or DataFrame the index and column must match 索引行和列必须同时匹配 other = pd.DataFrame({'num_legs': [8, 2], 'num_wings': [0, 2]}, index=['spider', 'falcon']) df.isin(other) num_legs num_wings falcon True True dog False False 4、根据多条件进行索引，此时用&（交集）或者|（并集）进行连接 import numpy as np import pandas as pd df=pd.DataFrame(np.random.randn(4,4),columns=['A','B','C','D']) df Out[189]: A B C D 0 0.289595 0.202207 -0.850390 0.197016 1 0.403254 -1.287074 0.916361 0.055136 2 -0.359261 -1.266615 -0.733625 -0.790208 3 0.164862 -0.649637 0.716620 1.447703 df['E'] = ['aa', 'bb', 'cc', 'cc'] df Out[191]: A B C D E 0 0.289595 0.202207 -0.850390 0.197016 aa 1 0.403254 -1.287074 0.916361 0.055136 bb 2 -0.359261 -1.266615 -0.733625 -0.790208 cc 3 0.164862 -0.649637 0.716620 1.447703 cc df[df.E.isin(['aa'])|df.E.isin(['cc'])] Out[194]: A B C D E 0 0.289595 0.202207 -0.850390 0.197016 aa 2 -0.359261 -1.266615 -0.733625 -0.790208 cc 3 0.164862 -0.649637 0.716620 1.447703 cc df[df.E.isin(['aa'])] Out[195]: A B C D E 0 0.289595 0.202207 -0.85039 0.197016 aa ~相当于 isnotin df[~(df.E=='cc')] Out[202]: A B C D E 0 0.289595 0.202207 -0.850390 1 aa 1 0.403254 -1.287074 0.916361 2 bb Update time： 2020-08-06 "},"Chapter2/Pandas ix 、loc 、 iloc区别.html":{"url":"Chapter2/Pandas ix 、loc 、 iloc区别.html","title":"Pandas ix 、loc 、 iloc区别","keywords":"","body":"Pandas ix 、loc 、 iloc区别 loc loc——通过行标签索引行数据 loc[1]表示索引的是第1行（ index 是整数） .loc主要是基于标签(label)的，包括行标签(index)和列标签(columns)，即行名称和列名称，可以使用df.loc[index_name,col_name]，选择指定位置的数据 使用单个标签 如果.loc[]中只有单个标签，那么选择的是某一行。 df.loc[1]选择的是 index 名为 ‘1’ 的一行，注意这里的’1’是index的名称，而不是序号 data = [[1,2,3],[4,5,6]] index = [0,1] columns=['a','b','c'] df = pd.DataFrame(data=data, index=index, columns=columns) df a b c 0 1 2 3 1 4 5 6 # 选取行号为 1 的 index df.loc[1] a 4 b 5 c 6 Name: 1, dtype: int64 使用标签的list：同样是只选择行 df.loc[[0,1]] a b c 0 1 2 3 1 4 5 6 标签的切片对象：与通常的python切片不同，在最终选择的数据中包含切片的start 和 stop df.loc['c':'f'] A B C D c 0.546339 0.009044 0.774770 0.121425 d 0.517394 0.665752 0.762913 0.940879 e 0.123988 0.209996 0.365368 0.401288 f 0.590005 0.993915 0.611509 0.683021 布尔型的数组：通常用于筛选符合某些条件的行 df.loc[df.A > 0.7] A B C D a 0.899727 0.61540 0.970942 0.621620 b 0.936262 0.59904 0.139279 0.678375 A 属性大于 0.7 的所有 C 和 D 列 df.loc[df.A > 0.7,['C', 'D']] C D a 0.970942 0.621620 b 0.139279 0.678375 可调用的函数 df.loc[lambda df : df.A > 0.7] A B C D a 0.899727 0.61540 0.970942 0.621620 b 0.936262 0.59904 0.139279 0.678375 lambda表达式语法： lambda 传入参数 ： 返回的计算表达式 loc扩展——索引某行某列 索引某行某列 data = [[1,2,3],[4,5,6]] index = ['d','e'] columns=['a','b','c'] df = pd.DataFrame(data=data, index=index, columns=columns) print(df) ''' a b c d 1 2 3 e 4 5 6 ''' print (df.loc['d',['b','c']]) ''' b 2 c 3 ''' 索引某列 import pandas as pd data = [[1,2,3],[4,5,6]] index = ['d','e'] columns=['a','b','c'] df = pd.DataFrame(data=data, index=index, columns=columns) print(df) ''' a b c d 1 2 3 e 4 5 6 ''' print (df.loc[:,['c']]) ''' c d 3 e 6 ''' df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], index=['cobra', 'viper', 'sidewinder'], columns=['max_speed', 'shield']) print(df) print(df.loc[['viper', 'sidewinder'],['max_speed', 'shield']]) ''' max_speed shield cobra 1 2 viper 4 5 sidewinder 7 8 max_speed shield viper 4 5 sidewinder 7 8 ''' iloc用法 iloc是基于位置的索引，利用元素在各个轴上的索引序号进行选择，序号超出范围会产生IndexError，切片时允许序号超过范围，用法包括： 使用整数：与.loc相同，如果只使用一个维度，则对行选择，下标从0开始 # 选择第六行数据 df.iloc[5] A 0.590005 B 0.993915 C 0.611509 D 0.683021 Name: f, dtype: float64 使用列表或数组，同样是对行选择 df.iloc[[1,3,4]] A B C D b 0.936262 0.599040 0.139279 0.678375 d 0.517394 0.665752 0.762913 0.940879 e 0.123988 0.209996 0.365368 0.401288 元素为整数的切片对象：与.loc不同的是，这里下标为stop的数据不被选择 df.iloc[0:3] A B C D a 0.899727 0.615400 0.970942 0.621620 b 0.936262 0.599040 0.139279 0.678375 c 0.546339 0.009044 0.774770 0.121425 也可以对列进行切片： df.iloc[0:3,1:3] B C a 0.615400 0.970942 b 0.599040 0.139279 c 0.009044 0.774770 import pandas as pd data = [[1,2,3],[4,5,6]] index = ['d','e'] columns=['a','b','c'] df = pd.DataFrame(data=data, index=index, columns=columns) print(df) ''' a b c d 1 2 3 e 4 5 6 ''' print (df.iloc[:,[1]]) ''' b d 2 e 5 ''' 使用布尔数组进行筛选： 注意这里可以使用 list 或者 array，使用 Series 的话会出错，NotImplementedError或者ValueError，前者是Series的index与待切片DataFrame的index不同时，后者是index相同时报的错，可以自己实现体会一下。与.loc使用布尔数组，可以使用list， array，也可以使用Series，使用Series时index需要一致，否则会报IndexingError df.iloc[np.array(df.A > 0.6)] A B C D a 0.899727 0.61540 0.970942 0.621620 b 0.936262 0.59904 0.139279 0.678375 使用可调用函数 df.iloc[lambda df : [0,1]] # 选择前两行 A B C D a 0.899727 0.61540 0.970942 0.621620 b 0.936262 0.59904 0.139279 0.678375 ix——结合前两种的混合索引 通过行号索引 df.ix[0] A 0.899727 B 0.615400 C 0.970942 D 0.621620 Name: a, dtype: float64 通过行标签索引 df.ix['a'] A 0.899727 B 0.615400 C 0.970942 D 0.621620 Name: a, dtype: float64 切片操作[] []操作只能输入一个维度，不能用逗号隔开输入两个维度： 使用列名：.loc和iloc只输入一维时选取的是行，而[]选取的是列，并且必须使用列名 df[['A','B']] A B a 0.899727 0.615400 b 0.936262 0.599040 c 0.546339 0.009044 d 0.517394 0.665752 e 0.123988 0.209996 f 0.590005 0.993915 用布尔数组：bool 数组的 index 需要和 dataframe 的 index 一致，此时选取的是行 df[df.B > 0.6] A B C D a 0.899727 0.615400 0.970942 0.621620 d 0.517394 0.665752 0.762913 0.940879 f 0.590005 0.993915 0.611509 0.683021 .a、.iat与loc 、iloc类似，.a、loc 、只能通过标签获取值，.iat、iloc通过行号，列号获取值 df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]], columns=['A', 'B', 'C']) print(df) ''' A B C 0 0 2 3 1 0 4 1 2 10 20 30 ''' print(df.at[2, 'B']) #20 print(df.at[2, 2]) #ValueError: At based indexing on an non-integer index can only have non-integer indexers print(df.get_value(2,'B')) #20 print(df.get_value(2,2)) #只能根据标签索引 参考： pandas索引和选择数据 python选取特定列——pandas的iloc和loc以及icol使用 Update time： 2020-05-25 "},"Chapter2/Pandas 数据对齐输出.html":{"url":"Chapter2/Pandas 数据对齐输出.html","title":"Pandas 数据对齐输出","keywords":"","body":"Pandas 数据对齐输出 用pandas展示数据输出时列名不能对齐 列名用了中文的缘故，设置pandas的参数即可，代码如下： import pandas as pd #这两个参数的默认设置都是False pd.set_option('display.unicode.ambiguous_as_wide', True) pd.set_option('display.unicode.east_asian_width', True) Update time： 2020-05-25 "},"Chapter2/Pandas Index对象.html":{"url":"Chapter2/Pandas Index对象.html","title":"Pandas Index对象","keywords":"","body":"Pandas Index对象 Index 对象保存着索引标签数据，它可以快速找到标签对应的整数下标，其功能与Python的字典类似。 dict1={\"Province\":[\"Guangdong\",\"Beijing\",\"Qinghai\",\"Fujiang\"], \"year\":[2018]*4, \"pop\":[1.3,2.5,1.1,0.7]} df1=DataFrame(dict1) print(df1) ''' Province year pop 0 Guangdong 2018 1.3 1 Beijing 2018 2.5 2 Qinghai 2018 1.1 3 Fujiang 2018 0.7 ''' 调用.columns返回DataFrame对象的列索引（即所有列标签）： col_index=df1.columns print(col_index) # Index(['Province', 'year', 'pop'], dtype='object') print(col_index.values) # ['Province' 'year' 'pop'] 调用.index ind_index=df1.index print(ind_index.values) # RangeIndex(start=0, stop=4, step=1) print(ind_index.values) #array([0, 1, 2, 3], dtype=int64) Index 对象可当做一维数组，适合 Numpy 数组的下标运算，但 Index 对象只是可读，创建后不可修改。 print(col_index[[1,2]]) print(ind_index[ind_index>1]) ''' Index(['year', 'pop'], dtype='object') Int64Index([2, 3], dtype='int64') ''' index 对象具有字典的映射功能，.get_loc(value)获得单值得下标，.get_indexer(values) 获得一组值得下标，当值不存在则返回-1： print(col_index.get_loc('pop')) print(col_index.get_indexer(['pop','year'])) ''' 2 [2 1] ''' Index 对象调用 Index()来创建，可传递给 DataFrame 对象的参数 index 和columns。因为 Index 是不可变的，因此多个 DataFrame 对象的索引可以是同个Index对象。 index=pd.Index(['a','b','c']) df2=DataFrame(np.random.randint(1,10,(3,3)),index=index,columns=index) print(df2) ''' a b c a 3 5 3 b 9 2 3 c 6 3 3 ''' Update time： 2020-05-25 "},"Chapter2/Pandas MultiIndex对象.html":{"url":"Chapter2/Pandas MultiIndex对象.html","title":"Pandas MultiIndex对象","keywords":"","body":"Pandas MultiIndex对象 分层/多级索引，因为它为一些非常复杂的数据分析和操作提供了可能性，特别是对于处理更高维度的数据。从本质上讲，它使你能在较低维度的数据结构(如Series（1d）和DataFrame（2d）)中存储和操作具有任意数量维度的数据。 创建MultiIndex MultiIndex 对象是标准Index对象的扩展, 你可以将 MultiIndex 视为元组构成的列表，其中每个元组都是唯一的, 它与Index的区别是, Index 可以视为数字或者字符串构成的列表。可以从数组列表（使用MultiIndex.from_arrays），元组列表（使用MultiIndex.from_tuples）或交叉迭代集（使用MultiIndex.from_product）创建MultiIndex。当构造函数传递元组列表时，它将尝试返回 MultiIndex。以下示例演示了创建 MultiIndexes 的不同方法。 from_tuples 创建一个元祖构成的列表 arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']] tuples = list(zip(*arrays)) print(tuples) ''' [('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')] ''' 使用 from_tuples 来创建 MultiIndex: arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']] tuples = list(zip(*arrays)) print(tuples) ''' [('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')] ''' from_arrays 如果说 from_tuples 接受的参数是”行”的列表, 那么 from_arrays 接受的参数是就是”列”的列表: arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']] index = pd.MultiIndex.from_arrays(arrays) s = pd.Series(np.random.randn(8), index=index) print(s) ''' bar one -0.268434 two -0.026027 baz one -0.658945 two 0.283218 foo one 1.054412 two 1.977076 qux one -0.563627 two -0.156421 dtype: float64 ''' 不过为了简便, 我们通常可以直接在Series的构造函数中使用: arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']] s = pd.Series(np.random.randn(8), index=arrays) print(s) ''' bar one -1.411561 two 0.667025 baz one -0.079449 two -0.715518 foo one -1.844785 two 0.257104 qux one -0.776653 two -1.159284 dtype: float64 ''' from_product 假如我们有两个 list , 这两个 list 内的元素相互交叉, 两两搭配, 这就是两个 list 的product: lists = [['bar', 'baz', 'foo', 'qux'], ['one', 'two']] index = pd.MultiIndex.from_product(lists, names=['first', 'second']) s = pd.Series(np.random.randn(len(index)), index=index) print(s) ''' first second bar one -1.309958 two 0.379085 baz one 0.739266 two -0.165164 foo one -0.022698 two -0.006190 qux one 0.299748 two -0.864639 dtype: float64 ''' MultiIndex.names 你可以为 MultiIndex 的各个层起名字, 这就是 names 属性: print(s.index.names) #['first', 'second'] s.index.names = ['FirstLevel', 'SecondLevel'] print(s.index.names) #['FirstLevel', 'SecondLevel'] MultiIndex可以作为列名称 Series 和 DataFrame 的列名称属性就是columns, 也可以是一个 MultiIndex 对象: df = pd.DataFrame(np.random.randn(3, 8), index=['A', 'B', 'C'], columns=index) print(df) ''' FirstLevel bar baz ... foo qux SecondLevel one two one ... two one two A 1.127256 1.453003 0.025833 ... -1.258916 0.711659 -1.460118 B 1.589573 2.057785 0.028412 ... -1.678021 -0.662083 -1.044907 C -0.879705 0.730224 -0.003296 ... -0.446753 -1.694061 1.440663 [3 rows x 8 columns] ''' 获取各水平的值 方法get_level_values将返回特定级别的每个位置的标签向量： print(index.get_level_values(0)) ''' Index(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], dtype='object', name='FirstLevel') ''' 如果你给 index 设置了名称, 那么你可以直接使用名称来获取水平值: print(index.get_level_values('FirstLevel')) ''' Index(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], dtype='object', name='FirstLevel') ''' 选择数据 这可能是MultiIndex最重要的功能之一。 print(df) ''' FirstLevel bar baz ... foo qux SecondLevel one two one ... two one two A 1.134195 -0.351198 0.218614 ... -0.755923 0.129988 -0.052183 B -0.308971 1.399584 -0.894162 ... -1.022070 0.045872 1.539032 C -0.399760 0.351285 1.069002 ... -0.771437 2.350300 -0.922563 [3 rows x 8 columns] ''' 获取 FirstLevel 是 bar 的所有数据 print(df['bar']) ''' SecondLevel one two A 0.204079 0.635272 B 1.691298 -0.449358 C -1.236855 -1.144767 ''' 获取FirstLevel是bar, SecondLevel是one的所有数据: print(df['bar']['one']) ''' A 0.121725 B 1.130435 C -0.011153 Name: one, dtype: float64 ''' 需要注意的是, 结果选择输出的结果的columns已经改变: print(df['bar'].columns) #Index(['one', 'two'], dtype='object', name='SecondLevel') 如果你要选择第二层的列名为 one 的所有数据, 你需要借助 xs 方法: print(df.xs('bar', level=0, axis=1)) ''' SecondLevel one two A -1.493262 -0.704469 B 3.509268 -1.283146 C -0.056974 0.710284 ''' print(df.xs('one', level=1, axis=1)) ''' FirstLevel bar baz foo qux A 0.537299 -1.054771 0.144664 -0.940659 B -0.560000 -0.952308 -1.667759 -1.480754 C 0.733805 0.930315 -0.144840 -1.261983 ''' 或者使用名称代替数字: print(df.xs('one', level='SecondLevel', axis='columns')) ''' FirstLevel bar baz foo qux A 0.537299 -1.054771 0.144664 -0.940659 B -0.560000 -0.952308 -1.667759 -1.480754 C 0.733805 0.930315 -0.144840 -1.261983 ''' axis, 它不仅可以用来选择列, 也可以用来选择行: print(s) ''' FirstLevel SecondLevel bar one 1.262375 two 0.211678 baz one -0.207278 two 0.386925 foo one -0.010342 two -1.775980 qux one -0.981227 two -2.159368 dtype: float64 ''' print(s.xs('one', level='SecondLevel', axis='index')) ''' FirstLevel bar 0.783906 baz 1.159497 foo -1.361565 qux -0.838874 dtype: float64 ''' 选择行 把 df 进行转置, 然后看看一些选择行的操作: df = df.T print(df) ''' A B C FirstLevel SecondLevel bar one 0.923159 3.412767 -2.821506 two 2.641013 0.229025 1.251807 baz one 0.900035 0.874848 0.246452 two -1.326872 0.647462 -0.088361 foo one 1.730885 -0.732980 -0.373840 two -1.369826 -1.532940 -0.204242 qux one 0.305318 0.929807 -0.331868 two 1.110960 -0.869984 0.990428 ''' 选择 FirstLevel 是 bar, SecondLevel 是 two 的数据: print(df.loc[('bar', 'two')]) ''' A -0.096414 B 0.796829 C -0.571624 Name: (bar, two), dtype: float64 ''' 多重索引的标签要一元组的形式('bar', 'two')，不能是列表的形式['bar', 'two'] 下面的用法是等效的: print(df.loc['bar'].loc['two']) ''' A -0.755750 B 0.387714 C 1.164027 Name: two, dtype: float64 ''' 选择行的同时也能选择列: print(df.loc[('bar', 'two'), 'A']) #-0.5189738423458226 还能使用切片操作: print(df.loc['baz': 'foo']) ''' A B C FirstLevel SecondLevel baz one -1.507232 -0.207591 1.242952 two -0.424180 -1.741234 -1.205756 foo one 0.858520 1.594259 -1.314260 two -0.711255 0.356851 -0.276307 ''' 或许, 使用更多的是这样: print(df.loc[('bar', 'two'): ('baz', 'two')]) ''' FirstLevel SecondLevel bar two -1.293961 -0.931613 0.090003 baz one -0.509716 -0.802122 1.583681 two -0.613451 0.121745 0.895646 ''' 推荐使用xs, 它可以使你的代码更容易被别人理解, 而且选择行和列都用统一的方式: print(df.xs('two', level='SecondLevel', axis='index')) ''' A B C FirstLevel bar -1.201396 -2.257291 0.831497 baz -1.204905 -0.351232 -0.025386 foo 0.987691 -0.454322 0.679079 qux 0.813230 0.073436 -0.951928 ''' MultiIndex对象属性 m_index1=pd.Index([(\"A\",\"x1\"),(\"A\",\"x2\"),(\"B\",\"y1\"),(\"B\",\"y2\"),(\"B\",\"y3\")],name=[\"class1\",\"class2\"]) print(m_index1) ''' MultiIndex([('A', 'x1'), ('A', 'x2'), ('B', 'y1'), ('B', 'y2'), ('B', 'y3')], names=['class1', 'class2']) ''' df1=DataFrame(np.random.randint(1,10,(5,3)),index=m_index1) print(df1) ''' class1 class2 A x1 1 9 2 x2 5 3 2 B y1 8 8 6 y2 9 2 2 y3 5 6 8 ''' m_index=df1.index print(m_index[0]) # ('A', 'x1') print(m_index[1]) # ('A', 'x2') 调用.get_loc()和.get_indexer()获取标签的下标： print(m_index.get_loc((\"A\",\"x2\"))) #1 print(m_index.get_indexer([(\"A\",\"x2\"),(\"B\",\"y1\"),\"nothing\"])) #[ 1 2 -1] MultiIndex 对象使用多个 Index 对象保存索引中每一级的标签： print(m_index.levels[0]) print(m_index.levels[1]) ''' Index(['A', 'B'], dtype='object', name='class1') Index(['x1', 'x2', 'y1', 'y2', 'y3'], dtype='object', name='class2') ''' MultiIndex 对象还有属性 labels 保存标签的下标： print(m_index.labels[0]) print(m_index.labels[1]) ''' [0 0 1 1 1] [0 1 2 3 4] ''' Update time： 2020-05-25 "},"Chapter2/Pandas 根据条件获取元素所在的位置（索引）.html":{"url":"Chapter2/Pandas 根据条件获取元素所在的位置（索引）.html","title":"Pandas 根据条件获取元素所在的位置（索引）","keywords":"","body":"Pandas 根据条件获取元素所在的位置（索引） .index.tolist() 在 dataframe 中根据一定的条件，得到符合要求的某行元素所在的位置。 df = pd.DataFrame({'BoolCol': [1, 2, 3, 3, 4], 'attr': [22, 33, 22, 44, 66] }, index=[10,20,30,40,50] ) print(df) ''' BoolCol attr 10 1 22 20 2 33 30 3 22 40 3 44 50 4 66 ''' a = df[(df.BoolCol==3)&(df.attr==22)].index.tolist() print(a) 选取“BoolCol”取值为3且“attr”取值为22的行，得到该行在df中的位置 a = df[(df.BoolCol==3)&(df.attr==22)].index.tolist() # a=30 df.BoolCol==3 10 False 20 False 30 True 40 True 50 False Name: BoolCol, dtype: bool _temp = {'job':['farmer', 'teacher', 'worker', 'acter', 'present'], 'money':[3000, 7000, 5000, 100000, 66666]} df = pd.DataFrame(_temp) print(df) >> job money >>0 farmer 3000 >>1 teacher 7000 >>2 worker 5000 >>3 acter 100000 >>4 present 66666 a = df[(df['money']>10000)].index.tolist() print(a) >>[3, 4] Update time： 2020-05-25 "},"Chapter2/Pandas 数据合并与重塑.html":{"url":"Chapter2/Pandas 数据合并与重塑.html","title":"Pandas 数据合并与重塑","keywords":"","body":"Pandas 数据合并与重塑 pandas 数据的行更新、表合并等操作，一般用到的方法有 concat、join、merge。 concat concat 函数是在 pandas 底下的方法，可以将数据根据不同的轴作简单的融合 pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False ) 参数说明 objs: series，dataframe或者是panel构成的序列lsit axis： 需要合并链接的轴，0是行，1是列 join：连接的方式 inner，或者outer 相同字段的表首尾相接 import pandas as pd df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']}, index=[0, 1, 2, 3]) df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'], 'B': ['B4', 'B5', 'B6', 'B7'], 'C': ['C4', 'C5', 'C6', 'C7'], 'D': ['D4', 'D5', 'D6', 'D7']}, index=[4, 5, 6, 7]) df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'], 'B': ['B8', 'B9', 'B10', 'B11'], 'C': ['C8', 'C9', 'C10', 'C11'], 'D': ['D8', 'D9', 'D10', 'D11']}, index=[8, 9, 10, 11]) # 现将表构成list，然后在作为concat的输入 frames = [df1, df2, df3] result = pd.concat(frames) print(result) 参数key 要在相接的时候在加上一个层次的 key来识别数据源自于哪张表，可以增加 key参数 result = pd.concat(frames, keys=['x', 'y', 'z']) ''' A B C D x 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 y 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 z 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 ''' 横向表拼接（行对齐） 参数 axis 当 axis = 1 的时候，concat 就是行对齐，然后将不同列名称的两张表合并 result = pd.concat([df1, df4], axis=1) 参数 join 加上 join参数的属性，如果为inner得到的是两表的交集，如果是outer，得到的是两表的并集。(行索引相同的行) result = pd.concat([df1, df4], axis=1, join='inner') 参数 join_axes 如果有join_axes的参数传入，可以指定根据那个轴来对齐数据 例如根据 df1 表对齐数据，就会保留指定的df1表的轴，然后将 df4 的表与之拼接 result = pd.concat([df1, df4], axis=1, join_axes=[df1.index]) 参数 ignore_index 如果两个表的 index 都没有实际含义，使用ignore_index参数，置true，合并的两个表就会根据列字段对齐，然后合并。最后再重新整理一个新的 index。 result=pd.concat([df1,df4],ignore_index=True) 合并的同时增加区分数据组 前面提到的keys参数可以用来给合并后的表增加key来区分不同的表数据来源 直接用key参数实现 result = pd.concat(frames, keys=['x', 'y', 'z']) ''' A B C D x 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 y 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 z 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 ''' 传入字典来增加分组键 pieces = {'x': df1, 'y': df2, 'z': df3} result = pd.concat(pieces) ''' A B C D x 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 y 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 z 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 ''' 表格列字段不同的表合并 如果遇到两张表的列字段本来就不一样，但又想将两个表合并，其中无效的值用nan来表示。那么可以使用 ignore_index来实现。 dicts = [{'A': 1, 'B': 2, 'C': 3, 'X': 4}, {'A': 5, 'B': 6, 'C': 7, 'Y': 8}] result = df1.append(dicts, ignore_index=True) append DataFrame.append(other, ignore_index=False, verify_integrity=False, sort=None ) other：DataFrame、series、dict、list这样的数据结构 ignore_index：默认值为False，如果为True则不使用index标签 verify_integrity ：默认值为False，如果为True当创建相同的index时会抛出ValueError的异常 sort：boolean，默认是None，该属性在pandas的0.23.0的版本才存在。 append是 series 和 dataframe的方法，使用它就是默认沿着列进行凭借（axis = 0，列对齐） result = df1.append(df2) append添加字典 import pandas as pd data = pd.DataFrame() a = {\"x\":1,\"y\":2} data = data.append(a,ignore_index=True) print(data) x y 0 1.0 2.0 append添加series 如果不添加ignore_index=True，会报错提示TypeError: Can only append a Series if ignore_index=True or if the Series has a name，如果不添加ignore_index=True，也可以改成以下代码 import pandas as pd data = pd.DataFrame() series = pd.Series({\"x\":1,\"y\":2},name=\"a\") data = data.append(series) print(data) x y 0 1.0 2.0 append添加list data = pd.DataFrame() a = [1,2,3] data = data.append(a) print(data) 0 0 1 1 2 2 3 如果list是一维的，则是以列的形式来进行添加，如果list是二维的则是以行的形式进行添加的 ，如果是三维的则只添加一个值 merge merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=True, suffixes=('_x', '_y'), copy=True, indicator=False ) on：列名，join用来对齐的那一列的名字，用到这个参数的时候一定要保证左表和右表用来对齐的那一列都有相同的列名。 left_on：左表对齐的列，可以是列名，也可以是和dataframe同样长度的arrays。 right_on：右表对齐的列，可以是列名，也可以是和dataframe同样长度的arrays。 left_index/ right_index: 如果是 True 的 haunted 以 index 作为对齐的key how：数据融合的方法。 sort：根据 dataframe 合并的 keys 按字典顺序排序，默认是，如果置false可以提高表现。 merge的默认合并方法： merge 用于表内部基于 index-on-index 和 index-on-column(s) 的合并，但默认是基于 index 来合并。 通过on指定数据合并对齐的列 left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'], 'key2': ['K0', 'K1', 'K0', 'K1'], 'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3']}) right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'], 'key2': ['K0', 'K0', 'K0', 'K0'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']}) result = pd.merge(left, right, on=['key1', 'key2']) ''' key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 ''' 没有指定how的话默认使用inner方法。 how的方法有： left 只保留左表的所有数据 result = pd.merge(left, right, how='left', on=['key1', 'key2']) right 只保留右表的所有数据 result = pd.merge(left, right, how='right', on=['key1', 'key2']) outer 保留两个表的所有信息 result = pd.merge(left, right, how='outer', on=['key1', 'key2']) inner 只保留两个表中公共部分的信息 result = pd.merge(left, right, how='inner', on=['key1', 'key2']) suffix后缀参数 如果和表合并的过程中遇到有一列两个表都同名，但是值不同，合并的时候又都想保留下来，就可以用suffixes给每个表的重复列名增加后缀。 result = pd.merge(left, right, on='k', suffixes=['_l', '_r']) 另外还有lsuffix 和 rsuffix分别指定左表的后缀和右表的后缀。 join dataframe 内置的join方 法是一种快速合并的方法。它默认以index 作为对齐的列。 join中的 how 参数和 merge 中的how 参数一样，用来指定表合并保留数据的规则。主要用于索引上的合并 join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False) 参数 on 在实际应用中如果右表的索引值正是左表的某一列的值，这时可以通过将 右表的索引 和 左表的列 对齐合并这样灵活的方式进行合并。 In [59]: left = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], ....: 'B': ['B0', 'B1', 'B2', 'B3'], ....: 'key': ['K0', 'K1', 'K0', 'K1']}) ....: In [60]: right = pd.DataFrame({'C': ['C0', 'C1'], ....: 'D': ['D0', 'D1']}, ....: index=['K0', 'K1']) ....: In [61]: result = left.join(right, on='key') 插入函数 insert DataFrame.insert(loc, column, value, allow_duplicates=False) 在指定列插入数据 Raises a ValueError if column is already contained in the DataFrame, unless allow_duplicates is set to True. Parameters: loc : int 使用整数定义列数据插入的位置，必须是0到columns列标签的长度 Insertion index. Must verify 0 column : string, number, or hashable object # 可选字符串、数字或者object；列标签名 label of the inserted column value : int, Series, or array-like # 整数、Series或者数组型数据 allow_duplicates : bool, optional # 可选参数，如果dataframe中已经存在某列，将allow_duplicates置为true才可以将指定得列插入。 实例详解： import pandas as pd import numpy as np df = pd.DataFrame(np.arange(12).reshape(4,3) ,columns=['a','b','c']) df a b c 0 0 1 2 1 3 4 5 2 6 7 8 3 9 10 11 在第二列插入数据： df.insert(1,'d',np.ones(4)) df a d b c 0 0 1.0 1 2 1 3 1.0 4 5 2 6 1.0 7 8 3 9 1.0 10 11 如果没有设定 allow_duplicates = True，此时如果添加的列已经存在，则会报错： df.insert(1,'d',np.ones(4)) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) in ----> 1 df.insert(1,'d',np.ones(4)) D:\\Application\\Anaconda3\\envs\\pycharm\\lib\\site-packages\\pandas\\core\\frame.py in insert(self, loc, column, value, allow_duplicates) 3494 self._ensure_valid_index(value) 3495 value = self._sanitize_column(column, value, broadcast=False) -> 3496 self._data.insert(loc, column, value, allow_duplicates=allow_duplicates) 3497 3498 def assign(self, **kwargs) -> \"DataFrame\": D:\\Application\\Anaconda3\\envs\\pycharm\\lib\\site-packages\\pandas\\core\\internals\\managers.py in insert(self, loc, item, value, allow_duplicates) 1171 if not allow_duplicates and item in self.items: 1172 # Should this be a different kind of error?? -> 1173 raise ValueError(f\"cannot insert {item}, already exists\") 1174 1175 if not isinstance(loc, int): ValueError: cannot insert d, already exists 因此，如果是添加的列已经存在，如下处理： df.insert(1,'d',np.ones(4),allow_duplicates=True) #allow_duplicates=True df a d d b c 0 0 1.0 1.0 1 2 1 3 1.0 1.0 4 5 2 6 1.0 1.0 7 8 3 9 1.0 1.0 10 11 参考 PANDAS 数据合并与重塑 pandas的DataFrame的append方法详细介绍 Update time： 2020-08-06 "},"Chapter2/Pandas 轴旋转stack和unstack.html":{"url":"Chapter2/Pandas 轴旋转stack和unstack.html","title":"Pandas 轴旋转stack和unstack","keywords":"","body":"Pandas 轴旋转stack和unstack pandas 的 DataFrame 的轴旋转操作，stack 和 unstack 首先，要知道以下五点： stack：将数据的列“旋转”为行 unstack：将数据的行“旋转”为列 stack和unstack默认操作为最内层 stack和unstack默认旋转轴的级别将会成果结果中的最低级别（最内层） stack和unstack为一组逆运算操作 创建DataFrame,行索引名为state，列索引名为number import pandas as pd import numpy as np data = pd.DataFrame(np.arange(6).reshape((2, 3)), index=pd.Index(['Ohio', 'Colorado'], name='state'), columns=pd.Index(['one', 'two', 'three'], name='number')) data 将DataFrame的列旋转为行，即stack操作。 从结果来理解上述点4stack 操作后将列索引number旋转为行索引，并且置于行索引的最内层（外层为索引state），也就是将旋转轴（number）的结果置于 最低级别。 将DataFrame的行旋转为列，即unstack操作。 result.unstack() unstack操作默认将内层索引number旋转为列索引。 同时，也可以指定分层级别或者索引名称来指定操作级别，下面做法同样会得到上面的结果。 stack 和 unstack 逆运算 s1 = pd.Series([0,1,2,3],index=list('abcd')) s2 = pd.Series([4,5,6],index=list('cde')) data2 = pd.concat([s1,s2],keys=['one','two']) data2 data2.unstack() data2.unstack().stack() 参考 【Python】pandas轴旋转stack和unstack用法详解 Update time： 2020-05-25 "},"Chapter2/Pandas 缺失值处理.html":{"url":"Chapter2/Pandas 缺失值处理.html","title":"Pandas 缺失值处理","keywords":"","body":"Pandas 缺失值处理 在实际应用中对于数据进行分析的时候，经常能看见缺失值，下如何利用pandas 来处理缺失值。常见的缺失值处理方式有，过滤、填充。 缺失值的判断 pandas 使用浮点值 NaN(Not a Number)表示浮点数和非浮点数组中的缺失值，同时 python内置None值也会被当作是缺失值。 DataFrame.dropna() DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False ) axis: 维度，axis=0 表示index行, axis=1表示columns列，默认为0 how: \"all\"表示这一行或列中的元素全部缺失（为nan）才删除这一行或列，\"any\"表示这一行或列中只要有元素缺失，就删除这一行或列 thresh:一行或一列中至少出现了thresh个才删除。 subset：在某些列的子集中选择出现了缺失值的列删除，不在子集中的含有缺失值得列或行不会删除（有axis决定是行还是列） inplace：刷选过缺失值得新数据是存为副本还是直接在原数据上进行修改。 默认参数：删除行，只要有空值就会删除，不替换。 df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'], \"toy\": [np.nan, 'Batmobile', 'Bullwhip'], \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"), pd.NaT]}) # print(df) ''' name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT ''' print(df.dropna()) ''' name toy born 1 Batman Batmobile 1940-04-25 ''' print(df) ''' name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT ''' #delete colums print(df.dropna(axis=1) )#delete co ''' name 0 Alfred 1 Batman 2 Catwoman ''' #\"所有值全为缺失值才删除\" print(df.dropna(how='all')) ''' name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT ''' #\"至少出现过两个缺失值才删除\" print(df.dropna(thresh=2)) ''' name toy born 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT ''' #\"删除这个subset中的含有缺失值的行或列\" print (df.dropna(subset=['name', 'born'])) ''' name toy born 1 Batman Batmobile 1940-04-25 ''' DataFrame.fillna() DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs ) 函数作用：填充缺失值 value:需要用什么值去填充缺失值 axis:确定填充维度，从行开始或是从列开始 method：ffill:用缺失值前面的一个值代替缺失值，如果axis =1，那么就是横向的前面的值替换后面的缺失值，如果axis=0，那么则是上面的值替换下面的缺失值。backfill/bfill，缺失值后面的一个值代替前面的缺失值。 注意这个参数不能与value同时出现 limit: 确定填充的个数，如果limit=2，则只填充两个缺失值。 df = pd.DataFrame([[np.nan, 2, np.nan, 0], [3, 4, np.nan, 1], [np.nan, np.nan, np.nan, 5], [np.nan, 3, np.nan, 4]], columns=list('ABCD')) print(df) ''' A B C D 0 NaN 2.0 NaN 0 1 3.0 4.0 NaN 1 2 NaN NaN NaN 5 3 NaN 3.0 NaN 4 ''' #\"横向用缺失值前面的值替换缺失值\" print(df.fillna(axis=1, method='ffill')) ''' A B C D 0 NaN 2.0 2.0 0.0 1 3.0 4.0 4.0 1.0 2 NaN NaN NaN 5.0 3 NaN 3.0 3.0 4.0 ''' #\"纵向用缺失值上面的值替换缺失值\" print(df.fillna(axis=0,method='ffill')) ''' A B C D 0 NaN 2.0 NaN 0 1 3.0 4.0 NaN 1 2 3.0 4.0 NaN 5 3 3.0 3.0 NaN 4 ''' print(df.fillna(0)) ''' A B C D 0 0.0 2.0 0.0 0 1 3.0 4.0 0.0 1 2 0.0 0.0 0.0 5 3 0.0 3.0 0.0 4 ''' 不同的列用不同的值填充：,可以通过一个字典用fillna,实现对不同的列填充不同的值 # 不同的列用不同的值填充： values={'A':0,'B':1,'C':2,'D':3} print(df.fillna(value=values)) ''' A B C D 0 0.0 2.0 2.0 0 1 3.0 4.0 2.0 1 2 0.0 1.0 2.0 5 3 0.0 3.0 2.0 4 ''' DataFrame.isna() 判断是不是缺失值： df = pd.DataFrame({'age':[5, 6,np.NaN], 'born':[pd.NaT, pd.Timestamp('1939-05-27'), pd.Timestamp('1929-05-27')], 'name':['alf', 'daes',''], 'toy':[None, 'asyeg','aued'] }) df.isna() age born name toy 0 False True False True 1 False False False False 2 True False False False isnull 同上。 df = pd.DataFrame([[np.nan, 2, np.nan, 0], [3, 4, \"\", 1], [np.nan, np.nan, np.nan, 5], [np.nan, 3, \"\", 4]], columns=list('ABCD')) print(df) ''' A B C D 0 NaN 2.0 NaN 0 1 3.0 4.0 1 2 NaN NaN NaN 5 3 NaN 3.0 4 ''' 如上，缺失值是NAN，空值是没有显示。 替换空值代码：需要把含有空值的那一列提出来单独处理，然后在放进去就好。 clean_z = df['C'].fillna(0) clean_z[clean_z==''] = 'hello' df['C'] = clean_z print(df) ''' A B C D 0 NaN 2.0 0 0 1 3.0 4.0 hello 1 2 NaN NaN 0 5 3 NaN 3.0 hello 4 ''' Update time： 2020-05-25 "},"Chapter2/Pandas 数据排序.html":{"url":"Chapter2/Pandas 数据排序.html","title":"Pandas 数据排序","keywords":"","body":"Pandas 数据排序 有的时候我们可以要根据索引的大小或者值的大小对Series和DataFrame进行排名和排序。 根据条件对Series对象或DataFrame对象的值排序（sorting）和排名(ranking)是一种重要的内置运算。 使用 pandas 对象的：sort_index()/ sort_values()/ rank()方法。 Series排序 sort_index 方法可以根据行或列的索引按照字典的顺序进行排序，返回一个已排 的新对象 sort_index 按索引进行排序 #定义一个Series s = Series([1,2,3],index=[\"a\",\"c\",\"b\"]) #对Series的索引进行排序，默认是升序 print(s.sort_index()) ''' a 1 b 3 c 2 ''' #对索引进行降序排序 print(s.sort_index(ascending=False)) ''' c 2 b 3 a 1 ''' sort_values按值进行排序 DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ) axis这个参数的默认值为0，匹配的是index，跨行进行排序，当axis=1时，匹配的是columns，跨列进行排序 by这个参数要求传入一个字符或者是一个字符列表，用来指定按照axis的中的哪个元素来进行排序 ascending这个参数的默认值是True，按照升序排序，当传入False时，按照降序进行排列 kind这个参数表示按照什么样算法来进行排序，默认值是quicksort（快速排序），也可以传入mergesort（归并排序）或者是heapsort（堆排序），至于具体每种算法是如何实现的，我们这里按下不表，同样的，对于inplace这个参数我们也不做讨论对于涉及到的In-place algorithm（原地算法）感兴趣的可以看看这里 最后一个参数na_position是针对DataFrame中的空缺值的，默认值是last表示将空缺值放在排序的最后，也可以传入first放在最前： s = Series([np.nan,1,7,2,0],index=[\"a\",\"c\",\"e\",\"b\",\"d\"]) #对Series的值进行排序，默认是按值的升序进行排序的 print(s.sort_values()) ''' d 0.0 c 1.0 b 2.0 e 7.0 a NaN ''' #对Seires的值进行降序排序 print(s.sort_values(ascending=False)) ''' e 7.0 b 2.0 c 1.0 d 0.0 a NaN ''' NaN值会放在Series末尾 se4=pd.Series([3,np.nan,-7,np.nan,5]) se4.sort_values() 2 -7.0 0 3.0 4 5.0 1 NaN 3 NaN dtype: float64 DataFrame排序 通过axis参数可以对任意轴排序 df1=pd.DataFrame(np.arange(9).reshape(3,3),index=list(\"bac\"),columns=list(\"yzx\")) print(df1) ''' y z x b 0 1 2 a 3 4 5 c 6 7 8 ''' print(df1.sort_index()) ''' y z x a 3 4 5 b 0 1 2 c 6 7 8 ''' print(df1.sort_index(axis=1)) ''' x y z b 2 0 1 a 5 3 4 c 8 6 7 ''' 根据一个列的值来排序 df2=pd.DataFrame({'a':[20,3,3],'b':[1,-6,18]}) print(df2.sort_values(by='b')) ''' a b 1 3 -6 0 20 1 2 3 18 ''' 对多个列来排序 df = DataFrame(np.arange(20).reshape(5,4),index=[3,1,2,4,6],columns=['d','c','a','b']) print(df.sort_index(ascending=False)) # 降序排列 ''' d c a b 6 16 17 18 19 4 12 13 14 15 3 0 1 2 3 2 8 9 10 11 1 4 5 6 7 ''' rank( )函数 rank()函数返回从小到大排序的下标 DataFrame.rank(axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False ) axis：设置沿着哪个轴计算排名（0或者1) numeric_only：是否仅仅计算数字型的columns，布尔值 na_option：NaN值是否参与排序及如何排序（‘keep’，‘top'，’bottom'） ascending：设定升序排还是降序排 pct：是否以排名的百分比显示排名（所有排名与最大排名的百分比） method：取值可以为'average'，'first'，'min'， 'max'，'dense'， \"first\": 顾名思义，第一个，谁出现的位置靠前，谁的排名靠前。李四和王五的成绩都为30，但是李四出现在王五的前面，所以李四的排名靠前 \"min\": 当method=“min”时，成绩相同的同学，取在顺序排名中最小的那个排名作为该值的排名，李四和王五同学排名分别为2和3，那么当method为min时，取2和3的最小的那个作为第2名作为成绩30的排名。 \"dense\": 是密集的意思，即相同成绩的同学排名相同，其他依次加1即可。 average，成绩相同时，取顺序排名中所有名次之和除以该成绩的个数，即为该成绩的名次；比如上述排名中，30排名为2,3，那么 30的排名 = （2+3）/2=2.5，成绩为50的同学只有1个，且排名为1，那50的排名就位1/1=1。 max，和min一样也是跳跃排名的一种，成绩相同时取顺序排名中排名最大的作为该成绩的名次，在顺序排名中，30最大的排名为3，那么当参数为max时，30的排名=3，此时，李四和王五的排名都为第3名了。 默认情况下，rank是通过“为各组分配一个平均排名”的方式破坏平级关系的 In [120]:obj = pd.Series([7,-5,7,4,2,0,4]) In [121]:obj.rank() Out [121]: 0 6.5 1 1.0 2 6.5 3 4.5 4 3.0 5 2.0 6 4.5 dtype: float64 在 obj 中，4和4的排名是第4名和第五名，取平均得4.5。7和7的排名分别是第六名和第七名，则其排名取平均得6.5 根据值在原数据中出现的顺序排名** In [122]:obj.rank(method='first') Out [122]: 0 6.0 1 1.0 2 7.0 3 4.0 4 3.0 5 2.0 6 5.0 dtype: float64 按降序进行 In [123]:obj.rank(ascending=False, method='max') Out [123]: 0 2.0 1 7.0 2 2.0 3 4.0 4 5.0 5 6.0 6 4.0 dtype: float64 若对DataFrame进行排序，则可根据axis指定要进行排序的轴 In [136]: frame=pd.DataFrame({'b':[5,7,-3,2],'a':[0,1,0,1],'c':[-2,5,8,-3]}) In [137]: frame Out[137]: a b c 0 0 5 -2 1 1 7 5 2 0 -3 8 3 1 2 -3 In [138]: frame.rank(axis=0) Out[138]: a b c 0 1.5 3.0 2.0 1 3.5 4.0 3.0 2 1.5 1.0 4.0 3 3.5 2.0 1.0 In [139]: frame.rank(axis=1) Out[139]: a b c 0 2.0 3.0 1.0 1 1.0 3.0 2.0 2 2.0 1.0 3.0 3 2.0 3.0 1.0 参考： pandas.sort_values方法 pandas的rank()函数 Update time： 2020-05-25 "},"Chapter2/Pandas 描述和汇总统计.html":{"url":"Chapter2/Pandas 描述和汇总统计.html","title":"Pandas 描述和汇总统计","keywords":"","body":"Pandas 描述和汇总统计 pandas 提供了很多常用的数学和统计方法，其中大部分都属于约简和汇总统计，用于从Series 中提取单个值（如sum或mean）或从DataFrame的行或列中提取一个 Series。 mean()平均值 median()中位数 max()最大值 min()最小值 sum()求和 std()标准差 Series类型独有的方法： argmax()最大值的位置 argmin()最小值的位置 DataFrame的sum和mean方法 a = [[1,np.nan,9],[2,8,3],[3,5,np.nan]] data = DataFrame(a,index=[\"a\",\"b\",\"c\"],columns=[\"one\",\"two\",\"three\"]) print(data) ''' one two three a 1 NaN 9.0 b 2 8.0 3.0 c 3 5.0 NaN ''' #对列求和 print(data.sum()) ''' one 6.0 two 13.0 three 12.0 ''' #对行求和 print(data.sum(axis=1)) ''' a 10.0 b 13.0 c 8.0 ''' #对行求平均值,默认排除NaN值 print(data.mean(axis=1)) ''' a 5.000000 b 4.333333 c 4.000000 ''' #对行求平均值，禁用自动排除NaN值 print(data.mean(axis=1,skipna=False)) ''' a NaN b 4.333333 c NaN ''' 统计 .idxmax(), .idxmin(), .cumsum() 上面的操作都是对于列，如果想要对行进行操作，只需要在方法中设置axis参数为1即可。 a = [[1,np.nan,9],[2,8,3],[3,5,np.nan]] data = DataFrame(a,index=[\"0\",\"1\",\"2\"],columns=[\"a\",\"b\",\"c\"]) print(data) ''' a b c 0 1 NaN 9.0 1 2 8.0 3.0 2 3 5.0 NaN ''' # 返回每一列中最大值的行索引 print(data.idxmax()) ''' a 2 b 1 c 0 dtype: object ''' #返回每一列中最小值的行索引 print(data.idxmin()) ''' a 0 b 2 c 1 dtype: object ''' #对每列的值进行累加 print(data.cumsum()) ''' a b c 0 1.0 NaN 9.0 1 3.0 8.0 12.0 2 6.0 13.0 NaN ''' 数据描述 DataFrame df.describe() a = [[1,np.nan,9],[2,8,3],[3,5,np.nan]] data = DataFrame(a,index=[\"0\",\"1\",\"2\"],columns=[\"a\",\"b\",\"c\"]) print(data) ''' a b c 0 1 NaN 9.0 1 2 8.0 3.0 2 3 5.0 NaN ''' #列出DataFrame的描述 ''' 四分位数用于绘制箱线图判断是否为异常值 count：该列（行）非NA值的个数 mean ：该列（行）的均值 std ：该列（行）的方差 25% ：上四分位数 50% ：非NA值的平均数 75% ：下四分位数 max ：最大值 ''' print(data.describe()) ''' a b c count 3.0 2.00000 2.000000 mean 2.0 6.50000 6.000000 std 1.0 2.12132 4.242641 min 1.0 5.00000 3.000000 25% 1.5 5.75000 4.500000 50% 2.0 6.50000 6.000000 75% 2.5 7.25000 7.500000 max 3.0 8.00000 9.000000 ''' Series 如果值是数值型的描述与DataFrame一致， s = Series([\"a\",\"b\",\"b\",\"d\"]) print(s) ''' 0 a 1 b 2 b 3 d ''' print(s.describe()) ''' count 4 unique 3 top b freq 2 ''' argmax(), argmin(),.tolist() argmax()某一列最大值的位置 argmin()最小值的位置,.tolist() tolist()转换成list类型 # coding=utf-8 import numpy as np import pandas as pd # 创建DataFrame df = pd.DataFrame(np.arange(12, 32).reshape((5, 4)), index=[\"a\", \"b\", \"c\", \"d\", \"e\"], columns=[\"WW\", \"XX\", \"YY\", \"ZZ\"]) print(df) ''' WW XX YY ZZ a 12 13 14 15 b 16 17 18 19 c 20 21 22 23 d 24 25 26 27 e 28 29 30 31 ''' # mean()平均值 median()中位数 max()最大值 min()最小值 sum()求和 std()标准差 print(df.mean()) # 每一列平均值 (Series类型) ''' WW 20.0 XX 21.0 YY 22.0 ZZ 23.0 dtype: float64 ''' print(df[\"YY\"].mean()) # 22.0 指定列的平均值 print(df[\"YY\"]) # Series类型 YY_list = df[\"YY\"].tolist() # tolist()转换成list类型 print(YY_list) # [14, 18, 22, 26, 30] print(len(YY_list)) # 5 print(len(set(YY_list))) # set集合可以去重 print(df[\"YY\"].unique()) # [14 18 22 26 30] unique()自动去重(ndarray类型) print(df.max()) # 每一列的最大值 Series类型。 min()最小值 ''' WW 28 XX 29 YY 30 ZZ 31 dtype: int64 ''' # argmax()某一列最大值的位置 argmin()最小值的位置 print(df[\"YY\"].argmax()) # e Update time： 2020-05-25 "},"Chapter2/Pandas 众数mode.html":{"url":"Chapter2/Pandas 众数mode.html","title":"Pandas 众数mode","keywords":"","body":"Pandas 众数mode DataFrame.mode(axis=0, numeric_only=False, dropna=True) 返回指定行/列的众数，可能是多个值。 参数： axis ：{0 or ‘index’, 1 or ‘columns’}, default 0 The axis to iterate over while searching for the mode: 0 or ‘index’ : get mode of each column 1 or ‘columns’ : get mode of each row. numeric_only ：bool default False 若 为 True 则只对数字进行众数计算。 dropna ：bool, default True 不考虑NaN/NaT的计数。 返回值： DataFrame 每列或每行的众数。 >>> df = pd.DataFrame({'A': [1, 2, 1, 2, 1, 2, 3]}) >>> df.mode() A 0 1 1 2 # 1和2 都出现了三次，所以返回的是这两个数 Examples df = pd.DataFrame([('bird', 2, 2), ('mammal', 4, np.nan), ('arthropod', 8, 0), ('bird', 2, np.nan)], index=('falcon', 'horse', 'spider', 'ostrich'), columns=('species', 'legs', 'wings')) df species legs wings falcon bird 2 2.0 horse mammal 4 NaN spider arthropod 8 0.0 ostrich bird 2 NaN df.mode() species legs wings 0 bird 2.0 0.0 1 NaN NaN 2.0 # wings 有两个众数，其它的只有一个，空缺的位置用NaN 填充 df.mode(dropna=False) species legs wings 0 bird 2 NaN 只对数字进行计算 df.mode(numeric_only=True) legs wings 0 2.0 0.0 1 NaN 2.0 对行/列计算 df.mode(axis='columns', numeric_only=True) 0 1 falcon 2.0 NaN horse 4.0 NaN spider 0.0 8.0 ostrich 2.0 NaN Update time： 2020-08-16 "},"Chapter2/Pandas 分位数quantile.html":{"url":"Chapter2/Pandas 分位数quantile.html","title":"Pandas 分位数quantile","keywords":"","body":"Pandas 分位数quantile DataFrame.quantile(q=0.5, axis=0, numeric_only=True, interpolation='linear') 参数： q ：float or array-like, default 0.5 (50% quantile) axis ：{0, 1, ‘index’, ‘columns’}, default 0 Equals 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise. numeric_only： bool, default True If False, the quantile of datetime and timedelta data will be computed as well. interpolation：（插值方法） : {‘linear’, ‘lower’, ‘higher’, ‘midpoint’, ‘nearest’} 当选中的分为点位于两个数数据点 i and j 之间时: linear: i + (j - i) * fraction, fraction由计算得到的pos的小数部分（可以通过下面一个例子来理解这个fraction）； lower: i. nearest: i or j whichever is nearest. midpoint: (i + j) / 2. 返回值： Series or DataFrame If q is an array, a DataFrame will be returned where the index is q, the columns are the columns of self, and the values are the quantiles. If q is a float, a Series will be returned where the index is the columns of self and the values are the quantiles. 统计学上的四分为函数 原则上p是可以取0到1之间的任意值的。但是有一个四分位数是p分位数中较为有名的。 所谓四分位数；即把数值由小到大排列并分成四等份，处于三个分割点位置的数值就是四分位数。 第1四分位数 (Q1)，又称“较小四分位数”，等于该样本中所有数值由小到大排列后第25%的数字。 第2四分位数 (Q2)，又称“中位数”，等于该样本中所有数值由小到大排列后第50%的数字。 第3四分位数 (Q3)，又称“较大四分位数”，等于该样本中所有数值由小到大排列后第75%的数字。 第3四分位数与第1四分位数的差距又称四分位距（InterQuartile Range,IQR） 计算方法与举例 五大因“数” 我们一组序列数为例：12，15，17，19，20，23，25，28，30，33，34，35，36，37讲解这五大因“数” 1、下四分位数Q1 （1）确定四分位数的位置。Qi所在位置=i*（n+1）/4，其中i=1，2，3。n表示序列中包含的项数。 （2）根据位置，计算相应的四分位数。 例中： Q1所在的位置=（14+1）/4=3.75， Q1=0.25×第三项+0.75×第四项=0.25×17+0.75×19=18.5； 2、中位数（第二个四分位数）Q2 中位数，即一组数由小到大排列处于中间位置的数。若序列数为偶数个，该组的中位数为中间两个数的平均数。 例中： Q2所在的位置=2*（14+1）/4=7.5， Q2=0.5×第七项+0.5×第八项=0.5×25+0.5×28=26.5 3、上四分位数Q3 计算方法同下四分位数。 例中： Q3所在的位置=3*（14+1）/4=11.25， Q3=0.75×第十一项+0.25×第十二项=0.75×34+0.25×35=34.25。 4、上限 上限是非异常范围内的最大值。 首先要知道什么是四分位距如何计算的？ 四分位距IQR=Q3-Q1，那么上限=Q3+1.5IQR 5、下限 下限是非异常范围内的最小值。 下限=Q1-1.5IQR 为了更一般化，在计算的过程中，我们考虑p分位。当p=0.25 0.5 0.75 时，就是在计算四分位数。 首先确定p分位数的位置（有两种方法）： 方法1 pos = (n+1)p 方法2 pos = 1+(n-1)p pandas 中使用的是方法2确定的。 df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]), columns=['a', 'b']) df a b 0 1 1 1 2 10 2 3 100 3 4 100 df.quantile(.1) a 1.3 b 3.7 Name: 0.1, dtype: float64 df.quantile([.1, .5]) a b 0.1 1.3 3.7 0.5 2.5 55.0 data.price.quantile([0.25,0.5,0.75]) //输出 0.25 42812.25 0.50 57473.00 0.75 76099.75 参考： 搞懂箱形图分析，快速识别异常值！ Update time： 2020-09-07 "},"Chapter2/Pandas 统计函数corr.html":{"url":"Chapter2/Pandas 统计函数corr.html","title":"Pandas 统计函数corr","keywords":"","body":"Pandas 统计函数corr 相关系数(correlation coefficients) 计算列与列之间的相关系数 (不包括NA/null)，返回相关系数矩阵 DataFrame.corr( method='pearson', min_periods=1) → ’DataFrame’ 参数： method:{‘pearson’, ‘kendall’, ‘spearman’} or callable pearson：标准相关系数 kendall：Kendall Tau相关系数 spearman：Spearman等级相关 callable：可输入两个1d ndarray来调用 返回值：DataFrame Examples allDf = pd.DataFrame({ 'x':[0,1,2,4,7,10], 'y':[0,3,2,4,5,7], 's':[0,1,2,3,4,5], 'c':[5,4,3,2,1,0] },index = ['p1','p2','p3','p4','p5','p6']) allDf 结果： x y s c p1 0 0 0 5 p2 1 3 1 4 p3 2 2 2 3 p4 4 4 3 2 p5 7 5 4 1 p6 10 7 5 0 allDf.corr() 结果： allDf.corr()['x'] 参考 pandas 统计函数corr pandas.DataFrame.corr Update time： 2020-07-12 "},"Chapter2/Pandas 统计函数scatter_matrix.html":{"url":"Chapter2/Pandas 统计函数scatter_matrix.html","title":"Pandas 统计函数scatter_matrix","keywords":"","body":"Pandas 统计函数scatter_matrix Update time： 2020-07-11 "},"Chapter2/Pandas 使用测试模块制作伪数据.html":{"url":"Chapter2/Pandas 使用测试模块制作伪数据.html","title":"Pandas 使用测试模块制作伪数据","keywords":"","body":"Pandas 使用测试模块制作伪数据 在pandas中，有一个测试模块可以帮助我们生成半真实（伪数据），并进行测试，它就是util.testing。 创建随机数据集 df = pd.util.testing.makeDataFrame() 创建随机日期索引数据集 df = pd.util.testing.makePeriodFrame() df = pd.util.testing.makeTimeDataFrame() 创建随机混合类型数据集 df = pd.util.testing.makeMixedDataFrame() >>> import pandas.util.testing as tm >>> tm.N, tm.K = 15, 3 # 默认的行和列 >>> import numpy as np >>> np.random.seed(444) >>> tm.makeTimeDataFrame(freq='M').head() A B C 2000-01-31 0.3574 -0.8804 0.2669 2000-02-29 0.3775 0.1526 -0.4803 2000-03-31 1.3823 0.2503 0.3008 2000-04-30 1.1755 0.0785 -0.1791 2000-05-31 -0.9393 -0.9039 1.1837 >>> tm.makeDataFrame().head() A B C nTLGGTiRHF -0.6228 0.6459 0.1251 WPBRn9jtsR -0.3187 -0.8091 1.1501 7B3wWfvuDA -1.9872 -1.0795 0.2987 yJ0BTjehH1 0.8802 0.7403 -1.2154 0luaYUYvy1 -0.9320 1.2912 -0.2907 创建随机个数的字符串 pd.util.testing.rands(n) [pd.util.testing.rands(3) for _ in range(4)] # ['crx', 'Z9k', 'KAU', 'mHz'] pd.util.testing.rand(n) 随机创建含有三个元素的数组 [pd.util.testing.rand(3) for _ in range(4)] [array([0.72753901, 0.01316105, 0.12749649]), array([0.68111966, 0.30113528, 0.27567208]), array([0.55361183, 0.19179808, 0.89546358]), array([0.97648769, 0.94504658, 0.1353476 ])] pd.util.testing.rands_array(nchars, size, dtype='O') 创建字符串矩阵 [pd.util.testing.rands_array(2,size=(2,2)) for _ in range(4)] [array([['YX', 'u9'], ['MY', 'Fa']], dtype=object), array([['Fa', 'CQ'], ['Kf', 'IR']], dtype=object), array([['Lz', 'q8'], ['1S', 'WU']], dtype=object), array([['EC', 'Jz'], ['2z', '6g']], dtype=object)] pd.util.testing.randbool(iterable) 根据 iterable 的shape 创建 boolen 类型的 iterable [pd.util.testing.randbool([1,2]) for _ in range(4)] [array([[ True, True]]), array([[False, False]]), array([[ True, False]]), array([[ True, True]])] Update time： 2020-05-25 "},"Chapter2/Pandas 字符数据的处理方法.html":{"url":"Chapter2/Pandas 字符数据的处理方法.html","title":"Pandas 字符数据的处理方法","keywords":"","body":"Pandas 字符数据的处理方法 在商业数据表中，经常需要处理字符型的数据，而 pandas 中的 Series.str属性就有几下几十种方法可以处理字符串数据。 Series.str.extract() str.extract()，可用正则从字符数据中抽取匹配的数据，注意要加上括号， 只返回第一个匹配的数据（要将自己要提取的内容加上括号，括号外的不提取） 注意，正则表达式中必须有分组，只是返回分组中的数据，如果给分组取了名称，则该名称就是返回结果中的字段名。 Series.str.extract(pat, flags=0, expand=None) 参数: pat : 字符串或正则表达式 flags : 整型, expand : 布尔型,是否返回DataFrame 当字符串中含有多个要提取的变量时，expand=True 会将其分割为多列 Returns: 数据框dataframe/索引index for i in df: df['Title']=df_raw['Name'].str.extract('([A-Za-z]+)\\.', expand=False) str.extractall() Series.str.extractall(pat, flags=0) 返回所有匹配的字符 参数: pat: 字符串或正则表达式 flags : 整型 返回值: Series.str.cat() Series.str.cat(others=None, sep=None, na_rep=None) 用给定的字符链接序列或索引中的字符串 返回：合并后的序列 注：当na_rep为None，序列中的nan值将被忽略，如果指定，将用该字符代替 >>> df.index.str.cat(['1','2','3'],sep = ',') Index(['f,1', 'g,2', 'h,3'], dtype='object') >>> df.index.str.cat([['1','2','3'],['4','5','6']],sep='*') Index(['f*1*4', 'g*2*5', 'h*3*6'], dtype='object') >>> df.index.str.cat(sep='*') 'f*g*h' >>> Series.str.get() 获取指定位置的字符串 Series.str.contains() 是否包含表达式 Series.str.contains(pat, case=True, flags=0, na=nan, regex=True) 判断给定的字符串或者正则表达式是否在序列或者索引中 返回：bool >>> df a b hh f 1 4 d g 2 5 f h 3 6 g >>> df['hh'].str.contains('f') f False g True h False Name: hh, dtype: bool 参数 na 当数据中有空值的时候 处理的策略 mapping = {'liucixin': '刘慈欣|大刘', 'guofan': '郭帆', 'quchuxiao': '屈楚萧|刘启|户口', 'wujing': '吴京|刘培强', 'liguangjie': '李光洁|王磊', 'wumengda': '吴孟达|达叔|韩子昂', 'zhaojinmai': '赵今麦|韩朵朵'} for key, value in mapping.items(): data[key] = data['content'].str.contains(value,na=False) 处理后得到结果 全部为 bool 类型的值， 不包含 NA/ 等空值 Series.str.count() Series.str.count(pat, flags=0, **kwargs) 计算pat在序列或者索引字符串中出现的次数 >>> df['count']=['hello','hello','hel'] >>> df a b hh count f 1 4 d hello g 2 5 f hello h 3 6 g hel >>> df['count'].str.count('hel') f 1 g 1 h 1 Name: count, dtype: int64 >>> df['count'].str.count('l') f 2 g 2 h 1 Name: count, dtype: int64 Series.str.decode() Series.str.decode(encoding, errors=’strict’) 对指定的编码方式进行解码（‘utf-8’,'gbk'等等） Series.str.endswith() Series.str.endswith(pat, na=nan) 判断是否已给定的 pat 结尾 其他 split()切分字符串 str.split()有三个参数： 第一个参数就是引号里的内容：就是分列的依据，可以是空格，符号，字符串等等。 第二个参数就是前面用到的expand=True，这个参数直接将分列后的结果转换成DataFrame。 第三个参数的n=数字就是限制分列的次数 。 就是当用于分列的依据符号在有多个的话需要指定分列的次数（不指定的话就会根据符号有几个分列几次）。 data.auth_capital.sample(5) \"\"\" 411 注册资本：100 万元 735 注册资本：53760 万元人民币 607 注册资本：1476.92万元人民币 464 注册资本：22.3464万元人民币 12 注册资本：430077.1898万人民币元 \"\"\" auth_capital = data['auth_capital'].str.split('：', expand = True) auth_capital.sample(5) join()对每个字符都用给点的字符串拼接起来，不常用 replace()替换 repeat() 重复 pad() 左右补齐 >>> s.str.pad(10, fillchar=\"?\") 0 ?????a_b_c 1 ?????c_d_e 2 NaN 3 ?????f_g_h dtype: object >>> >>> s.str.pad(10, side=\"right\", fillchar=\"?\") 0 a_b_c????? 1 c_d_e????? 2 NaN 3 f_g_h????? dtype: object slice() 按给定的开始结束位置切割字符串 >>> s.str.slice(1,3) 0 _b 1 _d 2 NaN 3 _g dtype: object slice_replace()使用给定的字符串，替换指定的位置的字符 >>> s.str.slice_replace(1, 3, \"?\") 0 a?_c 1 c?_e 2 NaN 3 f?_h dtype: object >>> s.str.slice_replace(1, 3, \"??\") 0 a??_c 1 c??_e 2 NaN 3 f??_h dtype: object findall() 查找所有符合正则表达式的字符，以数组形式返回 >>> s.str.findall(\"[a-z]\"); 0 [a, b, c] 1 [c, d, e] 2 NaN 3 [f, g, h] dtype: object partition() 把字符串数组切割称为DataFrame，注意切割只是切割称为三部分，分隔符前，分隔符，分隔符后 rpartition() 从右切起 lower() 全部小写 upper() 全部大写 参考 Pandas之Series.str列内置的方法 Update time： 2020-05-25 "},"Chapter2/Pandas 时间序列—date_range函数.html":{"url":"Chapter2/Pandas 时间序列—date_range函数.html","title":"Pandas 时间序列—date_range函数","keywords":"","body":"Pandas 时间序列—date_range函数 函数原型： pandas.date_range( start=None, end=None, periods=None, freq='D', tz=None, normalize=False, name=None, closed=None, **kwargs ) 参数： start：string 或 datetime-like，默认值是 None，表示日期的起点。 end：string 或 datetime-like，默认值是 None，表示日期的终点。 periods：integer 或 None，默认值是None，表示你要从这个函数产生多少个日期索引值；如果是None的话，那么start和end必须不能为None。 freq：string 或 DateOffset，默认值是 ’D’，表示以自然日为单位，这个参数用来指定计时单位，比如 ’5H’ 表示每隔5个小时计算一次。 tz：string 或 None，表示时区，例如：’Asia/Hong_Kong’。 normalize：bool，默认值为 False，如果为True的话，那么在产生时间索引值之前会先把start和end都转化为当日的午夜0点。 name：str，默认值为None，给返回的时间索引指定一个名字。 closed：string 或者 None，默认值为None，表示 start 和 end 这个区间端点是否包含在区间内，可以有三个值，left 表示左闭右开区间，righ 表示左开右闭区间，None 表示两边都是闭区间。 In [11]: import pandas as pd In [12]: pd.date_range(start='20170101',end='20170110') Out[12]: DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-07', '2017-01-08', '2017-01-09', '2017-01-10'], dtype='datetime64[ns]', freq='D') In [13]: pd.date_range(start='20170101',periods=10) Out[13]: DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-07', '2017-01-08', '2017-01-09', '2017-01-10'], dtype='datetime64[ns]', freq='D') In [14]: pd.date_range(start='20170101',periods=10,freq='1D') Out[14]: DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-07', '2017-01-08', '2017-01-09', '2017-01-10'], dtype='datetime64[ns]', freq='D') In [15]: pd.date_range(start='20170101',end='20170110',freq='3D',name='dt') Out[15]: DatetimeIndex(['2017-01-01', '2017-01-04', '2017-01-07', '2017-01-10'], dtype='datetime64[ns]', name='dt', freq='3D') In [16]: pd.date_range(start='2017-01-01 08:10:50',periods=10,freq='s',normaliz ...: e=True) Out[16]: DatetimeIndex(['2017-01-01 00:00:00', '2017-01-01 00:00:01', '2017-01-01 00:00:02', '2017-01-01 00:00:03', '2017-01-01 00:00:04', '2017-01-01 00:00:05', '2017-01-01 00:00:06', '2017-01-01 00:00:07', '2017-01-01 00:00:08', '2017-01-01 00:00:09'], dtype='datetime64[ns]', freq='S') In [17]: pd.date_range(start='2017-01-01 08:10:50',end='2017-01-02 09:20:40',fr ...: eq='s',normalize=True) Out[17]: DatetimeIndex(['2017-01-01 00:00:00', '2017-01-01 00:00:01', '2017-01-01 00:00:02', '2017-01-01 00:00:03', '2017-01-01 00:00:04', '2017-01-01 00:00:05', '2017-01-01 00:00:06', '2017-01-01 00:00:07', '2017-01-01 00:00:08', '2017-01-01 00:00:09', ... '2017-01-01 23:59:51', '2017-01-01 23:59:52', '2017-01-01 23:59:53', '2017-01-01 23:59:54', '2017-01-01 23:59:55', '2017-01-01 23:59:56', '2017-01-01 23:59:57', '2017-01-01 23:59:58', '2017-01-01 23:59:59', '2017-01-02 00:00:00'], dtype='datetime64[ns]', length=86401, freq='S') In [18]: pd.date_range(start='2017-01-01 08:10:50',periods=15,freq='s',normaliz ...: e=False) Out[18]: DatetimeIndex(['2017-01-01 08:10:50', '2017-01-01 08:10:51', '2017-01-01 08:10:52', '2017-01-01 08:10:53', '2017-01-01 08:10:54', '2017-01-01 08:10:55', '2017-01-01 08:10:56', '2017-01-01 08:10:57', '2017-01-01 08:10:58', '2017-01-01 08:10:59', '2017-01-01 08:11:00', '2017-01-01 08:11:01', '2017-01-01 08:11:02', '2017-01-01 08:11:03', '2017-01-01 08:11:04'], dtype='datetime64[ns]', freq='S') In [19]: pd.date_range(start='20170101',end='20170110',freq='3D',closed='left') ...: Out[19]: DatetimeIndex(['2017-01-01', '2017-01-04', '2017-01-07'], dtype='dateti me64[ns]', freq='3D') In [20]: pd.date_range(start='20170101',end='20170110',freq='3D',closed='right' ...: ) Out[20]: DatetimeIndex(['2017-01-04', '2017-01-07', '2017-01-10'], dtype='dateti me64[ns]', freq='3D') Update time： 2020-05-25 "},"Chapter2/Pandas 时间序列—period_range函数.html":{"url":"Chapter2/Pandas 时间序列—period_range函数.html","title":"Pandas 时间序列—period_range函数","keywords":"","body":"Pandas 时间序列—period_range函数 pd.period_range(start=None, end=None, periods=None, freq='D', name=None) #时间段范围返回PeriodIndex 参数 start=None : #string or period-like,期间开始 end=None : #string or period-like,期间结束 periods =None: #integer,要生成的周期数 freq='D' : #string or DateOffset name=None : str,# 生成PeriodIndex的名称 生成日期序列 主要提供pd.data_range()和pd.period_range()两个方法，给定参数有起始时间、结束时间、生成时期的数目及时间频率（freq='M’月，'D’天，‘W’，周，'Y’年）等。 两种主要区别在于pd.date_range() 生成的是DatetimeIndex 格式的日期序列；pd.period_range() 生成的是PeriodIndex格式的日期序列。 以下通过生成月时间序列和周时间序列来对比下： date_rng = pd.date_range('2019-01-01', freq='M', periods=12) print(f'month date_range()：\\n{date_rng}') \"\"\" date_range()： DatetimeIndex(['2019-01-31', '2019-02-28', '2019-03-31', '2019-04-30', '2019-05-31', '2019-06-30', '2019-07-31', '2019-08-31', '2019-09-30', '2019-10-31', '2019-11-30', '2019-12-31'], dtype='datetime64[ns]', freq='M') \"\"\" period_rng = pd.period_range('2019/01/01', freq='M', periods=12) print(f'month period_range()：\\n{period_rng}') \"\"\" period_range()： PeriodIndex(['2019-01', '2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-07', '2019-08', '2019-09', '2019-10', '2019-11', '2019-12'], dtype='period[M]', freq='M') \"\"\" date_rng = pd.date_range('2019-01-01', freq='W-SUN', periods=12) print(f'week date_range()：\\n{date_rng}') \"\"\" week date_range()： DatetimeIndex(['2019-01-06', '2019-01-13', '2019-01-20', '2019-01-27', '2019-02-03', '2019-02-10', '2019-02-17', '2019-02-24', '2019-03-03', '2019-03-10', '2019-03-17', '2019-03-24'], dtype='datetime64[ns]', freq='W-SUN') \"\"\" period_rng=pd.period_range('2019-01-01',freq='W-SUN',periods=12) print(f'week period_range()：\\n{period_rng}') \"\"\" week period_range()： PeriodIndex(['2018-12-31/2019-01-06', '2019-01-07/2019-01-13', '2019-01-14/2019-01-20', '2019-01-21/2019-01-27', '2019-01-28/2019-02-03', '2019-02-04/2019-02-10', '2019-02-11/2019-02-17', '2019-02-18/2019-02-24', '2019-02-25/2019-03-03', '2019-03-04/2019-03-10', '2019-03-11/2019-03-17', '2019-03-18/2019-03-24'], dtype='period[W-SUN]', freq='W-SUN') \"\"\" date_rng = pd.date_range('2019-01-01 00:00:00', freq='H', periods=12) print(f'hour date_range()：\\n{date_rng}') \"\"\" hour date_range()： DatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 01:00:00', '2019-01-01 02:00:00', '2019-01-01 03:00:00', '2019-01-01 04:00:00', '2019-01-01 05:00:00', '2019-01-01 06:00:00', '2019-01-01 07:00:00', '2019-01-01 08:00:00', '2019-01-01 09:00:00', '2019-01-01 10:00:00', '2019-01-01 11:00:00'], dtype='datetime64[ns]', freq='H') \"\"\" period_rng=pd.period_range('2019-01-01 00:00:00',freq='H',periods=12) print(f'hour period_range()：\\n{period_rng}') \"\"\" hour period_range()： PeriodIndex(['2019-01-01 00:00', '2019-01-01 01:00', '2019-01-01 02:00', '2019-01-01 03:00', '2019-01-01 04:00', '2019-01-01 05:00', '2019-01-01 06:00', '2019-01-01 07:00', '2019-01-01 08:00', '2019-01-01 09:00', '2019-01-01 10:00', '2019-01-01 11:00'], dtype='period[H]', freq='H') \"\"\" Update time： 2020-05-25 "},"Chapter2/Pandas 时间序列—to_datetime函数.html":{"url":"Chapter2/Pandas 时间序列—to_datetime函数.html","title":"Pandas 时间序列—to_datetime函数","keywords":"","body":"Pandas 时间序列—to_datetime函数 pandas.to_datetime()，它是pandas库的一个方法， 这个方法的实用性在于，当需要批量处理时间数据时，无疑是最好用的。 主要几个参数 pandas.to_datetime( arg, errors ='raise', utc = None, format = None, unit = None ) format 格式化显示时间的格式。如 \"%Y-%m-%d\",..... units 默认值为‘ns’，则将会精确到微妙，‘s'为秒。 errors 三种取值，‘ignore’, ‘raise’, ‘coerce’，默认为raise。 'raise'，则无效的解析将引发异常 'coerce'，那么无效解析将被设置为NaT 'ignore'，那么无效的解析将返回输入值 1、 df = pd.DataFrame({'year': [2015, 2016], 'month': [2, 3], 'day': [4, 5]}) pd.to_datetime(df) #0 2015-02-04 #1 2016-03-05 #dtype: datetime64[ns] #可以看到将字典形式时间转换为可读时间 2、 pd.to_datetime('13000101', format='%Y%m%d', errors='ignore') #datetime.datetime(1300, 1, 1, 0, 0) pd.to_datetime('13000101', format='%Y%m%d', errors='coerce') #NaT #如果日期不符合时间戳限制，则errors ='ignore'将返回原始输入，而不会报错。 #errors='coerce'将强制超出NaT的日期，返回NaT。 然而实际中遇到的可能是这样的数据： 通过pandas.read_csv()或者pandas.read_excel()读取文件过后，得到的数据列对应的类型是“object”，这样没法对时间数据处理，可以用过pd.to_datetime 将该列数据转换为时间类型，即datetime 。 data.dtypes # object data= pd.to_datetime(data) data.dtypes # datetime64[ns] 转换过后就可以对这些时间数据操作了，可以相减求时间差，计算相差的秒数和天数，调用的方法和datetime库的方法一致，分别是 data.dt.days()、data.dt.seconds() 、data.dt.total_seconds() 参考： https://www.pypandas.cn/docs/user_guide/timeseries.html#%E7%BA%B5%E8%A7%88 Update time： 2020-05-25 "},"Chapter2/Pandas 时间序列—to_period函数.html":{"url":"Chapter2/Pandas 时间序列—to_period函数.html","title":"Pandas 时间序列—to_period函数","keywords":"","body":"Pandas 时间序列—to_period函数 Series.dt可用于以datetimelike的形式访问序列的值并返回几个属性。 Pandas Series.dt.to_period()函数以特定频率将给定Series对象的基础数据强制转换为PeriodArray /Index。 参数： freq ：字符串或偏移量，可选 返回值：PeriodArray /索引 范例1： 采用Series.dt.to_period()函数以每周频率将给定系列对象的基础数据转换为索引。 import pandas as pd # Creating the Series sr = pd.Series(['2012-12-31', '2019-1-1 12:30', '2008-02-2 10:30', '2010-1-1 09:25', '2019-12-31 00:00']) # Creating the index idx = ['Day 1', 'Day 2', 'Day 3', 'Day 4', 'Day 5'] # set the index sr.index = idx # Convert the underlying data to datetime sr = pd.to_datetime(sr) # Print the series print(sr) 现在我们将使用Series.dt.to_period()函数以每周频率将给定系列对象的基础数据转换为索引 # cast to targert frequency result = sr.dt.to_period(freq = 'W') # print the result print(result) 正如我们在输出中看到的，Series.dt.to_period()功能已成功将数据投射到目标频率。 # cast to targert frequency result = sr.dt.to_period(freq = 'M') # print the result print(result) 范例2： 采用Series.dt.to_period()函数以两年的频率将给定系列对象的基础数据转换为Index。 # Creating the Series sr = pd.Series(pd.date_range('2012-12-31 00:00', periods = 5, freq = 'D')) # Creating the index idx = ['Day 1', 'Day 2', 'Day 3', 'Day 4', 'Day 5'] # set the index sr.index = idx # Print the series print(sr) 现在我们将使用Series.dt.to_period()函数以两年的频率将给定系列对象的基础数据转换为Index。 # cast to targert frequency result = sr.dt.to_period(freq = '2Y') # print the result print(result) 参考 Python Pandas Series.dt.to_period用法及代码示例 Update time： 2020-05-25 "},"Chapter2/Pandas sample随机抽样.html":{"url":"Chapter2/Pandas sample随机抽样.html","title":"Pandas sample随机抽样","keywords":"","body":"Pandas sample随机抽样 有时候我们只需要数据集中的一部分，并不需要全部的数据。这个时候我们就要对数据集进行随机的抽样。pandas中自带有抽样的方法。 DataFrame.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None) Update time： 2020-05-25 "},"Chapter2/Pandas resample方法详解.html":{"url":"Chapter2/Pandas resample方法详解.html","title":"Pandas resample方法详解","keywords":"","body":"Pandas resample方法详解 Pandas中的 resample，重新采样，是对原样本重新处理的一个方法，是一个对常规时间序列数据重新采样和频率转换的便捷的方法。 DataFrame.resample(rule, how=None, axis=0, fill_method=None, closed=None, label=None, convention='start', kind=None, loffset=None, limit=None, base=0) rule : string 偏移量表示目标字符串或对象转换 axis : int, optional, default 0 closed: {‘right’, ‘left’} 哪一个方向的间隔是关闭的 label : {‘right’, ‘left’} Which bin edge label to label bucket with convention : {‘start’, ‘end’, ‘s’, ‘e’} loffset : timedelta 调整重新取样时间标签 base : int, default 0 频率均匀细分1天,“起源”的聚合的间隔。例如,对于“5分钟”频率,基地可能范围从0到4。默认值为0 首先创建一个Series，采样频率为一分钟。 >>> index = pd.date_range('1/1/2000', periods=9, freq='T') >>> series = pd.Series(range(9), index=index) >>> series 2000-01-01 00:00:00 0 2000-01-01 00:01:00 1 2000-01-01 00:02:00 2 2000-01-01 00:03:00 3 2000-01-01 00:04:00 4 2000-01-01 00:05:00 5 2000-01-01 00:06:00 6 2000-01-01 00:07:00 7 2000-01-01 00:08:00 8 Freq: T, dtype: int64 降低采样频率为三分钟 >>> series.resample('3T').sum() 2000-01-01 00:00:00 3 2000-01-01 00:03:00 12 2000-01-01 00:06:00 21 Freq: 3T, dtype: int64 降低采样频率为三分钟，但是每个标签使用right来代替left。请注意，bucket中值的用作标签。 >>> series.resample('3T', label='right').sum() 2000-01-01 00:03:00 3 2000-01-01 00:06:00 12 2000-01-01 00:09:00 21 Freq: 3T, dtype: int64 降低采样频率为三分钟，但是关闭right区间。 >>> series.resample('3T', label='right', closed='right').sum() 2000-01-01 00:00:00 0 2000-01-01 00:03:00 6 2000-01-01 00:06:00 15 2000-01-01 00:09:00 15 Freq: 3T, dtype: int64 示例 看一个dataframe，index是时间，columns 是 price 和 volume： 该dataframe的时间分布式不均匀的，然而很多统计分析建模都是需要时间均匀间隔的数据的，好在 pandas 为我们提供了 resample 这一函数可以解决这一问题。比如如果我们想要洗成 1min 频率的数据我们就可以这样做： df = df.resample('60S').last() 可以看到经过 resample 函数的重新抽样，新的dataframe已经变成了等间距的分钟级别数据。其中 last() 表示使用每个分钟段区间内的最后一个数据作为该分钟最终显示的数据，如果该分钟内没有任何数据则会显示NaN，比如2014-12-07 00:02:00。除了last之外，还有first，mean，sum 等等。 但是，从新的 dataframe 中我们还是可以观察出两个问题，第一，原 dataframe 的第一个记录是00:00:07，然而这个数据却被自动附到了新dataframe中00:00:00的index上。而事实上在00:00:00时刻，我们并不知道00:00:07发生了什么。第二，对于volume而言一般不应该使用last而应该是用sum函数，代表了这个时间段内的总的交易量，会更有意义。 重新写我们的resample函数如下： price = df['price].resample('60S', label = 'right').last() volume = df['price].resample('60S', label = 'right').sum() df = pd.concat([price, volume], axis = 1) 在这段代码里，首先我们通过设置label的参数去解决第一个问题。在resample函数里，label默认值是left，也就是把一个区间内的数据的计算结果都附到区间左侧端点上，而我们想要的则是相反的情况，也就是label = right。其次，我们通过将price和volume分开处理，price使用last，volume则使用sum函数，达到了更合理的处理效果。最后，我们用concat函数将两个Series合并，就得到了如下结果： 最后，我们还可以利用apply和lambda函数计算每一分钟的VWAP，也就是每分钟交易量加权的价格，代码如下： df.resample('60S', label = 'right').apply(lambda x:np.average(x['price'] ,weights = x['volume']) if sum(x['volume']) != 0 else np.average(x['price']))[['price']] 可以看到，我们用到了numpy里的average函数帮我们计算weighted average，其中weights就是每个时间区段内的所有volume数据。后面我们为了避免错误加了一个判读：如果这个区间内volume加和等于0，也即没有数据我们就直接返回价格平均值（实际上也只会返回NaN）。最后由于resample函数返回的dataframe中price和volume都被附上了新的VWAP值，所以我们通过下标只选择price这一列数据，多用一个中括号是为了索引后得到的仍旧是一个dataframe，打印比较美观。 参考 【金融数据处理Tricks】1. Resample Pandas中resample方法详解 Update time： 2020-05-25 "},"Chapter2/Pandas nlargest函数.html":{"url":"Chapter2/Pandas nlargest函数.html","title":"Pandas nlargest函数","keywords":"","body":"Pandas nlargest函数 nlargest() DataFrame.nlargest(self, n, columns, keep='first') → 'DataFrame' 返回按列降序排列的前n行。 以降序返回column中具有最大值的前n行。未指定的列也将返回，但不用于排序。 此方法等效于 ，但性能更高。df.sort_values(columns, ascending=False).head(n) 参数 n : int要返回的行数。 columns ：标签或标签列表要排序的列标签。 keep ：{'first'，'last'，'all'}，默认为'first'其中有重复的值: 1) first：优先处理第一次出现的事件 2) last：确定最后出现的优先顺序 3) all: 请勿丢弃任何重复项，即使这意味着选择n个以上的项目。 pandas的分组取最大多行 选择不同location_road下的前五名要怎么操作呢？ 很多人可能第一反应会想到先分组然后进行max()操作，但是这样的操作只能选择最大的一列： nsmallest() 用法相同 参考 pandas的分组取最大多行并求和函数 Update time： 2020-05-25 "},"Chapter2/Pandas rename和reindex.html":{"url":"Chapter2/Pandas rename和reindex.html","title":"Pandas rename()和reindex()函数","keywords":"","body":"Pandas rename()和reindex()函数 pandas.DataFrame.rename 更改行名列名。传入的函数或字典值必须是1对1的，没有包含在字典或者Series中的标签将保持原来的名称。字典中包含df中没有的标签，不会报错。 DataFrame.rename(mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False, level=None) 参数： mapper, index, columns : 映射的规则。 axis：指定轴，可以是轴名称（'index'，'columns'）或数字（0,1），默认为index。 copy：布尔值，默认为True，复制底层数据。 inplace：布尔值，默认为False。指定是否返回新的DataFrame。如果为True，则在原df上修改，返回值为None。 level：int或level name，默认为None。如果是MultiIndex，只重命名指定级别的标签。 返回值： DataFrame 两种修改的方法： (index=index_mapper, columns=columns_mapper, ...) (mapper, axis={'index', 'columns'}, ...) In [1]: import numpy as np^M ...: import pandas as pd In [2]: list_l = [[1, 3, 3, 5, 4], [11, 7, 15, 13, 9], [4, 2, 7, 9, 3], [15, 11, 12, 6, 11]] ...: index = [\"2018-07-01\", \"2018-07-02\", \"2018-07-03\", \"2018-07-04\"] ...: df = pd.DataFrame(list_l, index=index, columns=['a', 'b', 'c', 'd', 'e']) In [3]: df Out[3]: a b c d e 2018-07-01 1 3 3 5 4 2018-07-02 11 7 15 13 9 2018-07-03 4 2 7 9 3 2018-07-04 15 11 12 6 11 # 第一种修改方法 In [4]: df_rename = df.rename({'a': 'rename_a', 'b': 'rename_b', 'c': 'rename_c', 'd': 'rename_d', 'e': 'rename_e'}, ...: axis='columns') In [5]: df_rename Out[5]: rename_a rename_b rename_c rename_d rename_e 2018-07-01 1 3 3 5 4 2018-07-02 11 7 15 13 9 2018-07-03 4 2 7 9 3 2018-07-04 15 11 12 6 11 # 第二种修改方法 In [6]: df_rename = df.rename(index={\"2018-07-01\": 71, \"2018-07-02\": 72, \"2018-07-03\": 73, \"2018-07-04\": 74}, ...: columns={'a': 'rename_a', 'b': 'rename_b', 'c': 'rename_c', 'd': 'rename_d', 'e': 'rename_e'}) In [7]: df_rename Out[7]: rename_a rename_b rename_c rename_d rename_e 71 1 3 3 5 4 72 11 7 15 13 9 73 4 2 7 9 3 74 15 11 12 6 11 当inplace=True 时，返回值为 None，函数会在原df上进行修改。 In [9]: df_rename = df.rename(index={\"2018-07-01\": 71, \"2018-07-02\": 72, \"2018-07-03\": 73, \"2018-07-04\": 74},^M ...: columns={'a': 'rename_a', 'b': 'rename_b', 'c': 'rename_c', 'd': 'rename_d', 'e': 'rename ...: _e'}, inplace=True) In [10]: df Out[10]: rename_a rename_b rename_c rename_d rename_e 71 1 3 3 5 4 72 11 7 15 13 9 73 4 2 7 9 3 74 15 11 12 6 11 pandas.DataFrame.reindex 重建索引，对于索引没有对应值的，使用 NaN 或者指定的值填充。 DataFrame.reindex(labels=None, index=None, columns=None, axis=None, method=None, copy=True, level=None, fill_value=nan, limit=None, tolerance=None) 参数： method : 用于填充重建索引的 DataFrame 中的缺失值的方法。（请注意：仅适用于具有单调递增/递减 index 的 DataFrames / Series。） 默认：不填充 pad / ffill: 使用上一个有效观察填充 backfill / bfill: 使用下一个有效的观察值填充 nearest: 使用最靠近的有效值来填充空白 copy: 布尔值，默认为 True，返回的是一个新的 df 对象（而不是在原对象上修改）。 fill_value : 标量，默认为 np.NaN，可以是任意的值。用于填充缺失的值。 返回 : 重建索引新 DataFrame 对象。 DataFrame.reindex 支持两种方法： (index=index_labels, columns=column_labels, ...) (labels, axis={'index', 'columns'}, ...) import numpy as np import pandas as pd import matplotlib.pyplot as plt list_l = [[1, 3, 3, 5, 4.], [11, 7, 15, 13, 9.1], [4, 2, 7, 9, 3.5], [15, 11, 12, 6, 11.1]] index = [\"2018-07-01\", \"2018-07-02\", \"2018-07-03\", \"2018-07-04\"] df = pd.DataFrame(list_l, index=index, columns=['a', 'b', 'c', 'd', 'e']) print(df) \"\"\" a b c d e 2018-07-01 1 3 3 5 4.0 2018-07-02 11 7 15 13 9.1 2018-07-03 4 2 7 9 3.5 2018-07-04 15 11 12 6 11.1 \"\"\" # index new_index = [\"2018-07-04\", \"2018-07-05\", \"2018-07-06\"] df_reindex = df.reindex(index=new_index) df_reindex2 = df.reindex(labels=new_index, axis=\"index\") print(df_reindex) print(df_reindex2) \"\"\" a b c d e 2018-07-04 15.0 11.0 12.0 6.0 11.1 2018-07-05 NaN NaN NaN NaN NaN 2018-07-06 NaN NaN NaN NaN NaN \"\"\" # columns new_columns = [\"c\", \"d\", \"e\", \"f\"] df_re = df.reindex(columns=new_columns) df_re2 = df.reindex(labels=new_columns, axis=\"columns\") print(df_re) print(df_re2) \"\"\" c d e f 2018-07-01 3 5 4.0 NaN 2018-07-02 15 13 9.1 NaN 2018-07-03 7 9 3.5 NaN 2018-07-04 12 6 11.1 NaN \"\"\" 参考 【pandas】 之 rename、reindex Update time： 2020-05-25 "},"Chapter2/Pandas set_index和reset_index函数.html":{"url":"Chapter2/Pandas set_index和reset_index函数.html","title":"Pandas set_index()和reset_index()函数","keywords":"","body":"Pandas set_index()和reset_index()函数 set_index() 作用：DataFrame可以通过set_index方法，将普通列设置为单索引/复合索引。 格式：DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False) 参数含义： keys：列标签或列标签/数组列表，需要设置为索引的普通列 drop：是否删除原普通列，默认为True，删除用作新索引的原普通列； append：是否变成复合索引，默认为False，即覆盖原索引，单索引； inplace：默认为False，适当修改DataFrame(不要创建新对象)； verify_integrity：默认为false，检查新索引的副本。否则，请将检查推迟到必要时进行。将其设置为false将提高该方法的性能。 参数drop # drop的使用： import pandas as pd df = pd.DataFrame({ 'A': ['A0', 'A1', 'A2', 'A3','A4'], 'B': ['B0', 'B1', 'B2', 'B3','B4'], 'C': ['C0', 'C1', 'C2', 'C3','C4'], 'D': ['D0', 'D1', 'D2', 'D3','D4']}) print ('输出结果:\\n',df) print('------') df_drop_t = df.set_index('A',drop=True) # drop默认True，普通列被用作索引后，原列删除 print (df_drop_t) print('------') df_drop_f = df.set_index('A',drop=False) # 普通列被用作索引后，原列保留 print (df_drop_f) ''' 输出结果: A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 ------ B C D A A0 B0 C0 D0 A1 B1 C1 D1 A2 B2 C2 D2 A3 B3 C3 D3 A4 B4 C4 D4 ------ A B C D A A0 A0 B0 C0 D0 A1 A1 B1 C1 D1 A2 A2 B2 C2 D2 A3 A3 B3 C3 D3 A4 A4 B4 C4 D4 ''' 参数 append # append的使用 import pandas as pd df = pd.DataFrame({ 'A': ['A0', 'A1', 'A2', 'A3','A4'], 'B': ['B0', 'B1', 'B2', 'B3','B4'], 'C': ['C0', 'C1', 'C2', 'C3','C4'], 'D': ['D0', 'D1', 'D2', 'D3','D4']}) print ('输出结果:\\n',df) print('------') df_append_f = df.set_index('A', append=False) # append默认为False，普通列变为索引，并覆盖原索引，原索引被删除 print (df_append_f) df_append_t = df.set_index('A', append=True) # 表示将普通列变为索引，原索引保留，变成了复合索引 print (df_append_t) print('------') ''' 输出结果: A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 ------ B C D A A0 B0 C0 D0 A1 B1 C1 D1 A2 B2 C2 D2 A3 B3 C3 D3 A4 B4 C4 D4 ------ B C D A 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 ''' 参数inplace # inplace的使用，输出None df = pd.DataFrame({ 'A': ['A0', 'A1', 'A2', 'A3','A4'], 'B': ['B0', 'B1', 'B2', 'B3','B4'], 'C': ['C0', 'C1', 'C2', 'C3','C4'], 'D': ['D0', 'D1', 'D2', 'D3','D4']}) df_inplace_f = df.set_index('A', inplace=False) # inpla默认为False，表示适当修改DataFrame(不要创建新对象) print ('输出结果：\\n',df_inplace_f) print('------') df1 = pd.DataFrame({ 'A': ['A0', 'A1', 'A2', 'A3','A4'], 'B': ['B0', 'B1', 'B2', 'B3','B4'], 'C': ['C0', 'C1', 'C2', 'C3','C4'], 'D': ['D0', 'D1', 'D2', 'D3','D4']}) df_inplace_t = df1.set_index('A',inplace=True) # 表示原地不动 print (df_inplace_t) print (type(df_inplace_t)) ''' 输出结果： B C D A A0 B0 C0 D0 A1 B1 C1 D1 A2 B2 C2 D2 A3 B3 C3 D3 A4 B4 C4 D4 ------ None ''' reset_index() 作用：reset_index可以还原索引为普通列，重新变为默认的整型索引 （注：reset_index还原分为两种类型，第一种是对原DataFrame进行reset，第二种是对使用过set_index()函数的DataFrame进行reset） 格式：DataFrame.reset_index(level=None, drop=False, inplace=False, col_level=0, col_fill=”) 参数含义： level：int、str、tuple或list，默认无，仅从索引中删除给定级别。默认情况下移除所有级别。控制了具体要还原的那个等级的索引 drop：索引被还原成普通列后，是否删掉列。默认为False，为False时则索引列会被还原为普通列，否则被还原后的的列又会被瞬间删掉； inplace：默认为false，适当修改DataFrame(不要创建新对象)； col_level：int或str，默认值为0，如果列有多个级别，则确定将标签插入到哪个级别。默认情况下，它将插入到第一级； col_fill：对象，默认‘’，如果列有多个级别，则确定其他级别的命名方式。如果没有，则重复索引名； 对原DataFrame进行reset # 一般情况下参数只使用到drop，这里只演示drop的使用 import pandas as pd df = pd.DataFrame({ 'A': ['A0', 'A1', 'A2', 'A3','A4'], 'B': ['B0', 'B1', 'B2', 'B3','B4'], 'C': ['C0', 'C1', 'C2', 'C3','C4'], 'D': ['D0', 'D1', 'D2', 'D3','D4']}) print ('输出结果\\ndf:\\n',df) print('------') df1 = df.reset_index(drop=False) # 默认为False，原有的索引不变,添加一列，列名index； print (df1) print('------') df2 = df.reset_index(drop=True) # 索引被还原为普通列，瞬间又被删掉了，同时在原位置重置原始索引012...； print (df2) ''' 输出结果 df: A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 ------ index A B C D 0 0 A0 B0 C0 D0 1 1 A1 B1 C1 D1 2 2 A2 B2 C2 D2 3 3 A3 B3 C3 D3 4 4 A4 B4 C4 D4 ------ A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 ''' 对使用过set_index()函数的DataFrame进行reset # 一般情况下参数只使用到drop，这里只演示drop的使用 import pandas as pd df = pd.DataFrame({ 'A': ['A0', 'A1', 'A2', 'A3','A4'], 'B': ['B0', 'B1', 'B2', 'B3','B4'], 'C': ['C0', 'C1', 'C2', 'C3','C4'], 'D': ['D0', 'D1', 'D2', 'D3','D4']}) print ('输出结果：\\ndf:\\n' ,df) print('------') newdf = df.set_index('A') # 这里的drop必需为True（默认为这里的drop必需为True），否则会报错ValueError: cannot insert A, already exists（意思是...只可意会不可言传哈哈） print (newdf) print('------') newdf1 = newdf.reset_index(drop=False) #索引列会被还原为普通列 print (newdf1) print('------') newdf2 = newdf.reset_index(drop=True) #索引被还原为普通列，瞬间又被删掉了，同时在原位置重置原始索引； print (newdf2) ''' 输出结果： df: A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 ------ B C D A A0 B0 C0 D0 A1 B1 C1 D1 A2 B2 C2 D2 A3 B3 C3 D3 A4 B4 C4 D4 ------ A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 ------ B C D 0 B0 C0 D0 1 B1 C1 D1 2 B2 C2 D2 3 B3 C3 D3 4 B4 C4 D4 ''' 参考 区别 python-pandas库set_index、reset_index用法区别 Update time： 2020-05-25 "},"Chapter2/Pandas pd.cut函数.html":{"url":"Chapter2/Pandas pd.cut函数.html","title":"Pandas pd.cut()函数","keywords":"","body":"Pandas pd.cut()函数 将数据进行离散化 pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False ) 参数 x : 进行划分的一维数组 bins: 整数---将 x 划分为多少个等间距的区间 序列—将 x 划分在指定的序列中，若不在该序列中，则是NaN right : 是否包含右端点 labels : 是否用标记来代替返回的bins precision: 精度 include_lowest: 是否包含左端点 返回值: 如果 retbins = False 则返回x中每个值对应的bin的列表，否者则返回x中每个值对应的bin的列表和对应的bins pd.cut( 的作用，有点类似给成绩设定优良中差，比如：0-59分为差，60-70分为中，71-80分为优秀等等，在pandas中，也提供了这样一个方法来处理这些事儿。 import numpy as np import pandas as pd np.random.seed(666) score_list = np.random.randint(25,100,20) score_list array([27, 70, 55, 87, 95, 98, 55, 61, 86, 76, 85, 53, 39, 88, 41, 71, 64, 94, 38, 94]) # 指定多个区间 bins = [0, 59, 70, 80, 100] score_cut = pd.cut(score_list, bins) type(score_cut) # pandas.core.arrays.categorical.Categorical score_cut [(0, 59], (59, 70], (0, 59], (80, 100], (80, 100], ..., (70, 80], (59, 70], (80, 100], (0, 59], (80, 100]] Length: 20 Categories (4, interval[int64]): [(0, 59] # 统计每个区间的人数 pd.value_counts(score_cut) (80, 100] 8 (0, 59] 7 (59, 70] 3 (70, 80] 2 dtype: int64 df = pd.DataFrame() df['score'] = score_list df['student'] = [pd.util.testing.rands(3) for i in range(len(score_list))] df score student 0 27 1ul 1 70 yuK 2 55 WWK 3 87 EU6 4 95 Vqn 5 98 KAf 6 55 QNT 7 61 HaE 8 86 aBo 9 76 MMa 10 85 Ctc 11 53 5BI 12 39 wBp 13 88 WMB 14 41 q5t 15 71 MjZ 16 64 nTc 17 94 Kyx 18 38 Rlh 19 94 2uV # 使用cut方法进行分箱 pd.cut(df['score'], bins) 0 (0, 59] 1 (59, 70] 2 (0, 59] 3 (80, 100] 4 (80, 100] 5 (80, 100] 6 (0, 59] 7 (59, 70] 8 (80, 100] 9 (70, 80] 10 (80, 100] 11 (0, 59] 12 (0, 59] 13 (80, 100] 14 (0, 59] 15 (70, 80] 16 (59, 70] 17 (80, 100] 18 (0, 59] 19 (80, 100] Name: score, dtype: category Categories (4, interval[int64]): [(0, 59] df['Categories'] = pd.cut(df['score'], bins) df score student Categories 0 27 1ul (0, 59] 1 70 yuK (59, 70] 2 55 WWK (0, 59] 3 87 EU6 (80, 100] 4 95 Vqn (80, 100] 5 98 KAf (80, 100] 6 55 QNT (0, 59] 7 61 HaE (59, 70] 8 86 aBo (80, 100] 9 76 MMa (70, 80] 10 85 Ctc (80, 100] 11 53 5BI (0, 59] 12 39 wBp (0, 59] 13 88 WMB (80, 100] 14 41 q5t (0, 59] 15 71 MjZ (70, 80] 16 64 nTc (59, 70] 17 94 Kyx (80, 100] 18 38 Rlh (0, 59] 19 94 2uV (80, 100] # 但是这样的方法不是很适合阅读，可以使用cut方法中的label参数 # 为每个区间指定一个label df['Categories'] = pd.cut(df['score'], bins, labels=[ 'low', 'middle', 'good', 'perfect']) df 参考： pandas中pd.cut()的功能和作用 Update time： 2020-05-25 "},"Chapter2/Pandas pd.Categorical函数.html":{"url":"Chapter2/Pandas pd.Categorical函数.html","title":"Pandas pd.Categorical()函数","keywords":"","body":"Pandas pd.Categorical()函数 pandas.Categorical(values, categories = None, ordered = None, dtype = Nonel fastpath = False ) 参数： values：类似列表。分类的值，如果给出了类别，则不在类别中的值将替换为NaN。 categories：索引式（唯一），可选。此分类的唯一类别。如果没有给出，则假定类别是值的唯一值。 ordered：布尔值，（默认为False）。这个分类是否被当作一个有序的分类。如果为True，产生的分类将是有序的。有序分类在排序时尊重其类别属性的顺序（如果提供了类别参数，则该属性也是类别参数 dtype：CategoricalDtype，CategoricalDtype用于此分类的实例 示例 import pandas as pd cat = pd.Categorical(['a', 'b', 'c', 'a', 'b', 'c']) cat 结果 ： [a, b, c, a, b, c] Categories (3, object): [a, b, c] 参数 categories= import pandas as pd cat = cat=pd.Categorical(['a','b','c','a','b','c','d'], categories=['c', 'b', 'a']) print (cat) 结果 [a, b, c, a, b, c, NaN] Categories (3, object): [c, b, a] 在类别中不存在的任何值将被视为NaN 参数 ordered import pandas as pd cat = cat=pd.Categorical(['a','b','c','a','b','c','d'], categories=['c', 'b', 'a'], ordered=True) print (cat) 结果： [a, b, c, a, b, c, NaN] Categories (3, object): [c 从逻辑上讲，排序(ordered)意味着，a大于b，b大于c 这里就可以看到 categorical 实际上是计算一个列表型数据中的类别数，即不重复项，它返回的是一个CategoricalDtype 类型的对象，相当于在原来数据上附加上类别信息 属性 codes和 categories 具体的类别可以通过和对应的序号可以通过codes和 categories来查看： In [23]: ss.codes Out[23]: array([0, 0, 1, 2, 2], dtype=int8) In [21]: ss.categories Out[21]: Index(['a', 'b', 'c'], dtype='object') 有序分类可以根据类别的自定义顺序进行排序，并且可以具有最小值和最大值。 >>>c = pd.Categorical(['a','b','c','a','b','c'], ordered=True, categories=['c', 'b', 'a']) >>> c [a, b, c, a, b, c] Categories (3, object): [c >> c.min() 'c' 属性 categories 这种分类的类别。 codes 此类别的类别代码。 ordered 类别是否具有有序关系 dtype 在CategoricalDtype此实例 参考 pandas 中 pd.Categorical用法 pandas.Categorical Pandas分类数据 Update time： 2020-07-11 "},"Chapter2/Pandas pd.get_dummies函数.html":{"url":"Chapter2/Pandas pd.get_dummies函数.html","title":"Pandas pd.get_dummies()函数","keywords":"","body":"Pandas pd.get_dummies()函数 get_dummies 是利用 pandas 实现one hot encode的方式。详细参数请查看官方文档 官方文档在这里 pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False ) 参数说明： data : array-like, Series, or DataFrame 输入的数据 prefix : string, list of strings, or dict of strings, default None get_dummies转换后，列名的前缀 columns: list-like, default None 指定需要实现类别转换的列名 dummy_na : bool, default False 增加一列表示空缺值，如果False就忽略空缺值 drop_first : bool, default False 获得k中的k-1个类别值，去除第一个 案例 import pandas as pd df = pd.DataFrame([ ['green' , 'A'], ['red' , 'B'], ['blue' , 'A']]) df.columns = ['color', 'class'] pd.get_dummies(df) get_dummies 前： color class 0 green A 1 red B 2 blue A get_dummies 后： color_blue color_green color_red class_A class_B 0 0 1 0 1 0 1 0 0 1 0 1 2 1 0 0 1 0 可以对指定列进行 get_dummies pd.get_dummies(df.color) Update time： 2020-05-25 "},"Chapter2/Pandas pd.Series.to_dict函数.html":{"url":"Chapter2/Pandas pd.Series.to_dict函数.html","title":"Pandas pd.Series.to_dict()函数","keywords":"","body":"Pandas pd.Series.to_dict()函数 Series.to_dict(self, into=) 参数： orient : str {‘dict’, ‘list’, ‘series’, ‘split’, ‘records’, ‘index’} dict (default) : {column -> {index -> value}} list: {column -> [values]} series : {column -> Series(values)} split : {‘index’ -> [index], ‘columns’ -> [columns], ‘data’ -> [values]} records : [{column -> value}, … , {column -> value}] index: {index -> {column -> value}} 返回 dict, list 或 collections.Mapping 选择参数 orient=dict dict也是默认的参数，下面的data数据类型为DataFrame结构, 会形成 {column -> {index -> value}}这样的结构的字典，可以看成是一种双重字典结构 单独提取每列的值及其索引，然后组合成一个字典 再将上述的列属性作为关键字（key），值（values）为上述的字典 查询方式为 ：data_dict[key1][key2] data_dict 为参数选择orient=’dict’时的数据名 key1 为列属性的键值（外层） key2 为内层字典对应的键值 >>> df = pd.DataFrame({'col1': [1, 2], ... 'col2': [0.5, 0.75]}, ... index=['row1', 'row2']) >>> df col1 col2 row1 1 0.50 row2 2 0.75 >>> df.to_dict() {'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}} 当关键字 orient= list 和1中比较相似，只不过内层变成了一个列表，结构为{column -> [values]} 查询方式为： data_list[keys][index] data_list 为关键字orient=’list’ 时对应的数据名 keys 为列属性的键值， index 为整型索引，从0开始到最后 >>> df.to_dict('list') # {'col1': [1, 2], 'col2': [0.5, 0.75]} df.to_dict('list')['col2'][1] # 0.75 关键字参数orient=series 形成结构{column -> Series(values)} 调用格式为：data_series[key1][key2]或 data_dict[key1] data_series 为数据对应的名字 key1 为列属性的键值 key2 使用数据原始的索引（可选） >>> df.to_dict('series') {'col1': row1 1 row2 2 Name: col1, dtype: int64, 'col2': row1 0.50 row2 0.75 Name: col2, dtype: float64} 关键字参数 orient=split 形成{index -> [index], columns -> [columns], data -> [values]}的结构，是将数据、索引、属性名单独脱离出来构成字典 调用方式有 data_split[‘index’],data_split[‘data’],data_split[‘columns’] df.to_dict('split') {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'], 'data': [[1, 0.5], [2, 0.75]]} df.to_dict('split')['data'] # [[1, 0.5], [2, 0.75]] 当关键字orient=records 形成[{column -> value}, … , {column -> value}]的结构 整体构成一个列表，内层是将原始数据的每行提取出来形成字典 调用格式为data_records[index][key1] df.to_dict(orient='records') # [{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}] 当关键字orient=index 形成{index -> {column -> value}}的结构，调用格式正好和’dict’ 对应的反过来 df.to_dict(orient='index') # {'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}} 参考 pandas to_dict 的用法 Update time： 2020-05-25 "},"Chapter2/Pandas Series.to_frame.html":{"url":"Chapter2/Pandas Series.to_frame.html","title":"Pandas Series.to_frame","keywords":"","body":"Pandas Series.to_frame Pandas 系列是带有轴标签的一维ndarray。标签不必是唯一的，但必须是可哈希的类型。该对象同时支持基于整数和基于标签的索引，并提供了许多方法来执行涉及索引的操作。 Pandas Series.to_frame()函数用于将给定的系列对象转换为 DataFrame 。 用法：Series.to_frame(name=None) 参数： name:传递的名称应替换系列名称(如果有的话)。 返回：data_frame：DataFrame # importing pandas as pd import pandas as pd # Creating the Series sr = pd.Series(['New York', 'Chicago', 'Toronto', 'Lisbon', 'Rio', 'Moscow']) # Create the Datetime Index didx = pd.DatetimeIndex(start ='2014-08-01 10:00', freq ='W', periods = 6, tz = 'Europe/Berlin') # set the index sr.index = didx # Print the series print(sr) 输出 现在我们将使用Series.to_frame()函数将给定的系列对象转换为 DataFrame # convert to dataframe sr.to_frame() 参考 Python Pandas Series.to_frame() Update time： 2020-05-25 "},"Chapter2/Pandas 数据去重.html":{"url":"Chapter2/Pandas 数据去重.html","title":"Pandas 数据去重","keywords":"","body":"Pandas 数据去重 数据去重可以使用 duplicated() 和 drop_duplicates() 两个方法。 drop_duplicates() df.drop_duplicates(subset=None, keep='first', inplace=False) drop_duplicate 方法是对 DataFrame 格式的数据，去除特定列下面的重复行。返回DataFrame 格式的数据。 参数 subset：可以指定传入单个列标签或者一个列标签的列表，默认是使用所有的列标签，即会删除一整行 keep：有 {‘first’, ‘last’, False} 三个可供选择, 默认值 ‘first’，意味着除了第一个, 后面重复的全部删除，keep='first’表示保留第一次出现的重复行，是默认值。keep另外两个取值为\"last\"和False，分别表示保留最后一次出现的重复行和去除所有重复行。 inplace : 返回是否替代过的值，默认False,即不改变原数据。 返回值 DataFrame DataFrame with duplicates removed or None if inplace=True. data = pd.DataFrame({'A':[1,1,2,2],'B':['a','b','a','b']}) data.drop_duplicates('B','first',inplace=True) data A B 0 1 a 1 1 b duplicated() DataFrame.duplicated（subset = None，keep =‘first’ ）返回boolean Series表示重复行 参数： subset：列标签或标签序列，可选 仅考虑用于标识重复项的某些列，默认情况下使用所有列 keep：{‘first’，‘last’，False}，默认’first’ first：标记重复，True除了第一次出现。 last：标记重复，True除了最后一次出现。 错误：将所有重复项标记为True。 返回值：Series Boolean series for each duplicated rows. 重复值标记为 True, 不是重复值标记为Falese ​ data['B'].duplicated() 0 False 1 False 2 True 3 True Name: B, dtype: bool data[data['A'].duplicated()] A B 2 2 a 3 2 b Update time： 2020-09-01 "},"Chapter2/Pandas drop函数.html":{"url":"Chapter2/Pandas drop函数.html","title":"Pandas drop函数","keywords":"","body":"Pandas drop函数 删除表中的某一行或者某一列更明智的方法是使用drop，它不改变原有的df中的数据，而是返回另一个dataframe来存放删除后的数据。 清理无效数据 df[df.isnull()] #返回的是个true或false的Series对象（掩码对象），进而筛选出我们需要的特定数据。 df[df.notnull()] df.dropna() #将所有含有nan项的row删除 df.dropna(axis=1,thresh=3) #将在列的方向上三个为NaN的项删除 df.dropna(how='ALL') #将全部项都是nan的row删除 drop函数的使用 删除行、删除列 print frame.drop(['a']) print frame.drop(['Ohio'], axis = 1) drop 函数默认删除行，列需要加axis = 1 inplace参数 1. DF= DF.drop('column_name', axis=1)； 2. DF.drop('column_name',axis=1, inplace=True) 3. DF.drop([DF.columns[[0,1, 3]]], axis=1, inplace=True) # Note: zero indexed 注意：凡是会对原数组作出修改并返回一个新数组的，往往都有一个 inplace可选参数。如果手动设定为True（默认为False），那么原数组直接就被替换。也就是说，采用 inplace=True 之后，原数组名（如2和3情况所示）对应的内存值直接改变； 而采用 inplace=False 之后，原数组名对应的内存值并不改变，需要将新的结果赋给一个新的数组或者覆盖原数组的内存位置（如1情况所示）。 数据类型转换 df['Name'] = df['Name'].astype(np.datetime64) DataFrame.astype()方法可对整个 DataFrame 或某一列进行数据格式转换，支持 Python 和 NumPy 的数据类型。 Update time： 2020-05-25 "},"Chapter2/Pandas 唯一值，计数值.html":{"url":"Chapter2/Pandas 唯一值，计数值.html","title":"Pandas 唯一值，计数值","keywords":"","body":"Pandas 唯一值，计数值 unique() pd.unique(Series) 获取 Series 中元素的唯一值（即去掉重复的） 计算 Series 中的唯一值数组，按发现的顺序返回。 obj=pd.Series(['c','a','d','a','a','b','b','c','c','c']) print(obj.unique()) # ['c' 'a' 'd' 'b'] value_counts() pd.value_counts(Series) 统计 Series 中不同元素出现的次数 print(obj.value_counts()) ''' c 4 a 3 b 2 d 1 dtype: int64 ''' isin() In [144]: obj.isin(['a','b']) Out[144]: 0 False 1 True 2 False 3 True 4 True 5 True 6 True 7 False 8 False 9 False dtype: bool pd.cut() pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False ) 参数 x : 进行划分的一维数组 bins : bins 是被切割后的区间（或者叫“桶”、“箱”、“面元”），有3中形式：一个int型的标量、标量序列（数组）或者pandas.IntervalIndex 。 一个int型的标量 当bins为一个int型的标量时，代表将x平分成bins份。x的范围在每侧扩展0.1%，以包括x的最大值和最小值。 标量序列 标量序列定义了被分割后每一个bin的区间边缘，此时x没有扩展。 pandas.IntervalIndex 定义要使用的精确区间 。 right : bool型参数， 默认为True，表示是否包含区间右部。比如如果bins=[1,2,3]，right=True，则区间为(1,2]，(2,3]；right=False，则区间为(1,2),(2,3)。 labels：给分割后的 bins 打标签，比如把年龄 x 分割成年龄段 bins 后，可以给年龄段打上诸如青年、中年的标签。 labels 的长度必须和划分后的区间长度相等，比如bins=[1,2,3]，划分后有2个区间(1,2]，(2,3]，则labels的长度必须为2。如果指定 labels = False，则返回x中的数据在第几个bin中（从0开始）。 retbins：bool 型的参数，表示是否将分割后的bins返回，当bins为一个int型的标量时比较有用，这样可以得到划分后的区间，默认为False。 precision: 保留区间小数点的位数，默认为3. include_lowest :bool型的参数， 表示区间的左边是开还是闭的，默认为false，也就是不包含区间左部（闭）。 返回 out：一个pandas.Categorical, Series或者ndarray类型的值，代表分区后x中的每个值在哪个bin（区间）中，如果指定了labels，则返回对应的label。 bins：分隔后的区间，当指定retbins为True时返回。 import numpy as np import pandas as pd # 年龄数据 ages = np.array([1,5,10,40,36,12,58,62,77,89,100,18,20,25,30,32]) # 将ages平分成5个区间 pd.cut(ages, 5) [(0.901, 20.8], (0.901, 20.8], (0.901, 20.8], (20.8, 40.6], (20.8, 40.6], ..., (0.901, 20.8], (0.901, 20.8], (20.8, 40.6], (20.8, 40.6], (20.8, 40.6]] Length: 16 Categories (5, interval[float64]): [(0.901, 20.8] 可以看到 ages 被平分成 5 个区间，且区间两边都有扩展以包含最大值和最小值。 将 ages 平分成 5 个区间并指定 labels ages = np.array([1,5,10,40,36,12,58,62,77,89,100,18,20,25,30,32]) #年龄数据 pd.cut(ages, 5, labels=[u\"婴儿\",u\"青年\",u\"中年\",u\"壮年\",u\"老年\"]) [婴儿, 婴儿, 婴儿, 青年, 青年, ..., 婴儿, 婴儿, 青年, 青年, 青年] Length: 16 Categories (5, object): [婴儿 给ages指定区间进行分割 ages = np.array([1,5,10,40,36,12,58,62,77,89,100,18,20,25,30,32]) #年龄数据 pd.cut(ages, [0,5,20,30,50,100], labels=[u\"婴儿\",u\"青年\",u\"中年\",u\"壮年\",u\"老年\"]) [婴儿, 婴儿, 青年, 壮年, 壮年, ..., 青年, 青年, 中年, 中年, 壮年] Length: 16 Categories (5, object): [婴儿 这里不再平分 ages，而是将 ages 分为了 5 个区间 (0, 5],(5, 20],(20, 30],(30,50],(50,100]. 返回分割后的 bins 令labels=False即可 ages = np.array([1,5,10,40,36,12,58,62,77,89,100,18,20,25,30,32]) #年龄数据 pd.cut(ages, [0,5,20,30,50,100], labels=False) array([0, 0, 1, 3, 3, 1, 4, 4, 4, 4, 4, 1, 1, 2, 2, 3], dtype=int64) 参考 pandas.cut pandas.cut使用总结 Update time： 2020-05-25 "},"Chapter2/PDataFrame nunique.html":{"url":"Chapter2/PDataFrame nunique.html","title":"DataFrame.nunique","keywords":"","body":"DataFrame.nunique DataFrame.nunique(axis=0, dropna=True) 统计指定轴上不重复值得个数。 Return Series with number of distinct observations. Can ignore NaN values. Parameters axis ：{0 or ‘index’, 1 or ‘columns’}, default 0 The axis to use. 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise. dropna ：bool, default True Don’t include NaN in the counts. Examples df = pd.DataFrame({'A': [1, 2, 3], 'B': [1, 1, 1]}) df.nunique() A 3 B 1 dtype: int64 m = df.nunique(axis=0) m.index #Index(['A', 'B'], dtype='object') m.values #array([3, 1], dtype=int64) df.nunique(axis=1) 0 1 1 2 2 2 dtype: int64 Update time： 2020-09-07 "},"Chapter2/Pandas transform函数.html":{"url":"Chapter2/Pandas transform函数.html","title":"Pandas transform函数","keywords":"","body":"Pandas transform函数 DataFrame.transform(func, axis=0, *args, **kwargs) 参数 func : function, str, list 或 dict 用于转换数据的函数。如果是函数，则必须在传递DataFrame 或传递到DataFrame.apply时工作 axis : {0 or ‘index’, 1 或 ‘columns’}, 默认 0 如果0或' index ':应用函数到每一列。 如果1或‘columns’:应用函数到每一行。 返回值：DataFrame 必须具有与自身相同长度的DataFrame。 特点 transform 方法通常是和 groupby 方法一起连用的 产生一个标量值，并且广播到各分组的尺寸数据中 transform可以产生一个和输入尺寸相同的对象 transform不可改变它的输入 例子， >>> df = pd.DataFrame({'A': range(3), 'B': range(1, 4)}) >>> df A B 0 0 1 1 1 2 2 2 3 >>> df.transform(lambda x: x + 1) A B 0 1 2 1 2 3 2 3 4 即使得到的DataFrame必须与输入DataFrame具有相同的长度，也可以提供几个输入函数: >>> s = pd.Series(range(3)) >>> s 0 0 1 1 2 2 dtype: int64 >>> s.transform([np.sqrt, np.exp]) sqrt exp 0 0.000000 1.000000 1 1.000000 2.718282 2 1.414214 7.389056 2 df = pd.DataFrame({ \"key\":[\"a\",\"b\",\"c\"]*2 , \"values\":np.arange(6) }) df key values 0 a 0 1 b 1 2 c 2 3 a 3 4 b 4 5 c 5 分组 g = df.groupby(\"key\").values # 分组再求平均 g.mean() key a 1.5 b 2.5 c 3.5 Name: values, dtype: float64 transform使用 每个位置被均值取代 g.transform(lambda x:x.mean()) 0 1.5 1 2.5 2 3.5 3 1.5 4 2.5 5 3.5 Name: values, dtype: float64 传递agg方法中的函数字符串别名 内建的聚合函数直接传递别名，max\\min\\sum\\mean g.transform(\"mean\") 0 1.5 1 2.5 2 3.5 3 1.5 4 2.5 5 3.5 Name: values, dtype: float64 降序排名 g.transform(lambda x:x.rank(ascending=False)) 0 2 1 2 2 2 3 1 4 1 5 1 Name: values, dtype: int32 transform 函数和 apply 函数的区别 transform 函数： ​ 1.只允许在同一时间在一个Series上进行一次转换，如果定义列‘a’ 减去列‘b’， 则会出现异常； ​ 2.必须返回与 group相同的单个维度的序列（行） ​ \\3. 返回单个标量对象也可以使用，如 . transform(sum) apply函数： ​ \\1. 不同于transform只允许在Series上进行一次转换， apply对整个DataFrame 作用 ​ 2.apply隐式地将group 上所有的列作为自定义函数 栗子： #coding=gbk import numpy as np import pandas as pd data = pd.DataFrame({'state':['Florida','Florida','Texas','Texas'], 'a':[4,5,1,3], 'b':[6,10,3,11] }) print(data) # a b state # 0 4 6 Florida # 1 5 10 Florida # 2 1 3 Texas # 3 3 11 Texas def sub_two(X): return X['a'] - X['b'] data1 = data.groupby(data['state']).apply(sub_two) # 此处使用transform 则会出现错误 print(data1) # state # Florida 0 -2 # 1 -5 # Texas 2 -2 # 3 -8 # dtype: int64 返回单个标量可以使用transform： ：我们可以看到使用transform 和apply 的输出结果形式是不一样的，transform返回与数据同样长度的行，而apply则进行了聚合 此时，使用apply说明的信息更明确 np.random.seed(666) df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'], 'B' : ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'], 'C' : np.random.randn(8), 'D' : np.random.randn(8)}) print(df) # A B C D # 0 foo one 0.824188 0.640573 # 1 bar one 0.479966 -0.786443 # 2 foo two 1.173468 0.608870 # 3 bar three 0.909048 -0.931012 # 4 foo two -0.571721 0.978222 # 5 bar two -0.109497 -0.736918 # 6 foo one 0.019028 -0.298733 # 7 foo three -0.943761 -0.460587 def zscore(x): return (x - x.mean())/ x.var() print(df.groupby('A').transform(zscore)) #自动识别CD列 print(df.groupby('A')['C','D'].apply(zscore)) #此种形式则两种输出数据是一样的 # df.groupby('A').apply(zscore) 此种情况则会报错，apply对整个dataframe作用 df['sum_c'] = df.groupby('A')['C'].transform(sum) #先对A列进行分组， 计算C列的和 df = df.sort_values('A') print(df) # A B C D sum_c # 1 bar one 0.479966 -0.786443 1.279517 # 3 bar three 0.909048 -0.931012 1.279517 # 5 bar two -0.109497 -0.736918 1.279517 # 0 foo one 0.824188 0.640573 0.501202 # 2 foo two 1.173468 0.608870 0.501202 # 4 foo two -0.571721 0.978222 0.501202 # 6 foo one 0.019028 -0.298733 0.501202 # 7 foo three -0.943761 -0.460587 0.501202 print(df.groupby('A')['C'].apply(sum)) # A # bar 1.279517 # foo 0.501202 # Name: C, dtype: float64 参考 pandas中 transform 函数和 apply 函数的区别 Update time： 2020-08-16 "},"Chapter2/Pandas value_counts函数.html":{"url":"Chapter2/Pandas value_counts函数.html","title":"Pandas value_counts()函数","keywords":"","body":"Pandas value_counts()函数 Series.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True) 返回一个包含唯一值计数的系列。 pandas 的 value_counts()函数可以对Series里面的每个值进行计数并且排序。 Series 情况： In [1]: import pandas as pd^M ...: df = pd.DataFrame({'区域' : ['西安', '太原', '西安', '太原', '郑州', '太原'], ^M ...: '10月份销售' : ['0.477468', '0.195046', '0.015964', '0.259654', '0.856412', '0.259644'],^M ...: '9月份销售' : ['0.347705', '0.151220', '0.895599', '0236547', '0.569841', '0.254784']}) In [2]: df Out[2]: 区域 10月份销售 9月份销售 0 西安 0.477468 0.347705 1 太原 0.195046 0.151220 2 西安 0.015964 0.895599 3 太原 0.259654 0236547 4 郑州 0.856412 0.569841 5 太原 0.259644 0.254784 # 统计每个区域出现多少次： In [3]: df['区域'].value_counts() Out[3]: 太原 3 西安 2 郑州 1 Name: 区域, dtype: int64 # 每个区域都被计数，并且默认从高到低排序。 # 如果想升序排列，设置参数 ascending = True： In [4]: df['区域'].value_counts(ascending=True) Out[4]: 郑州 1 西安 2 太原 3 Name: 区域, dtype: int64 # 如果想得出计数占比，可以加参数 normalize=True： In [5]: df['区域'].value_counts(normalize=True) Out[5]: 太原 0.500000 西安 0.333333 郑州 0.166667 Name: 区域, dtype: float64 DataFrame 情况 In [6]: df = pd.DataFrame({'区域1' : ['西安', '太原', '西安', '太原', '郑州', '太原'],^M ...: '区域2' : ['太原', '太原', '西安', '西安', '西安', '太原']}) In [7]: df.apply(pd.value_counts) Out[7]: 区域1 区域2 太原 3 3.0 西安 2 3.0 郑州 1 NaN # 区域2中没有郑州，所以是NaN。 属性 index In [1]: import pandas as pd^M ...: df = pd.DataFrame({'区域' : ['西安', '太原', '西安', '太原', '郑州', '太原'], ^M ...: '10月份销售' : ['0.477468', '0.195046', '0.015964', '0.259654', '0.856412', '0.259644'],^M ...: '9月份销售' : ['0.347705', '0.151220', '0.895599', '0236547', '0.569841', '0.254784']}) In [2]: df Out[2]: 区域 10月份销售 9月份销售 0 西安 0.477468 0.347705 1 太原 0.195046 0.151220 2 西安 0.015964 0.895599 3 太原 0.259654 0236547 4 郑州 0.856412 0.569841 5 太原 0.259644 0.254784 In [3]: df['区域'].value_counts() Out[3]: 太原 3 西安 2 郑州 1 Name: 区域, dtype: int64 In [4]: df['区域'].value_counts().index Out[4]: Index(['太原', '西安', '郑州'], dtype='object') 属性 values In [5]: df['区域'].value_counts().values Out[5]: array([3, 2, 1], dtype=int64) 参考 pandas计数 value_counts() Update time： 2020-05-25 "},"Chapter2/Pandas sort_values函数.html":{"url":"Chapter2/Pandas sort_values函数.html","title":"Pandas sort_values()函数","keywords":"","body":"Pandas sort_values()函数 pandas中的sort_values()函数原理类似于SQL中的order by，可以将数据集依照某个字段中的数据进行排序，该函数即可根据指定列数据也可根据指定行的数据排序。 sort_values()函数的具体参数 DataFrame.sort_values(by=‘##’, axis=0, ascending=True, inplace=False, na_position=‘last’) 参数 by 指定列名(axis=0或’index’)或索引值(axis=1或’columns’) axis 若axis=0或’index’，则按照指定列中数据大小排序；若axis=1或’columns’，则按照指定索引中数据大小排序，默认axis=0 ascending 是否按指定列的数组升序排列，默认为True，即升序排列 inplace 是否用排序后的数据集替换原来的数据，默认为False，即不替换 na_position {‘first’,‘last’}，设定缺失值的显示位置 sort_values用法举例 创建数据框 #利用字典dict创建数据框 import numpy as np import pandas as pd df=pd.DataFrame({'col1':['A','A','B',np.nan,'D','C'], 'col2':[2,1,9,8,7,7], 'col3':[0,1,9,4,2,8] }) print(df) >>> col1 col2 col3 0 A 2 0 1 A 1 1 2 B 9 9 3 NaN 8 4 4 D 7 2 5 C 7 8 依据第一列排序，并将该列空值放在首位 #依据第一列排序，并将该列空值放在首位 print(df.sort_values(by=['col1'],na_position='first')) >>> col1 col2 col3 3 NaN 8 4 0 A 2 0 1 A 1 1 2 B 9 9 5 C 7 8 4 D 7 2 依据第二、三列，数值降序排序 #依据第二、三列，数值降序排序 print(df.sort_values(by=['col2','col3'],ascending=False)) >>> col1 col2 col3 2 B 9 9 3 NaN 8 4 5 C 7 8 4 D 7 2 0 A 2 0 1 A 1 1 根据第一列中数值排序，按降序排列，并替换原数据 #根据第一列中数值排序，按降序排列，并替换原数据 df.sort_values(by=['col1'],ascending=False,inplace=True, na_position='first') print(df) >>> col1 col2 col3 3 NaN 8 4 4 D 7 2 5 C 7 8 2 B 9 9 1 A 1 1 0 A 2 0 按照索引值为0的行，即第一行的值来降序排序 x = pd.DataFrame({'x1':[1,2,2,3],'x2':[4,3,2,1],'x3':[3,2,4,1]}) print(x) #按照索引值为0的行，即第一行的值来降序排序 print(x.sort_values(by =0,ascending=False,axis=1)) >>> x1 x2 x3 0 1 4 3 1 2 3 2 2 2 2 4 3 3 1 1 x2 x3 x1 0 4 3 1 1 3 2 2 2 2 4 2 3 1 1 3 sort_index（） Series 的 sort_index(ascending=True)方法可以对 index 进行排序操作，ascending 参数用于控制升序或降序，默认为升序。 和 sort_values() 的用法相同。 Update time： 2020-05-25 "},"Chapter2/聚类 Groupby.html":{"url":"Chapter2/聚类 Groupby.html","title":"聚类 Groupby","keywords":"","body":"GroupBy pandas提供了一个灵活高效的groupby功能，它使你能以一种自然的方式对数据集进行切片、切块、摘要等操作。根据一个或多个键（可以是函数、数组或DataFrame列名）拆分pandas对象。计算分组摘要统计，如计数、平均值、标准差，或用户自定义函数。对DataFrame的列应用各种各样的函数。应用组内转换或其他运算，如规格化、线性回归、排名或选取子集等。计算透视表或交叉表。执行分位数分析以及其他分组分析 groupby DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False, **kwargs) 数据分组 将一个dataframe对象分割成组 import pandas as pd df = pd.DataFrame({'A': ['girl', 'boy', 'girl', 'boy', 'girl', 'boy', 'girl', 'girl'], 'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'], 'C': [3,1,4,5,6,7,8,1], 'D': [9,1,2,3,1,6,4,3]}) A B C D 0 girl one 3 9 1 boy one 1 1 2 girl two 4 2 3 boy three 5 3 4 girl two 6 1 5 boy two 7 6 6 girl one 8 4 将数据进行分组： grouped = df.groupby('A') print(grouped) # grouped = df.groupby(['A','B']) print(grouped) # 示例 >>> import pandas as pd >>> df = pd.DataFrame({'key1':['a', 'a', 'b', 'b', 'a'], ... 'key2':['one', 'two', 'one', 'two', 'one'], ... 'data1':np.random.randn(5), ... 'data2':np.random.randn(5)}) >>> df data1 data2 key1 key2 0 -0.410673 0.519378 a one 1 -2.120793 0.199074 a two 2 0.642216 -0.143671 b one 3 0.975133 -0.592994 b two 4 -1.017495 -0.530459 a one 假设你想要按 key1 进行分组，并计算 data1 列的平均值，我们可以访问 data1 ，并根据 key1调用 groupby： >>> grouped = df['data1'].groupby(df['key1']) # grouped = df.groupby(df['key1'])['data1'] 同样的结果 >>> grouped 变量grouped是一个GroupBy对象，它实际上还没有进行任何计算，只是含有一些有关分组键 df['key1'] 的中间数据而已，然后我们可以调用GroupBy的mean方法来计算分组平均值： >>> grouped.mean() key1 a -1.182987 b 0.808674 dtype: float64 说明：数据（Series）根据分组键进行了聚合，产生了一个新的Series，其索引为 key1 列中的唯一值。之所以结果中索引的名称为key1，是因为原始DataFrame的列df['key1']就叫这个名字。 如果我们一次传入多个数组，就会得到不同的结果： >>> means = df['data1'].groupby([df['key1'], df['key2']]).mean() >>> means key1 key2 a one -0.714084 two -2.120793 b one 0.642216 two 0.975133 dtype: float64 通过两个键对数据进行了分组，得到的Series具有一个层次化索引（由唯一的键对组成） >>> means.unstack() key2 one two key1 a -0.714084 -2.120793 b 0.642216 0.975133 无论你准备拿groupby做什么，都有可能会用到GroupBy的size方法，它可以返回一个含有分组大小的Series： >>> df.groupby(['key1', 'key2']).size() key1 key2 a one 2 two 1 b one 1 two 1 dtype: int64 属性 groups GroupBy.groups 返回值类型：Dict {group name -> group labels} df.groupby('A').groups # {'boy': Int64Index([1, 3, 5], dtype='int64'), # 'girl': Int64Index([0, 2, 4, 6, 7], dtype='int64'#)} 属性 indices GroupBy.indices Dict {group name -> group indices} df.groupby('A').indices # {'boy': array([1, 3, 5], dtype=int64), # 'girl': array([0, 2, 4, 6, 7], dtype=int64)} 函数 get_group() 获取分组后的数据 get_group get_group('key name') grouped = df.groupby('A') print(grouped.get_group('boy')) # A B C D #1 boy one 1 1 #3 boy three 5 3 #5 boy two 7 6 print(grouped.get_group('girl')) # A B C D #0 girl one 3 9 #2 girl two 4 2 #4 girl two 6 1 #6 girl one 8 4 #7 girl three 1 3 迭代遍历分组 GroupBy对象支持迭代，可以产生一组二元元组（由分组名和数据块组成） import pandas as pd ipl_data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings', 'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'], 'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2], 'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017], 'Points':[876,789,863,673,741,812,756,788,694,701,804,690]} df = pd.DataFrame(ipl_data) grouped = df.groupby('Year') for name,group in grouped: print (name) print (group) 2014 Team Rank Year Points 0 Riders 1 2014 876 2 Devils 2 2014 863 4 Kings 3 2014 741 9 Royals 4 2014 701 2015 Team Rank Year Points 1 Riders 2 2015 789 3 Devils 3 2015 673 5 kings 4 2015 812 10 Royals 1 2015 804 2016 Team Rank Year Points 6 Kings 1 2016 756 8 Riders 2 2016 694 2017 Team Rank Year Points 7 Kings 1 2017 788 11 Riders 2 2017 690 选择一个分组 使用get_group()方法，可以选择一个组。 ipl_data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings', 'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'], 'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2], 'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017], 'Points':[876,789,863,673,741,812,756,788,694,701,804,690]} df = pd.DataFrame(ipl_data) grouped = df.groupby('Year') grouped.groups #{2014: Int64Index([0, 2, 4, 9], dtype='int64'), # 2015: Int64Index([1, 3, 5, 10], dtype='int64'), # 2016: Int64Index([6, 8], dtype='int64'), # 2017: Int64Index([7, 11], dtype='int64')} grouped.get_group(2014) # Team Rank Year Points #0 Riders 1 2014 876 #2 Devils 2 2014 863 #4 Kings 3 2014 741 #9 Royals 4 2014 701 Function application 聚合函数为每个组返回单个聚合值。当创建了分组(group by)对象，就可以对分组数据执行多个聚合操作。 GroupBy.apply() 对于apply()方法来说，它做了这么一个操作 将groupby分组好的数据，一组，一组，一组的传递到了函数里面。 df = pd.DataFrame({'A':['bob','sos','bob','sos','bob','sos','bob','bob'], 'B':['one','one','two','three','two','two','one','three'], 'C':[3,1,4,1,5,9,2,6], 'D':[1,2,3,4,5,6,7,8]}) A B C D 0 bob one 3 1 1 sos one 1 2 2 bob two 4 3 3 sos three 1 4 4 bob two 5 5 5 sos two 9 6 6 bob one 2 7 7 bob three 6 8 grouped = df.groupby('A') for name,group in grouped: print(name) print(group) bob A B C D 0 bob one 3 1 2 bob two 4 3 4 bob two 5 5 6 bob one 2 7 7 bob three 6 8 sos A B C D 1 sos one 1 2 3 sos three 1 4 5 sos two 9 6 然后对结果应用apply方法 d = grouped.apply(lambda x:x.describe()) print(d) # 输出结果 C D A bob count 5.000000 5.000000 mean 4.000000 4.800000 std 1.581139 2.863564 min 2.000000 1.000000 25% 3.000000 3.000000 50% 4.000000 5.000000 75% 5.000000 7.000000 max 6.000000 8.000000 sos count 3.000000 3.000000 mean 3.666667 4.000000 std 4.618802 2.000000 min 1.000000 2.000000 25% 1.000000 3.000000 50% 1.000000 4.000000 75% 5.000000 5.000000 max 9.000000 6.000000 看好是一组，一组的传递进去, 所以，呈现出一种多层级的结构 获取分组之后的前2条数据 import pandas as pd df = pd.DataFrame({'A':['bob','sos','bob','sos','bob','sos','bob','bob'], 'B':['one','one','two','three','two','two','one','three'], 'C':[3,1,4,1,5,9,2,6], 'D':[1,2,3,4,5,6,7,8]}) grouped = df.groupby('A') d = grouped.apply(lambda x:x.head(2)) print(d) # A B C D #A #bob 0 bob one 3 1 # 2 bob two 4 3 #sos 1 sos one 1 2 # 3 sos three 1 4 不用lambda，实现一下 def get_top(df): return df.head(2) d = grouped.apply(get_top) print(d) # A B C D #A #bob 0 bob one 3 1 # 2 bob two 4 3 #sos 1 sos one 1 2 # 3 sos three 1 4 将分组后的数据在进行聚合函数的时候，得到的结果可以用 index 和 value 属性进行访问聚合的结果，但结果不是 Dataframe 类型的数据，聚合结果的列没有列名， data[data['city'] == c].groupby( 'bin')['aver_price'].mean() 结果 bin (0, 100] 111.885075 (100, 200] 121.225610 (200, 300] 107.873333 (300, 400] 113.877059 (400, 500] 114.099038 (500, 600] 107.006912 (600, 700] 121.664796 (700, 800] 108.501117 (800, 900] 113.650955 (900, 1000] 94.152482 (1000, 1100] 102.882639 (1100, 1200] 96.952336 Name: aver_price, dtype: float64 GroupBy.agg() 相比于apply()函数 agg()每次只传入一列数据，并对其进行操作，也可以传入自定义的函数，和apply的用法类似。 GroupBy.agg(self, func, *args, **kwargs) # 参数 # fun:数据聚合的方法 df.groupby('A').agg(lambda x:print(x)) # 输出结果 0 one 2 two 4 two 6 one 7 three Name: B, dtype: object 1 one 3 three 5 two Name: B, dtype: object 0 3 2 4 4 5 6 2 7 6 Name: C, dtype: int64 1 1 3 1 5 9 Name: C, dtype: int64 0 1 2 3 4 5 6 7 7 8 Name: D, dtype: int64 1 2 3 4 5 6 Name: D, dtype: int64 常用的聚合方法 函数名 说明 count 分组中非 NA 值的数量 sum 非 NA 值的和 mean 非 NA 值的平均值 median 非 NA 值的算术中位数 std 、var 无偏（分母为 n-1）标准差和方差 min、max 非 NA 值的最小值和最大值 prod 非 NA 值的积 first 、last 第一个和最后一个非 NA 值 或者使用 numpy 中的聚合函数； ipl_data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings', 'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'], 'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2], 'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017], 'Points':[876,789,863,673,741,812,756,788,694,701,804,690]} df = pd.DataFrame(ipl_data) Team Rank Year Points 0 Riders 1 2014 876 1 Riders 2 2015 789 2 Devils 2 2014 863 3 Devils 3 2015 673 4 Kings 3 2014 741 5 kings 4 2015 812 6 Kings 1 2016 756 7 Kings 1 2017 788 8 Riders 2 2016 694 9 Royals 4 2014 701 10 Royals 1 2015 804 11 Riders 2 2017 690 grouped = df.groupby('Year') grouped['Points'].agg(np.mean) # Year # 2014 795.25 # 2015 769.50 # 2016 725.00 # 2017 739.00 # Name: Points, dtype: float64 size()函数 grouped = df.groupby('Team') grouped.agg(np.size) # 输出结果 Rank Year Points Team Devils 2 2 2 Kings 3 3 3 Riders 4 4 4 Royals 2 2 2 kings 1 1 1 应用多个聚合函数 grouped = df.groupby('Team') grouped['Points'].agg([np.sum, np.mean, np.std]) # 输出结果 sum mean std Team Devils 1536 768.000000 134.350288 Kings 2285 761.666667 24.006943 Riders 3049 762.250000 88.567771 Royals 1505 752.500000 72.831998 kings 812 812.000000 NaN 求多种聚合运算的同时更改列名 grouped['Points'].agg([('求和',np.sum), ('均值',np.mean), ('标准差',np.std)]) # 输出结果 求和 均值 标准差 Team Devils 1536 768.000000 134.350288 Kings 2285 761.666667 24.006943 Riders 3049 762.250000 88.567771 Royals 1505 752.500000 72.831998 kings 812 812.000000 NaN 不同的列运用不同的聚合函数 df = pd.DataFrame({'A':['bar', 'bar', 'foo', 'foo', 'foo', 'foo', 'foo'], 'B':['one', 'two', 'one', 'two', 'one', 'two', 'three'], 'C':[3,1,4,5,9,2,6], 'D':[1,1,1,1,2,2,3]}) # 输出结果 A B C D 0 bar one 3 1 1 bar two 1 1 2 foo one 4 1 3 foo two 5 1 4 foo one 9 2 5 foo two 2 2 6 foo three 6 3 grouped = df.groupby(['A','B']) grouped.agg({'C':['sum','mean'],'D':['min','max']}) # 输出结果 C D sum mean min max A B bar one 3 3.0 1 1 two 1 1.0 1 1 foo one 13 6.5 1 2 three 6 6.0 3 3 two 7 3.5 1 2 agg里面是可以使用自定义的聚合函数 grouped = df.groupby('A') def max_min(group): return group.max() - group.min() grouped.agg(max_min) # C D #A #bar 2 0 #foo 7 2 过滤 GroupBy.filter() 过滤根据定义的标准过滤数据并返回数据的子集。filter()函数用于过滤数据。 返回三次以上参加IPL的队伍。 df.groupby('Team').filter(lambda x: len(x) >= 3) # 输出结果 Team Rank Year Points 0 Riders 1 2014 876 1 Riders 2 2015 789 4 Kings 3 2014 741 6 Kings 1 2016 756 7 Kings 1 2017 788 8 Riders 2 2016 694 11 Riders 2 2017 690 案例 实例 1 将分组后的字符拼接 import pandas as pd df=pd.DataFrame({ 'user_id':[1,2,1,3,3], 'content_id':[1,1,2,2,2], 'tag':['cool','nice','clever','clever','not-bad'] }) df 将df按content_id分组，然后将每组的tag用逗号拼接 df.groupby('content_id')['tag'].apply(lambda x:','.join(x)).to_frame() 主要是因为 for group_name,group_values in df.groupby('content_id')['tag']: print(group_name) print(group_values.values,'\\n') 参考 梦想橡皮檫：学习pandas apply方法 梦想橡皮檫：agg() Pandas之groupby( )用法笔记 易百教程 python/pandas数据挖掘（十四）-groupby,聚合，分组级运算 Pandas GroupBy 使用教程 Update time： 2020-05-25 "},"Chapter2/DataFrame agg.html":{"url":"Chapter2/DataFrame agg.html","title":"DataFrame.agg","keywords":"","body":"DataFrame.agg DataFrame.agg(func, axis=0, *args, **kwargs) 使用指定axis上的一个或多个操作Aggregate(聚合)。 参数 func : function, str, list或 dict 函数，用于聚合数据。如果是函数， 则必须在传递DataFrame或传递到DataFrame.apply时工作。 接受的组合是: function string function name functions的list 和/或 function names, 例如， [np.sum, 'mean'] axis labels的dict -> functions, function names 或 这样的list. axis : {0 or ‘index’, 1 或 ‘columns’}, 默认 0 如果0或' index ':应用函数到每一列。 如果1或‘columns’:应用函数到每一行。 *args 要传递给func的位置参数。 **kwargs 要传递给func的关键字参数。 返回值： DataFrame,Series或scalar 如果使用单个函数调用DataFrame.agg，则返回Series， 如果使用多个函数调用DataFrame.agg， 如果使用单个函数调用Series.agg则返回DataFrame， 如果使用多个函数调用Series.agg则返回标量， 返回一个Series。 聚合操作总是在轴上执行，或者是index（默认）或列轴。 这种行为不同于 numpy聚合函数（mean，median，prod，sum，std，var），其中默认值是计算展平的聚合数组，例如numpy.mean（arr_2d） 而不是numpy.mean（arr_2d，轴= 0）。agg是aggregate的别名。 使用别名。 agg是聚合的别名。使用别名。 传递的用户定义函数将被传递一Series用于求值。 df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9], [np.nan, np.nan, np.nan]], columns=['A', 'B', 'C']) 在行上聚合这些函数 df.agg(['sum', 'min']) A B C sum 12.0 15.0 18.0 min 1.0 2.0 3.0 每个列有不同的聚合 df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']}) A B max NaN 8.0 min 1.0 2.0 sum 12.0 NaN 对列进行聚合 df.agg(\"mean\", axis=\"columns\") 0 2.0 1 5.0 2 8.0 3 NaN dtype: float64 参考 Python pandas.DataFrame.agg函数方法的使用 Update time： 2020-08-18 "},"Chapter2/Pandas Apply函数.html":{"url":"Chapter2/Pandas Apply函数.html","title":"Pandas Apply函数","keywords":"","body":"Pandas Apply函数 pandas 的 apply() 函数可以作用于 Series 或者整个 DataFrame，功能也是自动遍历整个 Series 或者 DataFrame, 对每一个元素运行指定的函数。 DataFrame.apply(func, axis=0, broadcast=False, raw=False, reduce=None, args=(), **kwds ) 该函数最有用的是第一个参数，这个参数是函数，相当于C/C++的函数指针。 func : function作用于每一列或行。 axis : {0 或 ‘index’, 1 或 ‘columns’}, 默认 0 函数所应用的轴: 0 或 ‘index’: 对每一列应用函数。 1 或 ‘columns’: 对每一行应用函数。 broadcast : bool, 可选 仅与聚合函数相关: False 或 None : 返回一个Series，该Series的长度是索引的长度或列的数量(基于axis参数) True : 结果将广播到框架的原始形状，原始索引和列将保留。 从0.23.0版本开始就不推荐使用:这个参数将在将来的版本中被删除，取而代之的是result_type= ' broadcast '。 raw : bool, 默认 False False : 将每一行或每一列作为一个Series传递给函数。 True : t传递的函数将接收ndarray对象。如果您只是应用一个NumPy约简函数，这将获得更好的性能。 reduce : bool或 None, 默认 None 试着使用减量程序。如果DataFrame为空，apply将使用reducto确定结果应该是一个Series还是一个DataFrame。如果reduce=None(缺省值)，apply的返回值将通过在空序列上调用func来猜测(注意:在猜测时，func引发的异常将被忽略)。如果reduce=True，则始终返回一个Series，如果reduce=False，则始终返回一个DataFrame。 从0.23.0版本开始就不推荐使用:这个参数将在将来的版本中被删除，取而代之的是result_type='reduce'。 result_type : {‘expand’, ‘reduce’, ‘broadcast’, None}, 默认 None 这些只在axis=1(列)时起作用: ‘expand’ : 类似列表的结果将转换为列。 ‘reduce’ : 如果可能，返回一个Series，而不是展开类似列表的结果。这是‘expand’的反义词。 ‘broadcast’ : 结果将广播到DataFrame的原始形状，保留原始索引和列。 默认行为(None)取决于应用函数的返回值:类似列表的结果将作为这些结果的Series返回。但是，如果apply函数返回一个Series，这些列就会展开为列。 args : tuple 除了array/series外，还要传递给func的位置参数。 \\kwds 要作为关键字参数传递给func的其他关键字参数。 这个函数需要自己实现，函数的传入参数根据axis来定，比如 axis = 1，就会把一行数据作为Series的数据结构传入给自己实现的函数中，我们在函数中实现对Series不同属性之间的计算，返回一个结果，则apply 函数会自动遍历每一行 DataFrame 的数据，最后将所有结果组合成一个Series数据结构并返回。 Series.apply() 举一个例子，现在有这样一组数据，学生的考试成绩： Name Nationality Score 张 汉 400 李 回 450 王 汉 460 如果民族不是汉族，则总分在考试分数上再加 5 分，现在需要用 pandas 来做这种计算，我们在 Dataframe 中增加一列。当然如果只是为了得到结果， numpy.where() 函数更简单，这里主要为了演示 Series.apply() 函数的用法。 import pandas as pd df = pd.read_csv(\"studuent-score.csv\") df['ExtraScore'] = df['Nationality'].apply(lambda x : 5 if x != '汉' else 0) df['TotalScore'] = df['Score'] + df['ExtraScore'] apply() 函数当然也可执行 python 内置的函数，比如我们想得到 Name 这一列字符的个数，如果用 apply() 的话： df['NameLength'] = df['Name'].apply(len) DataFrame.apply() DataFrame.apply() 函数则会遍历每一个元素，对元素运行指定的 function。比如下面的示例： import pandas as pd import numpy as np matrix = [ [1,2,3], [4,5,6], [7,8,9] ] df = pd.DataFrame(matrix, columns=list('xyz'), index=list('abc')) df.apply(np.square) 对 df 执行 square() 函数后，所有的元素都执行平方运算： x y z a 1 4 9 b 16 25 36 c 49 64 81 如果只想 apply() 作用于指定的行和列，可以用行或者列的 name 属性进行限定。比如下面的示例将 x 列进行平方运算： df.apply(lambda x : np.square(x) if x.name=='x' else x) x y z a 1 2 3 b 16 5 6 c 49 8 9 下面的示例对 x 和 y 列进行平方运算： df.apply(lambda x : np.square(x) if x.name in ['x', 'y'] else x) x y z a 1 4 3 b 16 25 6 c 49 64 9 下面的示例对第一行 （a 标签所在行）进行平方运算： df.apply(lambda x : np.square(x) if x.name == 'a' else x, axis=1) 默认情况下 axis=0 表示按列，axis=1 表示按行。 apply()计算日期相减示例 平时我们会经常用到日期的计算，比如要计算两个日期的间隔，比如下面的一组关于 wbs 起止日期的数据： wbs date_from date_to job1 2019-04-01 2019-05-01 job2 2019-04-07 2019-05-17 job3 2019-05-16 2019-05-31 job4 2019-05-20 2019-06-11 假定要计算起止日期间隔的天数。比较简单的方法就是两列相减（datetime 类型)： import pandas as pd import datetime as dt wbs = { \"wbs\": [\"job1\", \"job2\", \"job3\", \"job4\"], \"date_from\": [\"2019-04-01\", \"2019-04-07\", \"2019-05-16\",\"2019-05-20\"], \"date_to\": [\"2019-05-01\", \"2019-05-17\", \"2019-05-31\", \"2019-06-11\"] } df = pd.DataFrame(wbs) df['elpased'] = df['date_to'].apply(pd.to_datetime) - df['date_from'].apply(pd.to_datetime) apply() 函数将 date_from 和 date_to 两列转换成 datetime 类型。 print 一下 df: wbs date_from date_to elpased 0 job1 2019-04-01 2019-05-01 30 days 1 job2 2019-04-07 2019-05-17 40 days 2 job3 2019-05-16 2019-05-31 15 days 3 job4 2019-05-20 2019-06-11 22 days 日期间隔已经计算出来，但后面带有一个单位 days，这是因为两个 datetime 类型相减，得到的数据类型是 timedelta64，如果只要数字，还需要使用 timedelta 的 days 属性转换一下。 elapsed= df['date_to'].apply(pd.to_datetime) - df['date_from'].apply(pd.to_datetime) df['elpased'] = elapsed.apply(lambda x : x.days) 使用 DataFrame.apply() 函数也能达到同样的效果，我们需要先定义一个函数 get_interval_days() 函数的第一列是一个 Series 类型的变量，执行的时候，依次接收 DataFrame 的每一行。 import pandas as pd import datetime as dt def get_interval_days(arrLike, start, end): start_date = dt.datetime.strptime(arrLike[start], '%Y-%m-%d') end_date = dt.datetime.strptime(arrLike[end], '%Y-%m-%d') return (end_date - start_date).days wbs = { \"wbs\": [\"job1\", \"job2\", \"job3\", \"job4\"], \"date_from\": [\"2019-04-01\", \"2019-04-07\", \"2019-05-16\",\"2019-05-20\"], \"date_to\": [\"2019-05-01\", \"2019-05-17\", \"2019-05-31\", \"2019-06-11\"] } df = pd.DataFrame(wbs) df['elapsed'] = df.apply( get_interval_days, axis=1, args=('date_from', 'date_to')) 参考： pandas apply() 函数 pandas.DataFrame.apply函数方法的使用 Update time： 2020-05-25 "},"Chapter2/DataFrame where.html":{"url":"Chapter2/DataFrame where.html","title":"DataFrame where","keywords":"","body":"DataFrame where DataFrame.where(cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=False) 替换条件为假的值, 可以过滤不满足cond的值并赋予NaN空值 Parameters: cond : bool Series/DataFrame, array-like, or callable 如果cond为True，请保留原始值。如果为False，则用其他的相应值替换。如果cond是可调用的，则它是在Series / DataFrame上计算的，并且应返回布尔Series / DataFrame或数组。可调用对象不得更改输入Series / DataFrame（尽管pandas不会对其进行检查）。 other :scalar, Series/DataFrame, or callable Entries where cond is False are replaced with corresponding value from other. If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn’t check it). inplace :bool, default False 是否对数据执行适当的操作。 axis :int, default None level :int, default None Alignment level if needed. errors :str, {‘raise’, ‘ignore’}, default ‘raise’ Note that currently this parameter won’t affect the results and will always coerce to a suitable dtype. ‘raise’ : allow exceptions to be raised. ‘ignore’ : suppress exceptions. On error return original object. Examples 返回大于 0 的值，不符合的用 NaN填充 s = pd.Series(range(5)) s.where(s > 0) 0 NaN 1 1.0 2 2.0 3 3.0 4 4.0 dtype: float64 s.mask(s > 0) # 与 where 的结果相反 0 0.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 #返回大于 1 的值，不符合的用10填充 #cond = s > 1,other = 10 s.where(s > 1, 10) 0 10 1 10 2 2 3 3 4 4 dtype: int64 df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B']) df A B 0 0 1 1 2 3 2 4 5 3 6 7 4 8 9 m = df % 3 == 0 df.where(m, -df) A B 0 0 -1 1 -2 3 2 -4 -5 3 6 -7 4 -8 9 df.where(m, -df) == np.where(m, df, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True df.where(m, -df) == df.mask(~m, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True 去掉特定的某行某列： df.where(df !=N).dropna(axis = 1) DataFrame mask 与函数wheremask：where条件不符合进行替换，mask是条件符合进行替换。 DataFrame.mask(self, cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=False) s = pd.Series(range(5)) s.where(s > 0) 0 NaN 1 1.0 2 2.0 3 3.0 4 4.0 dtype: float64 #符合条件的替换 NaN s.mask(s > 0) 0 0.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B']) df A B 0 0 1 1 2 3 2 4 5 3 6 7 4 8 9 m = df % 3 == 0 df.where(m, -df) A B 0 0 -1 1 -2 3 2 -4 -5 3 6 -7 4 -8 9 df.where(m, -df) == np.where(m, df, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True df.where(m, -df) == df.mask(~m, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True 与np.where的异同？ np.where(condition, [x, y]),这里三个参数,其中必写参数是condition(判断条件),后边的x和y是可选参数.那么这三个参数都有怎样的要求呢? condition：array_like，bool ,当为True时，产生x，否则产生y 简单说,对第一个参数的要求是这样的,首先是数据类型的要求,类似于数组或者布尔值,当判断条件为真时返回x中的值,否则返回y中的值 x，y：array_like，可选,要从中选择的值。 x，y和condition需要可广播到某种形状 x和y是可选参数,并且对这两个参数的数据类型要求只有类似数组这一条,当条件判断为true或者false时从这两个类似数组的容器中取数. 参考 [pandas.DataFrame.where和mask 解读] Update time： 2020-08-18 "},"Chapter2/Pandas map, applymap and apply的区别.html":{"url":"Chapter2/Pandas map, applymap and apply的区别.html","title":"Pandas map, applymap and apply的区别","keywords":"","body":"Pandas map, applymap and apply的区别 apply() Pandas Apply函数 pandas 的 apply() 函数可以作用于 Series 或者整个 DataFrame，功能也是自动遍历整个 Series 或者 DataFrame, 对每一个元素运行指定的函数。 In [116]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon']) In [117]: frame Out[117]: b d e Utah -0.029638 1.081563 1.280300 Ohio 0.647747 0.831136 -1.549481 Texas 0.513416 -0.884417 0.195343 Oregon -0.485454 -0.477388 -0.309548 In [118]: f = lambda x: x.max() - x.min() In [119]: frame.apply(f) Out[119]: b 1.133201 d 1.965980 e 2.829781 dtype: float64 applymap() 如果想让方程作用于DataFrame中的每一个元素，可以使用applymap() In [120]: format = lambda x: '%.2f' % x In [121]: frame.applymap(format) Out[121]: b d e Utah -0.03 1.08 1.28 Ohio 0.65 0.83 -1.55 Texas 0.51 -0.88 0.20 Oregon -0.49 -0.48 -0.31 map() map()只要是作用将函数作用于一个Series的每一个元素，用法如下所示 In [122]: frame['e'].map(format) Out[122]: Utah 1.28 Ohio -1.55 Texas 0.20 Oregon -0.31 Name: e, dtype: object 总的来说就是apply()是一种让函数作用于列或者行操作，applymap()是一种让函数作用于DataFrame每一个元素的操作，而map是一种让函数作用于Series每一个元素的操作 map 也可以用字典进行映射 data = pd.read_csv(\"data.csv\") data['gender'].head() 结果： 0 1.0 1 0.0 2 2.0 3 1.0 4 2.0 Name: gender, dtype: float64 mp = { 0:'未知', 1:'男', 2:'女' } data['gender'] = data['gender'].map(mp) data['gender'].head() 结果： 0 男 1 未知 2 女 3 男 4 女 Name: gender, dtype: object key 或者 value 必须有一个是 int 型的值 company_size_map = { \"2000人以上\": 6, \"500-2000人\": 5, \"150-500人\": 4, \"50-150人\": 3, \"15-50人\": 2, \"少于15人\": 1 } workYear_map = { \"5-10年\": 5, \"3-5年\": 4, \"1-3年\": 3, \"1年以下\": 2, \"应届毕业生\": 1 } df[\"company_size\"] = df[\"companySize\"].map(company_size_map) df[\"work_year\"] = df[\"workYear\"].map(workYear_map) 参考 Pandas 中map, applymap and apply的区别 Update time： 2020-05-25 "},"Chapter2/Pandas pivot函数.html":{"url":"Chapter2/Pandas pivot函数.html","title":"Pandas pivot函数","keywords":"","body":"Pandas pivot函数 DataFrame.pivot(index=None, columns=None, values=None ) 返回按给定索引/列值组织的重新构造的DataFrame。 根据列值重塑数据(生成一个 \"pivot\" 表)。使用来自指定索引/列的惟一值来形成结果DataFrame的轴。此函数不支持数据聚合，多个值将导致列中的多索引。 index ：str或 object, 可选用于制作新frame索引的列。如果为None，则使用现有索引。 columns ：str或object位置参数传递给func。 values ：str, object 或 之前的列表, 可选于填充新frame值的列。如果未指定，将使用所有剩余的列，并且结果将具有按层次结构索引的列。 返回 DataFrame 返回调整后的DataFrame。 例子 import pandas as pd df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two', 'two'], 'bar': ['A', 'B', 'C', 'A', 'B', 'C'], 'baz': [1, 2, 3, 4, 5, 6], 'zoo': ['x', 'y', 'z', 'q', 'w', 't']}) df foo bar baz zoo 0 one A 1 x 1 one B 2 y 2 one C 3 z 3 two A 4 q 4 two B 5 w 5 two C 6 t df.pivot(index='foo', columns='bar', values='baz') # 或 df.pivot(index='foo', columns='bar')['baz'] 查看多列数据 df.pivot(index='foo', columns='bar', values=['baz', 'zoo']) 如果存在重复项，则会引发ValueError 参考 pandas.DataFrame.pivot函数 Update time： 2020-05-25 "},"Chapter2/Pandas pivot_table函数.html":{"url":"Chapter2/Pandas pivot_table函数.html","title":"Pandas pivot_table函数","keywords":"","body":"Pandas pivot_table函数 透视表是一种可以对数据动态排布并且分类汇总的表格格式。或许大多数人都在Excel使用过数据透视表（如下图），也体会到它的强大功能，而在pandas中它被称作pivot_table。 DataFrame.pivot_table(values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False ) pivot_table中的级别将存储在结果DataFrame的索引和列上的MultiIndex对象（分层索引）中。 参数 values ：要汇总的列，可选 index : column，Grouper，array或上一个list 如果传递数组，则其长度必须与数据长度相同。 该列表可以包含任何其他类型（列表除外）。 在pivot table索引上进行分组的键。 如果传递了数组，则其使用方式与列值相同。 columns : column，Grouper，array或上一个list 如果传递数组，则其长度必须与数据长度相同。 该列表可以包含任何其他类型（列表除外）。 在pivot table列上进行分组的键。如果传递了数组， 则其使用方式与列值相同。 aggfunc ：函数，函数列表，字典，默认numpy.mean 如果传递了函数列表， 则生成的pivot table将具有层次结构列， 其顶层是函数名称（从函数对象本身推论得出）。 如果传递了dict，则键为要汇总的列， 值是函数或函数列表。 fill_value ：scalar(标量)，默认为None 用于替换缺失值的值。 margins ：bool，默认为False 添加所有行/列（例如，小计/总计）。 dropna ：bool，默认为True 不要包括所有条目均为NaN的列。 margins_name ：str，默认为\"All\" 当margins为True时将包含总计的行/列的名称。 observed ：bool，默认为False 仅当任何 groupers是分类者时才适用。 如果为True：仅显示分类 groupers 的观测值。 如果为False：显示分类 groupers 的所有值。 返回值 DataFrame Excel样式的pivot table. df = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\", \"bar\"], \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\", \"one\", \"one\", \"two\", \"two\"], \"C\": [\"small\", \"large\", \"large\", \"small\", \"small\", \"large\", \"small\", \"small\", \"large\"], \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7], \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]}) A B C D E 0 foo one small 1 2 1 foo one large 2 4 2 foo one large 2 5 3 foo two small 3 5 4 foo two small 3 6 5 bar one large 4 6 6 bar one small 5 8 7 bar two small 6 9 8 bar two large 7 9 通过计算和来聚合值 table = pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'], aggfunc=np.sum) 使用fill_value参数来填充缺失的值 table = pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'], aggfunc=np.sum, fill_value=0) 通过跨多个列取平均值来汇总 table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'], aggfunc={'D': np.mean, 'E': np.mean}) 可以为任何给定值列计算多种类型的聚合 table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'], aggfunc={'D': np.mean, 'E': [min, max, np.mean]}) 案例 pivot_table 有四个最重要的参数index、values、columns、aggfunc， 以火箭队James_Harden本赛季比赛数据作为数据集进行讲解。 读取数据 import pandas as pd import numpy as np df = pd.read_csv('James_Harden.csv',encoding='utf8') df.head() 参数 Index 每个 pivot_table 必须拥有一个index，如果想查看哈登对阵每个队伍的得分，首先我们将对手设置为index pd.pivot_table(df,index=['对手']).head() 对手成为了第一层索引，还想看看对阵同一对手在不同主客场下的数据，试着将对手与胜负与主客场都设置为index pd.pivot_table(df,index=[u'对手',u'主客场']) 试着交换下它们的顺序，数据结果一样： pd.pivot_table(df,index=[u'主客场',u'对手']) Index就是层次字段，要通过透视表获取什么信息就按照相应的顺序设置字段，所以在进行pivot之前你也需要足够了解你的数据。 参数 Values 通过上面的操作，我们获取了james harden在对阵对手时的所有数据，而Values可以对需要的计算数据进行筛选，如果我们只需要james harden在主客场和不同胜负情况下的得分、篮板与助攻三项数据： pd.pivot_table(df,index=[u'主客场',u'胜负'],values=[u'得分',u'助攻',u'篮板']) ​ 参数 Columns Columns类似Index可以设置列层次字段，它不是一个必要参数，作为一种分割数据的可选方式。 #fill_value填充空值,margins=True进行汇总 pd.pivot_table(df,index=[u'主客场'],columns=[u'对手'],values=[u'得分'],aggfunc=[np.sum]) table=pd.pivot_table(df,index=[u'对手',u'胜负'],columns=[u'主客场'],values=[u'得分',u'助攻',u'篮板'],aggfunc=[np.mean],fill_value=0) query 当表格生成后如何查询某一项数据呢？ ex.根据上表查询哈登对阵灰熊时的数据 table.query('对手 == [\"灰熊\"]') Cheat Sheet Pandas | 一文看懂透视表pivot_table 数据 Update time： 2020-05-25 "},"Chapter2/Pandas crosstab交叉表.html":{"url":"Chapter2/Pandas crosstab交叉表.html","title":"Pandas crosstab交叉表","keywords":"","body":"Pandas crosstab交叉表 交叉表：crosstab 什么是交叉表 交叉表是一种常用的分类汇总表格，用于频数分布统计，主要价值在于描述了变量间关系的深刻含义。虽然两个（或以上）变量可以是分类的或数量的，但是以都是分类的情形最为常见。 交叉表(cross-tabulation, 简称crosstab)是一种用于计算分组频率的特殊透视表。 pandas.crosstab(index, columns, values=None, rownames=None, colnames=None, aggfunc=None, margins=False, margins_name: str = 'All', dropna: bool = True, normalize=False) → 'DataFrame' pandas.crosstab 例子中用到的数据 假设我们有两个变量，性别（男性或女性）和手性（右或左手）。 进一步假设，从非常大的人群中随机抽取100个人，作为对手性的性别差异研究的一部分。 可以创建一个交叉表来显示男性和右撇子，男性和左撇子，女性和右撇子以及女性和左撇子的个人数量。 这样的交叉表如下所示。 男性，女性以及右撇子和左撇子个体的数量称为边际总数。总计（即交叉表中所代表的个人总数）是右下角的数字。 数据来自http://Kaggle.com，是一个关于facebook的真实和虚假账号的数据集，包含了889个账户真实与否信息、好友数、教育情况、性别、等信息。数据集里有一个缺失值，进一步处理之前，先把缺失值去掉。 数据 import pandas as pd df = pd.read_csv('facebookac.csv') dr = df.dropna() df = df.replace(' ', 'not available') 用Pandas构建交叉表 基本的pandas方法 Pandas的crosstab()方法（官方文档在此）能够快速构建交叉表，并可以通过参数加以个性化的设置。其中，第一个参数将构成交叉表的行，第二个参数将构成交叉表的列。通过这个快捷的方法，我们能看到真实和虚假账户中不同情感关系状态的频数，非常清晰明了。 pd.crosstab(df['relationship'], df['Status']) pandas的DataFrame中的另外两个用于数据汇总转换的方法——groupby()、pivot_table()——也分别都可以实现这个效果，不过会麻烦一些。DataFrame.grouby()官方文档在此，DataFrame.pivot_table()官方文档在此。 df.groupby(['relationship', 'Status'])['relationship'].count().unstack() df.pivot_table(values='education', index='relationship', columns='Status', aggfunc=len) crosstab()重要参数 在交叉表中，我们常常需要统计边际总数（各行和各列的总和）。当然我们可以用sum()先算出各行、各列的和再用cancat()合并到交叉表中，不过这样实在太麻烦了。Crosstab()的参数margins可以帮我们“一键解决”，还可以通过margins_name设置总计行（列）的名称（默认名称是“All”）。 pd.crosstab(df['relationship'], df['Status'], margins=True, margins_name='Total') 在实际应用中，我们常常需要统计交叉表中各项的相对频率（即所占百分比）。一开始，总是繁琐地先用sum()算出总和，然后用div()来求出相对相对频率。后来发现可以在crosstab()中，直接通过参数 normalize设置，方便多了。 pd.crosstab(df['relationship'], df['Status'], normalize=True) 如果需要计算的是各项在所在行（列）的相对频数，normalize一样可以解决。normalize可以接受三种不同类型的参数，分别是{True, False}、{‘all’, ‘index’, ‘columns’}和 {0,1}。其中“index”或0表示按行统计（每行总和都为1），“columns”或“1”表示按列统计（每列总和都为1）。 pd.crosstab(df['relationship'], df['Status'], normalize=0) pd.crosstab(df['relationship'], df['Status'], normalize=1) 分层交叉表 crosstab()的参数index和columns可以接受列表传入，构建分层交叉表。比如，我们在交叉表中再加入性别（表格中的“gender”列）信息，并按列计算相对频率，看看在真实和虚假账户中，不同的性别和情感关系状态和是怎么分布的。这个交叉表中，每一列的总和为1。 dfv = pd.crosstab([df['gender'], df['relationship']], df['Status'], normalize=1) dfv 可以按指定的列排序 dfv[['real','fake']] 交叉表可视化 看到上面的这个交叉表，大家心里可能会默默算一下哪些类型占比高，哪些类型占比低。如果数据多了，就会很难有直观感受。好在，我们有热图。Seaborn库中的heatmap()能快速生成热图，官方文档在此。 import seaborn as sns sns.heatmap(dfv, cmap='YlOrRd', annot=True) cmap='YlOrRd'是把颜色设置为：数值从低到高，颜色依次是黄色、橙色、红色。annot=True是指在热图中保留数值。可以看到，返回的热图中，每一列的把两层分组的索引值标注了出来，非常清晰。在虚假账户中，最多的是“单身女性”，其次是“单身男性”，其他的组合极少甚至没有。而真实账户中，最多的是“单身男性”，并且在各种组合中都有分布。 交叉表数据量大的时候，即使热图也会让人看不过来。这时可以通过参数mask来筛选热图中显示的数据，不符合条件的就不会显示了。这样，我们就可以更集中地关注特定的数据。比如，我们只关注占比超过10%的类型，dfv sns.heatmap(dfv, cmap='YlOrRd', annot=True, mask=dfv others import pandas as pd df = pd.read_excel('survey.xls') df pd.crosstab(df.Nationality, df.Handedness) 输出: crosstab 第一个参数是列，第二个参数是行。还可以添加第三个参数: pd.crosstab(df.Sex, df.Handedness, margins = True) pd.crosstab([df.Nationality, df.Sex], df.Handedness, margins = True) 求百分比: pd.crosstab(df.Sex, df.Handedness, normalize='index') 求指定列的平均值: import numpy as np pd.crosstab(df.Sex, df.Handedness, values=df.Age, aggfunc=np.average) 参考 Pandas 基础 (13) - Crosstab 交叉列表取值 用Python统计推断——交叉表篇（上：crosstab与热图) https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html Update time： 2020-05-25 "},"Chapter2/rolling滚动计算函数.html":{"url":"Chapter2/rolling滚动计算函数.html","title":"rolling滚动计算函数","keywords":"","body":"rolling滚动计算函数 Rolling.count() 窗口内任何非NaN观测值的滚动计数 >>> s = pd.Series([2, 3, np.nan, 10]) >>> s.rolling(2).count() 0 1.0 1 2.0 2 1.0 3 1.0 dtype: float64 >>> s.rolling(3).count() 0 1.0 1 2.0 2 2.0 3 2.0 dtype: float64 >>> s.rolling(4).count() 0 1.0 1 2.0 2 2.0 3 3.0 dtype: float64 Rolling.sum 计算给定DataFrame或Series的滚动总和 >>> s = pd.Series([1, 2, 3, 4, 5]) >>> s 0 1 1 2 2 3 3 4 4 5 dtype: int64 >>> s.rolling(3).sum() 0 NaN 1 NaN 2 6.0 3 9.0 4 12.0 dtype: float64 >>> s.expanding(3).sum() 0 NaN 1 NaN 2 6.0 3 10.0 4 15.0 dtype: float64 >>> s.rolling(3, center=True).sum() 0 NaN 1 6.0 2 9.0 3 12.0 4 NaN dtype: float64 Rolling.median 计算滚动中位数。 Rolling.median(self, **kwargs) >>> s = pd.Series([0, 1, 2, 3, 4]) >>> s.rolling(3).median() 0 NaN 1 NaN 2 1.0 3 2.0 4 3.0 dtype: float64 Rolling.var 计算无偏滚动方差。 Rolling.var(self, ddof=1, *args, **kwargs) ddof : int, default 1 Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. *args, **kwargs For NumPy compatibility. No additional arguments are used. >>> s = pd.Series([5, 5, 6, 7, 5, 5, 5]) >>> s.rolling(3).var() 0 NaN 1 NaN 2 0.333333 3 1.000000 4 1.000000 5 1.333333 6 0.000000 dtype: float64 >>> s.expanding(3).var() 0 NaN 1 NaN 2 0.333333 3 0.916667 4 0.800000 5 0.700000 6 0.619048 dtype: float64 Rolling.std 计算滚动标准偏差。 Rolling.min 计算滚动最小值。 Rolling.max 计算滚动最大值。 Rolling.corr 计算滚动相关系数 Rolling.corr(self, other=None, pairwise=None, **kwargs) other : Series, DataFrame, or ndarray, optional If not supplied then will default to self. pairwise : bool, default None Calculate pairwise combinations of columns within a DataFrame. If other is not specified, defaults to True, otherwise defaults to False. Not relevant for Series. **kwargs Unused. >>> v1 = [3, 3, 3, 5, 8] >>> v2 = [3, 4, 4, 4, 8] >>> s1 = pd.Series(v1) >>> s2 = pd.Series(v2) >>> s1.rolling(4).corr(s2) 0 NaN 1 NaN 2 NaN 3 0.333333 4 0.916949 dtype: float64 使用pairwise选项对DataFrame进行类似的滚动计算 >>> matrix = np.array([[51., 35.], [49., 30.], [47., 32.], [46., 31.], [50., 36.]]) >>> df = pd.DataFrame(matrix, columns=['X','Y']) >>> df X Y 0 51.0 35.0 1 49.0 30.0 2 47.0 32.0 3 46.0 31.0 4 50.0 36.0 >>> df.rolling(4).corr(pairwise=True) X Y 0 X NaN NaN Y NaN NaN 1 X NaN NaN Y NaN NaN 2 X NaN NaN Y NaN NaN 3 X 1.000000 0.626300 Y 0.626300 1.000000 4 X 1.000000 0.555368 Y 0.555368 1.000000 # 索引为 3 和 4 的两个矩阵, 右下对角线为自身的相关系数，右上对角线为两者的相关系数 Rolling.cov 计算滚动样本协方差 Rolling.cov(self, other=None, pairwise=None, ddof=1, **kwargs) Update time： 2020-05-25 "},"Chapter2/cum累积计算函数.html":{"url":"Chapter2/cum累积计算函数.html","title":"cum累积计算函数","keywords":"","body":"cum累积计算函数 cum系列函数是作为DataFrame或Series对象的方法出现的，因此命令格式为D.cumsum() 方法名 函数功能 cumsum() 依次给出前1、2、… 、n个数的和 cumprod() 依次给出前1、2、… 、n个数的积 cummax() 依次给出前1、2、… 、n个数的最大值 cummin() 依次给出前1、2、… 、n个数的最小值 D=pd.Series(range(0,20)) D.cumsum() 0 0 1 1 2 3 3 6 .... 19 190 dtype: int64 Update time： 2020-05-25 "},"Chapter2/数据移动函数shift.html":{"url":"Chapter2/数据移动函数shift.html","title":"数据移动函数shift","keywords":"","body":"数据移动函数shift shift函数是对数据进行移动的操作 pandas.DataFrame.shift DataFrame.shift(periods=1, freq=None, axis=0, fill_value=None) 参数： period：表示移动的幅度，可以是正数，也可以是负数，默认值是1,1就表示移动一次，注意这里移动的都是数据，而索引是不移动的，移动之后没有对应值的，就赋值为NaN。 freq： DateOffset, timedelta, or time rule string，可选参数，默认值为None，只适用于时间序列，如果这个参数存在，那么会按照参数值移动时间索引，而数据值没有发生变化。 axis： 轴向。 {0 or ‘index’, 1 or ‘columns’, None}, default None 表示移动的方向，如果是0或者’index’表示上下移动，如果是1或者’columns’，则会左右移动。 fill_value : object, optional 数据移动后缺失值填补的方法 Examples 设定 period 与 axis df = pd.DataFrame(np.arange(16).reshape(4,4),columns=['AA','BB','CC','DD'],index =['a','b','c','d']) df Out[14]: AA BB CC DD a 0 1 2 3 b 4 5 6 7 c 8 9 10 11 d 12 13 14 15 #当period为正时，默认是axis = 0轴的设定，向下移动 df.shift(2) Out[15]: AA BB CC DD a NaN NaN NaN NaN b NaN NaN NaN NaN c 0.0 1.0 2.0 3.0 d 4.0 5.0 6.0 7.0 #当axis=1，沿水平方向进行移动，正数向右移，负数向左移 df.shift(2,axis = 1) Out[16]: AA BB CC DD a NaN NaN 0.0 1.0 b NaN NaN 4.0 5.0 c NaN NaN 8.0 9.0 d NaN NaN 12.0 13.0 #当period为负时，默认是axis = 0轴的设定，向上移动 df.shift(-1) Out[17]: AA BB CC DD a 4.0 5.0 6.0 7.0 b 8.0 9.0 10.0 11.0 c 12.0 13.0 14.0 15.0 d NaN NaN NaN NaN freq 参数实例 df = pd.DataFrame(np.arange(16).reshape(4,4) ,columns=['AA','BB','CC','DD'] ,index =pd.date_range('6/1/2012','6/4/2012')) df Out[38]: AA BB CC DD 2012-06-01 0 1 2 3 2012-06-02 4 5 6 7 2012-06-03 8 9 10 11 2012-06-04 12 13 14 15 df.shift(freq=datetime.timedelta(1)) Out[39]: AA BB CC DD 2012-06-02 0 1 2 3 2012-06-03 4 5 6 7 2012-06-04 8 9 10 11 2012-06-05 12 13 14 15 df.shift(freq=datetime.timedelta(-2)) Out[40]: AA BB CC DD 2012-05-30 0 1 2 3 2012-05-31 4 5 6 7 2012-06-01 8 9 10 11 2012-06-02 12 13 14 15 fill_value 参数 df = pd.DataFrame({\"Col1\": [10, 20, 15, 30, 45], \"Col2\": [13, 23, 18, 33, 48], \"Col3\": [17, 27, 22, 37, 52] ,index=pd.date_range(\"2020-01-01\", \"2020-01-05\")) df Col1 Col2 Col3 2020-01-01 10 13 17 2020-01-02 20 23 27 2020-01-03 15 18 22 2020-01-04 30 33 37 2020-01-05 45 48 52 df.shift(periods=3, fill_value=0) Col1 Col2 Col3 2020-01-01 0 0 0 2020-01-02 0 0 0 2020-01-03 0 0 0 2020-01-04 10 13 17 2020-01-05 20 23 27 periods 与 freq 同时使用： df = pd.DataFrame({\"Col1\": [10, 20, 15, 30, 45], \"Col2\": [13, 23, 18, 33, 48], \"Col3\": [17, 27, 22, 37, 52] ,index=pd.date_range(\"2020-01-01\", \"2020-01-05\")) df Col1 Col2 Col3 2020-01-01 10 13 17 2020-01-02 20 23 27 2020-01-03 15 18 22 2020-01-04 30 33 37 2020-01-05 45 48 52 df.shift(periods=3, freq=\"D\") Col1 Col2 Col3 2020-01-04 10 13 17 2020-01-05 20 23 27 2020-01-06 15 18 22 2020-01-07 30 33 37 2020-01-08 45 48 52 Update time： 2020-08-06 "},"Chapter2/DataFrame add_add_prefix_suffix.html":{"url":"Chapter2/DataFrame add_add_prefix_suffix.html","title":"DataFrame.add_add_prefix/suffix","keywords":"","body":"DataFrame.add_add_prefix/suffix DataFrame.add_prefix(prefix) 为标签添加前缀 For Series, the row labels are prefixed. For DataFrame, the column labels are prefixed 参数： prefix ：str 在每个标签前添加的字符串。 返回值： Series or DataFrame 更新标签后的的新 Series 或 DataFrame。 Examples s = pd.Series([1, 2, 3, 4]) s 0 1 1 2 2 3 3 4 dtype: int64 s.add_prefix('item_') item_0 1 item_1 2 item_2 3 item_3 4 dtype: int64 df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]}) df A B 0 1 3 1 2 4 2 3 5 3 4 6 df.add_prefix('col_') col_A col_B 0 1 3 1 2 4 2 3 5 3 4 6 添加后缀同样的用法。 Update time： 2020-08-18 "},"Chapter2/DataFrame select_dtypes.html":{"url":"Chapter2/DataFrame select_dtypes.html","title":"DataFrame.select_dtypes","keywords":"","body":"DataFrame.select_dtypes 根据列的 dtypes返回DataFrame的列的子集。 DataFrame.select_dtypes(include=None, exclude=None) 参数： include, exclude ：标量或数据类型构成的数组 include 要选取的，exclude 要排除的 返回值： DataFrame 包括include中的dtypes和exclude中的dtypes的DataFrame子集。 Notes To select all numeric types, use np.number or 'number' To select strings you must use the object dtype, but note that this will return all object dtype columns To select datetimes, use np.datetime64, 'datetime' or 'datetime64' To select Pandas categorical dtypes, use 'category' To select timedeltas, use np.timedelta64, 'timedelta' or 'timedelta64' See the numpy dtype hierarchy Examples df = pd.DataFrame({'a': [1, 2] * 3, 'b': [True, False] * 3, 'c': [1.0, 2.0] * 3}) df a b c 0 1 True 1.0 1 2 False 2.0 2 1 True 1.0 3 2 False 2.0 4 1 True 1.0 5 2 False 2.0 df.select_dtypes(include='bool') b 0 True 1 False 2 True 3 False 4 True 5 False df.select_dtypes(include=['float64']) c 0 1.0 1 2.0 2 1.0 3 2.0 4 1.0 5 2.0 df.select_dtypes(exclude=['int64']) b c 0 True 1.0 1 False 2.0 2 True 1.0 3 False 2.0 4 True 1.0 5 False 2.0 假设你仅仅需要选取数值型的列，那么你可以使用 select_dtypes()函数： 这包含了int和float型的列。 你还可以选取多种数据类型，只需要传递一个列表即可： 你还可以用来排除特定的数据类型： Update time： 2020-08-18 "},"Chapter2/DataFrame style.html":{"url":"Chapter2/DataFrame style.html","title":"DataFrame.style","keywords":"","body":"DataFrame.style pandas.io.formats.style.Styler class pandas.io.formats.style.Styler(data, precision=None, table_styles=None, uuid=None, caption=None, table_attributes=None, cell_ids=True, na_rep=None) 参数： data ：Series or DataFrame 要格式化的数据 precision ：int Precision to round floats to, defaults to pd.options.display.precision. table_styles： list-like, default None caption ：str, default None 标题 Caption to attach to the table. table_attributes ： str, default None 表格属性 。。。 Methods apply(func[, axis, subset]) ： 按列、按行或按表应用函数。 applymap(func[, subset]) ：按元素顺序应用函数。 background_gradient([cmap, low, high, axis, …]) ：用渐变样式给背景上色 bar([subset, axis, color, width, align, …]) ：在单元格背景中绘制条形图。 format(formatter[, subset, na_rep]) ： 格式化单元格的文本显示值。 hide_columns(subset) ：隐藏列从呈现。 hide_index()： Hide any indices from rendering. highlight_max([subset, color, axis]) ：通过阴影背景来突出最大值。 highlight_min([subset, color, axis])：通过阴影背景突出最小值。 highlight_null([null_color, subset])：为缺少的值阴影背景空色。 set_caption(caption)：设置标题 set_na_rep(na_rep) ：在样式器上设置丢失的数据表示形式。 set_precision(precision) ：设置精度(精度) set_properties([subset]) ：Method to set one or more non-data dependent properties or each cell. set_table_styles(table_styles) ： Set the table styles on a Styler. to_excel(excel_writer[, sheet_name, na_rep, …]) ：Write Styler to an Excel sheet. use(styles) ： Set the styles on the current Styler. where(cond, value[, other, subset]) ： Apply a function elementwise. 案例 一： 我们可以创建一个格式化字符串的字典，用于对每一列进行格式化。然后将其传递给DataFrame的 style.format()函数： 注意到，Date列是month-day-year的格式，Close列包含一个$符号，Volume列包含逗号。 我们可以通过链式调用函数来应用更多的格式化： 我们现在隐藏了索引，将Close列中的最小值高亮成红色，将Close列中的最大值高亮成浅绿色。 这里有另一个DataFrame格式化的例子： Volume列现在有一个渐变的背景色，你可以轻松地识别出大的和小的数值。 最后一个例子： 现在，Volumn列上有一个条形图，DataFrame上有一个标题。 二： 表格样式创建 表格视觉样式：Dataframe.style → 返回pandas.Styler对象的属性，具有格式化和显示Dataframe的有用方法 样式创建： ① Styler.applymap：elementwise → 按元素方式处理Dataframe ② Styler.apply：column- / row- / table-wise → 按行/列处理Dataframe import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline # 样式 df = pd.DataFrame(np.random.randn(10,4),columns=['a','b','c','d']) sty = df.style print(sty,type(sty)) # 查看样式类型 sty # 显示样式 # 按元素处理样式：style.applymap() def color_neg_red(val): if val # 按行/列处理样式：style.apply() def highlight_max(s): is_max = s == s.max() #print(is_max) # 布尔型索引 lst = [] for v in is_max: if v: lst.append('background-color: yellow') else: lst.append('') return(lst) df.style.apply(highlight_max, axis = 0, subset = ['b','c']) # 创建样式方法，每列最大值填充黄色 # axis：0为列，1为行，默认为0 # subset：索引 # 样式索引、切片 df.style.apply(highlight_max, axis = 1, subset = pd.IndexSlice[2:5,['b', 'd']]) # 通过pd.IndexSlice[]调用切片 # 也可：df[2:5].style.apply(highlight_max, subset = ['b', 'd']) → 先索引行再做样式 参考 表格样式创建 Update time： 2020-08-18 "},"Chapter2/DataFrame divide.html":{"url":"Chapter2/DataFrame divide.html","title":"DataFrame divide","keywords":"","body":"DataFrame divide DataFrame.divide(other, axis='columns', level=None, fill_value=None) Get Floating division of dataframe Equivalent to dataframe / other, but with support to substitute a fill_value for missing data in one of the inputs. Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, , /, //, %, *. Parameters : other : scalar, sequence, Series, or DataFrame axis : {0 or ‘index’, 1 or ‘columns’} Whether to compare by the index (0 or ‘index’) or columns (1 or ‘columns’). For Series input, axis to match Series index on. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. 返回值 ： DataFrame final result = final.divide(final['当月新增'],axis = 0).iloc[:,1:] result['当月新增'] = final['当月新增'] result final.divide([10, 100, 1000, 10000, 100000], axis=0).iloc[:, 1:] Update time： 2020-09-05 "},"Chapter3/":{"url":"Chapter3/","title":"Scipy","keywords":"","body":"Scipy Update time： 2020-05-25 "},"Chapter3/统计函数库/统计函数库scipy.stats.html":{"url":"Chapter3/统计函数库/统计函数库scipy.stats.html","title":"统计函数库scipy.stats","keywords":"","body":"统计函数库scipy.stats 常见分布 可能用到的分布对照表 名称 含义 f F分布 gamma gam分布 poisson 泊松分布 binom 二项分布 uniform 均匀分布 chi2 卡方分布 t 学生T分布 norm 正态分布 expon 指数分布 通用函数 名称 备注 rvs 产生服从指定分布的随机数 pdf 概率密度函数 cdf 累计分布函数 sf 残存函数（1-CDF） ppf 分位点函数（CDF的逆） isf 逆残存函数（sf的逆） fit 对一组随机取样进行拟合，最大似然估计方法找出最适合取样数据的概率密度函数系数。 离散分布的简单方法大多数与连续分布很类似，但是 pdf 被更换为密度函数pmf。 CDF : 累积分布函数 (cumulative distribution function)，是概率密度函数的积分，能完整描述一个实随机变量X的概率分布。 CDF是PDF的（从负无穷-oo到当前值的）积分，PDF是CDF的导数． CDF相当于其左侧的面积，也相当于小于该值的概率，负无穷的CDF值为０，正无穷的CDF值总为１ Update time： 2020-06-10 "},"Chapter3/统计函数库/norm 函数.html":{"url":"Chapter3/统计函数库/norm 函数.html","title":"norm 函数","keywords":"","body":"norm 函数 scipy.stats.norm 函数 正态分布 scipy.stats.norm函数可以实现正态分布（也就是高斯分布） norm(loc=,scale=) # loc: mean 均值， scale: standard deviation 标准差 生成服从指定分布的随机数 norm.rvs() 通过 loc和scale 参数可以指定随机变量的偏移和缩放参数，这里对应的是正态分布的期望和标准差。size 得到随机数数组的形状参数。(也可以使用np.random.normal(loc=0.0, scale=1.0, size=None)) 求概率密度函数指定点的函数值 stats.norm.pdf 正态分布概率密度函数。 标准形式是： f(x)=exp(−x2/2)2π f(x)=\\frac{\\exp \\left(-x^{2} / 2\\right)}{\\sqrt{2 \\pi}} f(x)=​√​2π​​​​​exp(−x​2​​/2)​​ import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import norm import numpy as np sns.set_style('whitegrid') np.random.seed(1) X = np.linspace(-5,5,20) # 第一种调用方式 gauss = norm(loc=1,scale=2) # loc: mean 均值， scale: standard deviation 标准差 r_1 = gauss.pdf(X) # 第二种调用方式 r_2 = norm.pdf(X, loc=0, scale=2) for i in range(len(X)): plt.scatter(X[i], 0, s=50) for g, c ,l in zip([r_1, r_2], ['r', 'g'],['r_1: $\\mu:1,\\sigma:2$','r_2: $\\mu:0,\\sigma:2$']): # 'r': red, 'g':green plt.plot(X, g, c=c,label=l) plt.legend() plt.show() 标准正态分布表与常用值 Z-score 是非标准正态分布标准化后的 x即 ：z=x−μσz=\\frac{x-\\mu}{\\sigma}z=​σ​​x−μ​​ 表头的横向表示小数点后第二位，表头的纵向则为整数部分以及小数点后第一位；两者联合作为完整的 x，坐标轴的横轴 表中的值为图中红色区域的面积，也即 cdf，连续分布的累积概率函数，记为 Φ(x)\\Phi(x)Φ(x) cdf 的逆，记为Φ−1(x)\\Phi^{-1}(x)Φ​−1​​(x) ，如Φ−1(3/4)\\Phi^{-1}(3/4)Φ​−1​​(3/4) ，表示 x 取何值时，阴影部分的面积为 0.75，查表可知，x 介于 0.67 和 0.68 之间； from scipy.stats import norm norm.ppf(3/4) # 0.6744897501960817 cdf 与 ppf（分位函数） from scipy.stats import norm 覆盖的概率范围： norm.cdf(1) - norm.cdf(-1) # 0.6826894921370859 norm.cdf(2) - norm.cdf(-2) # 0.9544997361036416 norm.cdf(3) - norm.cdf(-3) # 0.9973002039367398 求累计分布函数指定点的函数值 stats.norm.cdf正态分布累计概率密度函数。 Φ(x)\\Phi(x)Φ(x) 为 累积概率密度函数，也即 cdf： norm.cdf(0) # 0.5 累计分布函数的逆函数 stats.norm.ppf正态分布的累计分布函数的逆函数，即下分位点。 Φ−1(x)\\Phi^{-1}(x)Φ​−1​​(x) 通过 ppf(x)进行计算： >> from scipy.stats import norm # Q3 分位点； >> norm.ppf(3/4) # 0.6744897501960817 # 即 Q3 分位点处的横坐标取值为 0.6744897501960817 # Q1 分位点 >> norm.ppf(1/4) -0.6744897501960817 例如 在某一假设检验中，计算得到的 z=3.125z=3.125z=3.125 # 当 α=0.01 时 ,通过查表的 z_{0.01}=2.33 ,此时的 z_{0.01} 可以通过 `norm.ppf` 计算的到 from scipy.stats import norm norm.ppf(1-0.01) # 2.3263478740408408 参考 标准正态分布表（scipy.stats） scipy.stats.norm函数 python统计函数库scipy.stats的用法解析 Update time： 2020-06-10 "},"Chapter3/线性代数linalg/线性代数linalg.html":{"url":"Chapter3/线性代数linalg/线性代数linalg.html","title":"线性代数linalg","keywords":"","body":"线性代数linalg 主要参考 lijin-thu: 线性代数 huaxz1986: Scipy Update time： 2020-07-05 "},"Chapter3/线性代数linalg/求解线性方程组linalg.solve.html":{"url":"Chapter3/线性代数linalg/求解线性方程组linalg.solve.html","title":"求解线性方程组linalg.solve","keywords":"","body":"求解线性方程组linalg.solve numpy中的求解线性方程组：numpy.linalg.solve(a, b)。而scipy中的求解线性方程组： scipy.linalg.solve(a, b, sym_pos=False, lower=False, overwrite_a=False, overwrite_b=False, debug=False, check_finite=True) a：方阵，形状为 (M,M) b：一维向量，形状为(M,)。它求解的是线性方程组 。如果有 个线性方程组要求解，且 a，相同，则 b的形状为 (M,k) sym_pos：一个布尔值，指定a是否正定的对称矩阵 lower：一个布尔值。如果sym_pos=True时：如果为lower=True，则使用a的下三角矩阵。默认使用a的上三角矩阵。 overwrite_a：一个布尔值，指定是否将结果写到a的存储区。 overwrite_b：一个布尔值，指定是否将结果写到b的存储区。 check_finite：如果为True，则检测输入中是否有nan或者inf 返回线性方程组的解。 通常求解矩阵 A−1BA^{-1}BA​−1​​B，如果使用solve(A,B)，要比先求逆矩阵、再矩阵相乘来的快。 普通使用 求解线性方程组： [1234912718]x=[111] \\left[\\begin{array}{ccc} 1 & 2 & 3 \\\\ 4 & 9 & 1 \\\\ 27 & 1 & 8 \\end{array}\\right] \\mathbf{x}=\\left[\\begin{array}{c} 1 \\\\ 1 \\\\ 1 \\end{array}\\right] ​⎣​⎡​​​1​4​27​​​2​9​1​​​3​1​8​​​⎦​⎤​​x=​⎣​⎡​​​1​1​1​​​⎦​⎤​​ import numpy as np from scipy.linalg import solve a = np.array([[1,2,3], [4,9,1], [27,1,8]]) b = np.array([1,1,1]) x = solve(a,b) x 结果： array([-0.05030488, 0.10213415, 0.2820122 ]) Update time： 2020-07-05 "},"Chapter3/线性代数linalg/最小二乘解lstsq.html":{"url":"Chapter3/线性代数linalg/最小二乘解lstsq.html","title":"最小二乘解lstsq","keywords":"","body":"最小二乘解lstsq lstsq比solve更一般化，它不要求矩阵 AAA 是方阵。 它找到一组解 XXX，使得 ∥b−Ax∥\\|\\mathbf{b}-\\mathbf{A} \\mathbf{x}\\|∥b−Ax∥ 最小，我们称得到的结果为最小二乘解。 scipy.linalg.lstsq(a, b, cond=None, overwrite_a=False, overwrite_b=False, check_finite=True, lapack_driver=None) a：为矩阵，形状为(M,N) b：一维向量，形状为(M,)。它求解的是线性方程组 Ax=b\\mathbf{A} \\mathbf{x}=\\mathbf{b}Ax=b 。如果有 kkk 个线性方程组要求解，且 a，相同，则 b的形状为 (M,k) cond：一个浮点数，去掉最小的一些特征值。当特征值小于cond * largest_singular_value时，该特征值认为是零 overwrite_a：一个布尔值，指定是否将结果写到a的存储区。 overwrite_b：一个布尔值，指定是否将结果写到b的存储区。 check_finite：如果为True，则检测输入中是否有nan或者inf lapack_driver：一个字符串，指定求解算法。可以为：'gelsd'/'gelsy'/'gelss'。默认的'gelsd'效果就很好，但是在许多问题上'gelsy'效果更好。 返回值： x：最小二乘解，形状和b相同 residures：残差。如果 rank(A)rank(A)rank(A) 大于N或者小于M，或者使用了gelsy，则是个空数组；如果b是一维的，则它的形状是(1,)；如果b是二维的，则形状为(K,) rank：返回矩阵a的秩 s：a的奇异值。如果使用gelsy，则返回None 基本使用 求解线性方程组 [123449161276418]x=[111] \\left[\\begin{array}{cccc} 1 & 2 & 3 & 4 \\\\ 4 & 9 & 16 & 1 \\\\ 27 & 64 & 1 & 8 \\end{array}\\right] \\mathbf{x}=\\left[\\begin{array}{c} 1 \\\\ 1 \\\\ 1 \\end{array}\\right] ​⎣​⎡​​​1​4​27​​​2​9​64​​​3​16​1​​​4​1​8​​​⎦​⎤​​x=​⎣​⎡​​​1​1​1​​​⎦​⎤​​ import numpy as np from scipy.linalg import lstsq a = np.array([[1, 2, 3, 4], [4, 9, 16, 1], [27, 64, 1, 8]]) b = np.array([1, 1, 1]) lstsq(a, b) 结果： (array([ 0.00205847, -0.01287038, 0.0558478 , 0.21403472]), array([], dtype=float64), 3, array([70.75236556, 15.95524212, 3.67872487])) Update time： 2020-07-05 "},"Chapter3/线性代数linalg/矩阵的LU分解.html":{"url":"Chapter3/线性代数linalg/矩阵的LU分解.html","title":"矩阵的LU分解","keywords":"","body":"矩阵的LU分解 linalg.lu M∗NM*NM∗N 矩阵 AAA 的 LU 分解为： A=PLU A =PLU A=PLU P 是 M×M 的单位矩阵的一个排列，L 是下三角阵，U 是上三角阵。 可以使用 linalg.lu 进行 LU 分解的求解： 具体原理可以查看维基百科： https://en.wikipedia.org/wiki/LU_decomposition A = np.array([[1,2,3],[4,5,6]]) P, L, U = linalg.lu(A) 结果： # p array([[0., 1.], [1., 0.]]) # L array([[1. , 0. ], [0.25, 1. ]]) # U array([[4. , 5. , 6. ], [0. , 0.75, 1.5 ]]) scipy.linalg.lu_factor scipy.linalg.lu_factor(a, overwrite_a=False, check_finite=True) a：方阵，形状为(M,M)，要求非奇异矩阵 overwrite_a：一个布尔值，指定是否将结果写到a的存储区。 check_finite：如果为True，则检测输入中是否有nan或者inf 返回: lu：一个数组，形状为(N,N)，该矩阵的上三角矩阵就是U，下三角矩阵就是L（L矩阵的对角线元素并未存储，因为它们全部是1） piv：一个数组，形状为(N,)。它给出了P矩阵：矩阵a的第 i行被交换到了第piv[i]行 矩阵LU分解： A=PLU A=PLU A=PLU 其中： PPP为转置矩阵，该矩阵任意一行只有一个1，其他全零；任意一列只有一个1，其他全零。LLL 为单位下三角矩阵（对角线元素为1）， UUU为上三角矩阵（对角线元素为0） 基本使用 矩阵 LULULU 分解 [1234912718] \\left[\\begin{array}{ccc} 1 & 2 & 3 \\\\ 4 & 9 & 1 \\\\ 27 & 1 & 8 \\end{array}\\right] ​⎣​⎡​​​1​4​27​​​2​9​1​​​3​1​8​​​⎦​⎤​​ import numpy as np from scipy.linalg import lu_factor a = np.array([[1,2,3],[4,9,1],[27,1,8]]) lu,piv=lu_factor(a) 结果： # lu array([[27. , 1. , 8. ], [ 0.14814815, 8.85185185, -0.18518519], [ 0.03703704, 0.22175732, 2.74476987]]) LLL 为单位下三角矩阵（对角线元素为1）， UUU为上三角矩阵 也可使使用上面的linalg.lu 求出 L,UL,UL,U Update time： 2020-07-05 "},"Chapter3/线性代数linalg/矩阵的奇异值分解svd.html":{"url":"Chapter3/线性代数linalg/矩阵的奇异值分解svd.html","title":"矩阵的奇异值分解svd","keywords":"","body":"矩阵的奇异值分解svd 矩阵的奇异值分解： 设矩阵 AAA 为 阶的矩阵，则存在一个分解，使得：A=UΣVT\\mathbf{A}=\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{T}A=UΣV​T​​ ，其中UUU 为 M∗MM*MM∗M 阶酉矩阵； Σ\\SigmaΣ为半正定的 阶的对焦矩阵； 而VVV 为N∗NN*NN∗N 阶酉矩阵。 Σ\\SigmaΣ 对角线上的元素为 AAA 的奇异值，通常按照从大到小排列。 scipy.linalg.svd(a, full_matrices=True, compute_uv=True, overwrite_a=False, check_finite=True, lapack_driver='gesdd') a：一个矩阵，形状为(M,N)，待分解的矩阵。 full_matrices：如果为True，则 UUU 的形状为(M,M)、 VT\\mathbf{V}^{T}V​T​​ 的形状为(N,N)；否则 U\\mathbf{U}U 的形状为(M,K)、 VT\\mathbf{V}^{T}V​T​​ 的形状为(K,N)，其中 K=min(M,N) compute_uv：如果True，则结果中额外返回U以及Vh；否则只返回奇异值 overwrite_a：一个布尔值，指定是否将结果写到a的存储区。 overwrite_b：一个布尔值，指定是否将结果写到b的存储区。 check_finite：如果为True，则检测输入中是否有nan或者inf lapack_driver：一个字符串，指定求解算法。可以为：'gesdd'/'gesvd'。默认的'gesdd'。 返回值： U： UUU矩阵 s：奇异值，它是一个一维数组，按照降序排列。长度为 K=min(M,N) Vh：就是 VT\\mathbf{V}^{T}V​T​​ 矩阵 基本使用 奇异值分解; [011110] \\left[\\begin{array}{cccc} 0 & 1 \\\\ 1 & 1 \\\\ 1 & 0 \\end{array}\\right] ​⎣​⎡​​​0​1​1​​​1​1​0​​​⎦​⎤​​ from scipy.linalg import svd a = np.array([ [0,1], [1,1], [1,0] ]) u,s,vt = svd(a) 结果： # u array([[-0.40824829, 0.70710678, 0.57735027], [-0.81649658, 0. , -0.57735027], [-0.40824829, -0.70710678, 0.57735027]]) # s array([1.73205081, 1. ]) # vt array([[-0.70710678, -0.70710678], [-0.70710678, 0.70710678]]) 参考 奇异值分解(SVD)原理与在降维中的应用 奇异值分解的揭秘（一）：矩阵的奇异值分解过程 Update time： 2020-07-05 "},"Chapter3/线性代数linalg/求解特征值和特征向量.html":{"url":"Chapter3/线性代数linalg/求解特征值和特征向量.html","title":"求解特征值和特征向量","keywords":"","body":"求解特征值和特征向量 scipy.linalg.eig(a, b=None, left=False, right=True, overwrite_a=False, overwrite_b=False, check_finite=True) a：一个方阵，形状为(M,M)。待求解特征值和特征向量的矩阵。 b：默认为None，表示求解标准的特征值问题： Ax=λx\\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}Ax=λx。 也可以是一个形状与a相同的方阵，此时表示广义特征值问题： Ax=λBx\\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{B} \\mathbf{x}Ax=λBx left：一个布尔值。如果为True，则计算左特征向量 right：一个布尔值。如果为True，则计算右特征向量 overwrite_a：一个布尔值，指定是否将结果写到a的存储区。 overwrite_b：一个布尔值，指定是否将结果写到b的存储区。 check_finite：如果为True，则检测输入中是否有nan或者inf 返回值： w：一个一维数组，代表了M特特征值。 vl：一个数组，形状为(M,M)，表示正则化的左特征向量（每个特征向量占据一列，而不是一行）。仅当left=True时返回 vr：一个数组，形状为(M,M)，表示正则化的右特征向量（每个特征向量占据一列，而不是一行）。仅当right=True时返回 numpy提供了numpy.linalg.eig(a)来计算特征值和特征向量 有特征值：Axr=λxr\\mathbf{A} \\mathbf{x}_{r}=\\lambda \\mathbf{x}_{r}Ax​r​​=λx​r​​ ; 左特征值： 为特征值的共轭。 令 P=[xr1,xr2,⋯,xrM]\\mathbf{P}=\\left[\\mathbf{x}_{r 1}, \\mathbf{x}_{r 2}, \\cdots, \\mathbf{x}_{r M}\\right]P=[x​r1​​,x​r2​​,⋯,x​rM​​] Σ=[λ100⋯00λ20⋯0⋮⋮⋮⋱⋮000⋯λM] \\mathbf{\\Sigma}=\\left[\\begin{array}{ccccc} \\lambda_{1} & 0 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_{2} & 0 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\cdots & \\lambda_{M} \\end{array}\\right] Σ=​⎣​⎢​⎢​⎡​​​λ​1​​​0​⋮​0​​​0​λ​2​​​⋮​0​​​0​0​⋮​0​​​⋯​⋯​⋱​⋯​​​0​0​⋮​λ​M​​​​​⎦​⎥​⎥​⎤​​ 则有： AP=PΣ⟹A=PΣP−1 \\mathbf{A P}=\\mathbf{P \\Sigma} \\Longrightarrow \\mathbf{A}=\\mathbf{P} \\mathbf{\\Sigma P}^{-1} AP=PΣ⟹A=PΣP​−1​​ 求解问题 linalg.eig(A) 返回矩阵的特征值与特征向量 linalg.eigvals(A) 返回矩阵的特征值 linalg.eig(A, B) 求解 Av=λBv\\mathbf{A} \\mathbf{v}=\\lambda \\mathbf{B} \\mathbf{v}Av=λBv 的问题 基本使用 例子 矩阵为： A=[152241362] \\mathbf{A}=\\left[\\begin{array}{lll} 1 & 5 & 2 \\\\ 2 & 4 & 1 \\\\ 3 & 6 & 2 \\end{array}\\right] A=​⎣​⎡​​​1​2​3​​​5​4​6​​​2​1​2​​​⎦​⎤​​ 特征多项式为： ∣A−λI∣=(1−λ)[(4−λ)(2−λ)−6]−5[2(2−λ)−3]+2[12−3(4−λ)]=−λ3+7λ2+8λ−3 \\begin{aligned} |\\mathbf{A}-\\lambda \\mathbf{I}|=&(1-\\lambda)[(4-\\lambda)(2-\\lambda)-6]-\\\\ & 5[2(2-\\lambda)-3]+2[12-3(4-\\lambda)] \\\\ =&-\\lambda^{3}+7 \\lambda^{2}+8 \\lambda-3 \\end{aligned} ​∣A−λI∣=​​=​​​(1−λ)[(4−λ)(2−λ)−6]−​5[2(2−λ)−3]+2[12−3(4−λ)]​−λ​3​​+7λ​2​​+8λ−3​​ 特征根为： λ1=7.9579λ2=−1.2577λ3=0.2997 \\begin{array}{l} \\lambda_{1}=7.9579 \\\\ \\lambda_{2}=-1.2577 \\\\ \\lambda_{3}=0.2997 \\end{array} ​λ​1​​=7.9579​λ​2​​=−1.2577​λ​3​​=0.2997​​ 求解特征值和特征向量 [152241362] \\left[\\begin{array}{lll} 1 & 5 & 2 \\\\ 2 & 4 & 1 \\\\ 3 & 6 & 2 \\end{array}\\right] ​⎣​⎡​​​1​2​3​​​5​4​6​​​2​1​2​​​⎦​⎤​​ import numpy as np from scipy.linalg import eig a = np.array([[1,5,2],[2,4,1],[3,6,2]]) w,vr=eig(a,right=True) print(w) print(vr) 结果： # 每一个为一个特征值 [ 7.9579162 +0.j -1.25766471+0.j 0.2997485 +0.j] # 每一列对应一个特征值的特征向量 [[-0.5297175 -0.90730751 0.28380519] [-0.44941741 0.28662547 -0.39012063] [-0.71932146 0.30763439 0.87593408]] Update time： 2020-07-05 "},"Chapter3/拟合与优化optimize/优化算法optimize.html":{"url":"Chapter3/拟合与优化optimize/优化算法optimize.html","title":"拟合与优化optimize","keywords":"","body":"拟合与优化optimize scipy.optimize包提供了几种常用的优化算法。 该模块包含以下几个方面 使用各种算法(例如BFGS，Nelder-Mead单纯形，牛顿共轭梯度，COBYLA 或 SLSQP )的无约束和约束最小化多元标量函数(minimize()) 全局(蛮力)优化程序(例如，anneal()，basinhopping()) 最小二乘最小化(leastsq())和曲线拟合(curve_fit())算法 标量单变量函数最小化(minim_scalar())和根查找(newton()) 使用多种算法(例如，Powell，Levenberg-Marquardt混合或Newton-Krylov等大规模方法)的多元方程系统求解(root) Update time： 2020-07-05 "},"Chapter3/拟合与优化optimize/最小二乘法拟合leastsq.html":{"url":"Chapter3/拟合与优化optimize/最小二乘法拟合leastsq.html","title":"最小二乘法函数拟合leastsq","keywords":"","body":"最小二乘法函数拟合leastsq 最小二乘估计原理是这样的： y=f(x,θ)+ε y=f(x, \\theta)+\\varepsilon y=f(x,θ)+ε 其中 ε 独立同分布。 θ=argmin∑(yi−f(xi,θ))2 \\theta=\\arg \\min \\sum\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2} θ=argmin∑(y​i​​−f(x​i​​,θ))​2​​ 非线性最小二乘法中，SST=SSR+SSE不再成立，但仍然可以定义R_squared=1-SSE/SST leastsq可以用来做最小二乘估计，可以在线性拟合和非线性拟合中使用。 leastsq scipy.optimize.leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None) 参数： 一般我们只要指定前三个参数就可以了: func :是我们自己定义的一个计算误差的函数， x0 :是计算的初始参数值 args：tuple, optional 是指定func的其他参数，都放在这个元组中 func：是个可调用对象，给出每次拟合的残差。最开始的参数是待优化参数；后面的参数由args给出 x0：初始迭代值 args：一个元组，用于给func提供额外的参数。 Dfun：用于计算func的雅可比矩阵（按行排列）。默认情况下，算法自动推算。它给出残差的梯度。最开始的参数是待优化参数；后面的参数由args给出 full_output：如果非零，则给出更详细的输出 col_deriv：如果非零，则计算雅可比矩阵更快（按列求导） ftol：指定相对的均方误差的阈值 xtol：指定参数解收敛的阈值 gtol：Orthogonality desired between the function vector and the columns of the Jacobian maxfev：设定算法迭代的最大次数。如果为零：如果为提供了Dfun，则为 100*(N+1)，N为x0的长度；如果未提供Dfun,则为200*(N+1) epsfcn：采用前向差分算法求解雅可比矩阵时的步长。 factor：它决定了初始的步长 diag：它给出了每个变量的缩放因子 返回值： x：拟合解组成的数组 cov_x：Uses the fjac and ipvt optional outputs to construct an estimate of the jacobian around the solution infodict：给出了可选的输出。它是个字典，其中的键有： nfev：func调用次数 fvec：最终的func输出 fjac：A permutation of the R matrix of a QR factorization of the final approximate Jacobian matrix, stored column wise. ipvt：An integer array of length N which defines a permutation matrix, p, such that fjacp = qr, where r is upper triangular with diagonal elements of nonincreasing magnitude ier：一个整数标记。如果为 1/2/3/4，则表示拟合成功 mesg：一个字符串。如果解未找到，则它给出详细信息 基本使用 线性拟合 import numpy as np import matplotlib.pyplot as plt from scipy.optimize import leastsq # 样本数据(Xi,Yi)，需要转换成数组(列表)形式 Xi = np.array([160, 165, 158, 172, 159, 176, 160, 162, 171]) Yi = np.array([58, 63, 57, 65, 62, 66, 58, 59, 62]) 定义残差并寻优: # 需要拟合的函数func :指定函数的形状 k= 0.42116973935 b= -8.28830260655 def func(p, x): k, b = p return k*x+b # 偏差函数：x,y都是列表:这里的x,y更上面的Xi,Yi中是一一对应的 def error(p, x, y): return func(p, x)-y 给定初始值，进行拟合 # k,b的初始值，可以任意设定,经过几次试验，发现p0的值会影响cost的值：Para[1] p0 = [1, 20] # 把error函数中除了p0以外的参数打包到args中(使用要求) Para = leastsq(error, p0, args=(Xi, Yi)) 读取结果： # 读取结果 k, b = Para[0] print(\"k=\", k, \"b=\", b) 计算误差: 残差平方和 def S(k,b): error=np.zeros(k.shape) for x,y in zip(X,Y): error+=(y-(k*x+b))**2 return error S(k,b) # array(405808.61964307) 可视化： # 画样本点 plt.figure(figsize=(8, 6)) # 指定图像比例： 8：6 plt.scatter(Xi, Yi, color=\"green\", label=\"YB\", linewidth=2) # 画拟合直线 x = np.linspace(150, 190, 100) # 在150-190直接画100个连续点 y = k*x+b # 函数式 plt.plot(x, y, color=\"red\", label=\"NH\", linewidth=2) plt.legend() # 绘制图例 曲线拟合 定义目标函数，准备一一检验其拟合效果 # 目标函数 def test_func(x, p): a, b, c = p return a*x**2+b*x+c # 残差 def residuals(p,y,x): return y-test_func(x,p) 生成模拟数据 p_true = [0.4, -2, 0.9] # 真实值 X = np.linspace(0, 10, 100) y = test_func(X, p_true)+np.random.randn(len(X)) 拟合 from scipy.optimize import leastsq # 先验的估计，真实数据分析流程中，先预估一个接近的值。这里为了测试效果，先验设定为 1 p_prior = np.ones_like(p_true) plsq = leastsq(residuals, p_prior, args=(y, X)) print(p_true) print(plsq) [0.4, -2, 0.9] (array([ 0.40067724, -1.99507171, 0.86224339]), 3) 画图 import matplotlib.pyplot as plt plt.plot(X, y) plt.plot(X, test_func(X, plsq[0])) 参考 cipy中最小二乘法函数leastsq的用法及其实例应用 【最小二乘估计】scipy.optimize.leastsq Update time： 2020-07-05 "},"Chapter3/拟合与优化optimize/拟合optimize.curve_fit.html":{"url":"Chapter3/拟合与优化optimize/拟合optimize.curve_fit.html","title":"拟合curve_fit","keywords":"","body":"拟合curve_fit scipy.optimize 中有 curve_fit方法可以拟合自定义的曲线，如指数函数拟合，幂指函数拟合和多项式拟合 scipy.optimize.curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False, check_finite=True, bounds=- inf, inf, method=None, jac=None, **kwargs) 参数： f模型函数f（x，…）。它必须将自变量作为第一个参数，并将要拟合的参数作为单独的剩余参数。 xdata : array_like or object, 要拟合的自变量数组 ydata : array_like, 要拟合的因变量的值 p0：初始迭代值 sigma：y值的不确定性的度量 absolute_sigma： If False, sigma denotes relative weights of the data points. The returned covariance matrix pcov is based on estimated errors in the data, and is not affected by the overall magnitude of the values in sigma. Only the relative magnitudes of the sigma values matter.If True, sigma describes one standard deviation errors of the input data points. The estimated covariance in pcov is based on these values. check_finite：如果为True，则检测输入中是否有nan或者inf bounds：指定变量的取值范围 method：指定求解算法。可以为 'lm'/'trf'/'dogbox' kwargs：传递给 leastsq/least_squares的关键字参数。 返回值： popt：最优化参数 pcov：The estimated covariance of popt. 基本使用 用样本拟合函数 f(x)=ae−bx+c f(x)=a e^{-b x}+c f(x)=ae​−bx​​+c import numpy as np import matplotlib.pyplot as plt from scipy.optimize import curve_fit # 定义目标函数 def func(x, a, b, c): return a*np.exp(-b*x)+c # 这部分生成样本点，对函数值加上高斯噪声作为样本点 # [0, 4]共50个点 xdata = np.linspace(0, 4, 50) # # a=2.5, b=1.3, c=0.5 y = func(xdata, 2.5, 1.3, 0.5) np.random.seed(1) err_stdev = 0.2 # 生成均值为0，标准差为err_stdev为0.2的高斯噪声 y_noise = err_stdev*np.random.normal(size=len(xdata)) ydata = y + y_noise plt.scatter(xdata, ydata, label='data') # 利用curve_fit作简单的拟合，popt为拟合得到的参数,pcov是参数的协方差矩阵 popt_1, pcov = curve_fit(func, xdata, ydata) print(popt_1) #[2.52560138 1.44826091 0.53725085] plt.plot(xdata, func(xdata, *popt_1), 'r-', label='fit_1') # 限定参数范围：0 计算拟合结果的指标 总平方和 $SST$： 总平方和(SST) = 回归平方和(SSR)十残差平方和(SSE) SST=∑(yi−y¯)2 \\mathrm{SST}=\\sum\\left(y_{i}-\\bar{y}\\right)^{2} SST=∑(y​i​​−​y​¯​​)​2​​ 回归平方和 $SSR$ : SSR=∑(y^i−y¯)2 SSR=\\sum\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2} SSR=∑(​y​^​​​i​​−​y​¯​​)​2​​ 残差平方和 $SSE$ : SSE=∑(yi−y^i) SSE=\\sum\\left(y_{i}-\\hat{y}_{i}\\right) SSE=∑(y​i​​−​y​^​​​i​​) 判定系数： $MSE$ (均方差、方差): MSE=SSE/n=1n∑i=1n(yi−y^i)2 M S E=S S E / n=\\frac{1}{n} \\sum_{i=1}^{n} \\left(y_{i}-\\hat{y}_{i}\\right)^{2} MSE=SSE/n=​n​​1​​​i=1​∑​n​​(y​i​​−​y​^​​​i​​)​2​​ $RMSE$(均方根、标准差): RMSE=MSE=SSE/n=1n∑i=1n(yi−y^i)2 R M S E=\\sqrt{M S E}=\\sqrt{S S E / n}=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(y_{i}-\\hat{y}_{i}\\right)^{2}} RMSE=√​MSE​​​=√​SSE/n​​​=​⎷​​​​​​​n​​1​​​i=1​∑​n​​(y​i​​−​y​^​​​i​​)​2​​​​​ def get_indexes(y_predict, y_data): n = y_data.size #计算 残差平方方差 SSE SSE = ((y_data - y_predict)**2).sum() # 均方差 MSE MSE = SSE / n # 均方根 RMSE 越接近 0，拟合效果越好 RMSE = np.sqrt(MSE) # 求R方，0 比较上一节的两次拟合，哪个拟合得更好 # 比较上一节的两次拟合，哪个拟合得更好 y_predict_1=func(xdata, *popt_1) indexes_1=get_indexes(y_predict_1, ydata) print(indexes_1) # 模型2 y_predict_2=func(xdata, *popt_2) indexes_2=get_indexes(y_predict_2, ydata) print(indexes_2) 结果： (1.833619516810147, 0.03667239033620294, 0.19150036641271195, 0.9163487663138368) (2.306216123348346, 0.04612432246696692, 0.21476573857803047, 0.8947885195939563) 参考 利用scipy.optimize.curve_fit对函数进行拟合 scipy.optimize.curve_fit Update time： 2020-08-04 "},"Chapter3/拟合与优化optimize/求解非线性方程组fsolve.html":{"url":"Chapter3/拟合与优化optimize/求解非线性方程组fsolve.html","title":"求解非线性方程组fsolve","keywords":"","body":"求解非线性方程组fsolve 使用 scipy.optimize 模块的 fsolve函数进行数值求解线性及非线性方程，可以看到这一个问题实际上还是一个优化问题，也可以用之前拟合函数的 leastsq 求解。下面用这两个方法进行对比： scipy.optimize.fsolve scipy.optimize.fsolve(func, x0, args=(), fprime=None, full_output=0, col_deriv=0, xtol=1.49012e-08, maxfev=0, band=None, epsfcn=None, factor=100, diag=None) 参数： func：是个可调用对象，它代表了非线性方程组。给他传入方程组的各个参数，它返回各个方程残差（残差为零则表示找到了根） x0：预设的方程的根的初始值 args：一个元组，用于给func提供额外的参数。 fprime：用于计算func的雅可比矩阵（按行排列）。默认情况下，算法自动推算 full_output：如果为True，则给出更详细的输出 col_deriv：如果为True，则计算雅可比矩阵更快（按列求导） xtol：指定算法收敛的阈值。当误差小于该阈值时，算法停止迭代 maxfev：设定算法迭代的最大次数。如果为零，则为 100*(N+1)，N为x0的长度 band：If set to a two-sequence containing the number of sub- and super-diagonals within the band of the Jacobi matrix, the Jacobi matrix is considered banded (only for fprime=None) epsfcn：采用前向差分算法求解雅可比矩阵时的步长。 factor：它决定了初始的步长 diag：它给出了每个变量的缩放因子 返回值： x：方程组的根组成的数组 infodict：给出了可选的输出。它是个字典，其中的键有： nfev：func调用次数 njev：雅可比函数调用的次数 fvec：最终的func输出 fjac：the orthogonal matrix, q, produced by the QR factorization of the final approximate Jacobian matrix, stored column wise r：upper triangular matrix produced by QR factorization of the same matrix ier：一个整数标记。如果为 1，则表示根求解成功 mesg：一个字符串。如果根未找到，则它给出详细信息 如果要求解方程： {f1(u1,u2,u3)=0f2(u1,u2,u3)=0f3(u1,u2,u3)=0 \\left\\{\\begin{array}{l} f 1(u 1, u 2, u 3)=0 \\\\ f 2(u 1, u 2, u 3)=0 \\\\ f 3(u 1, u 2, u 3)=0 \\end{array}\\right. ​⎩​⎨​⎧​​​f1(u1,u2,u3)=0​f2(u1,u2,u3)=0​f3(u1,u2,u3)=0​​ 那么 func 这么定义： def func(x): u1, u2, u3 = x return [f1(u1,u2,u3),f2(u1,u2,u3),f3(u1,u2,u3)] 案例1： import scipy.optimize as opt import numpy as np def f(x): x0, x1, x2 = x return np.array([ 5*x1+3, 4*x0*x0-2*np.sin(x1*x2), x1*x2-1.5 ]) result = opt.fsolve(f, [1, 1, 1]) print(\"解：\",result) print(\"各向量的值：\",f(result)) 结果： 解： [-0.70622057 -0.6 -2.5 ] 各向量的值： [ 0.00000000e+00 -9.12603326e-14 5.32907052e-15] 同样可以用最小二乘法 leastsq 求解上述问题 #拟合函数来求解 h = opt.leastsq(f,[1, 1, 1]) print(\"解：\",h[0]) print(\"各向量的值：\",f(h[0])) 结果： 解： [-0.70622057 -0.6 -2.5 ] 各向量的值： [ 0.00000000e+00 -2.22044605e-16 0.00000000e+00] 案例2： 如果给了Jacobian矩阵，那么迭代速度更快 Jacobian矩阵的定义是： [∂f1∂u1∂f1∂u2∂f1∂u2∂f2∂u1∂f2∂u2∂f2∂u2∂f3∂u1∂f3∂u2∂f3∂u2] \\left[\\begin{array}{ccc} \\frac{\\partial f 1}{\\partial u 1} & \\frac{\\partial f 1}{\\partial u 2} & \\frac{\\partial f 1}{\\partial u 2} \\\\ \\frac{\\partial f 2}{\\partial u 1} & \\frac{\\partial f 2}{\\partial u 2} & \\frac{\\partial f 2}{\\partial u 2} \\\\ \\frac{\\partial f 3}{\\partial u 1} & \\frac{\\partial f 3}{\\partial u 2} & \\frac{\\partial f 3}{\\partial u 2} \\end{array}\\right] ​⎣​⎢​⎢​⎢​⎢​⎡​​​​∂u1​​∂f1​​​​∂u1​​∂f2​​​​∂u1​​∂f3​​​​​​∂u2​​∂f1​​​​∂u2​​∂f2​​​​∂u2​​∂f3​​​​​​∂u2​​∂f1​​​​∂u2​​∂f2​​​​∂u2​​∂f3​​​​​⎦​⎥​⎥​⎥​⎥​⎤​​ import scipy.optimize as opt import numpy as np def obj_func(x): x0,x1,x2=x return [5*x1+3,4*x0*x0-2*np.sin(x1*x2),x1*x2-1.5] def jacobian(x): x0, x1, x2 = x return [ [0,5,0], [8*x0,-2*x2*np.cos(x1*x2),-2*x1*np.cos(x1*x2)], [0,x2,x1] ] result=opt.fsolve(obj_func,[1,1,1],fprime=jacobian) print(\"解：\",result) print(\"各向量的值：\",obj_func(result)) 结果： 解： [-0.70622057 -0.6 -2.5 ] 各向量的值： [0.0, -9.126033262418787e-14, 5.329070518200751e-15] 参考 【解方程】scipy.optimize.solve. python用fsolve、leastsq对非线性方程组进行求解 Update time： 2020-07-05 "},"Chapter3/拟合与优化optimize/函数最小值optimize.minimize.html":{"url":"Chapter3/拟合与优化optimize/函数最小值optimize.minimize.html","title":"函数最小值optimize.minimize","keywords":"","body":"函数最小值optimize.minimize 因为在生活中，人们总是希望幸福值或其它达到一个极值，比如做生意时希望成本最小，收入最大，所以在很多商业情境中，都会遇到求极值的情况。 minimize 求解函数的极小值（无约束） scipy.optimize.minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None) fun：可调用对象，待优化的函数。最开始的参数是待优化的自变量；后面的参数由args给出 x0：自变量的初始迭代值 args：一个元组，提供给fun的额外的参数 method：一个字符串，指定了最优化算法。可以为：'Nelder-Mead'、'Powell'、'CG'、 'BFGS'、'Newton-CG'、'L-BFGS-B'、'TNC'、'COBYLA'、'SLSQP'、 'dogleg'、'trust-ncg' jac：一个可调用对象（最开始的参数是待优化的自变量；后面的参数由args给出），雅可比矩阵。只在CG/BFGS/Newton-CG/L-BFGS-B/TNC/SLSQP/dogleg/trust-ncg算法中需要。如果jac是个布尔值且为True，则会假设fun会返回梯度；如果是个布尔值且为False，则雅可比矩阵会被自动推断（根据数值插值）。 hess/hessp：可调用对象（最开始的参数是待优化的自变量；后面的参数由args给出），海森矩阵。只有Newton-CG/dogleg/trust-ncg算法中需要。二者只需要给出一个就可以，如果给出了hess，则忽略hessp。如果二者都未提供，则海森矩阵自动推断 bounds：一个元组的序列，给定了每个自变量的取值范围。如果某个方向不限，则指定为None。每个范围都是一个(min,max)元组。 constrants：一个字典或者字典的序列，给出了约束条件。只在COBYLA/SLSQP中使用。字典的键为： type：给出了约束类型。如'eq'代表相等；'ineq'代表不等 fun：给出了约束函数 jac：给出了约束函数的雅可比矩阵（只用于SLSQP） args：一个序列，给出了传递给fun和jac的额外的参数 tol：指定收敛阈值 options：一个字典，指定额外的条件。键为： maxiter：一个整数，指定最大迭代次数 disp：一个布尔值。如果为True，则打印收敛信息 callback：一个可调用对象，用于在每次迭代之后调用。调用参数为x_k，其中x_k为当前的参数向量 返回值：返回一个OptimizeResult对象。其重要属性为： x：最优解向量 success：一个布尔值，表示是否优化成功 message：描述了迭代终止的原因 因为 Scipy.optimize.minimize 提供的是最小化方法，所以最大化距离就相当于最小化距离的负数：在函数的前面添加一个负号 基本使用 假设我们要求解最小值的函数为： f(x,y)=(1−x)2+100(y−x2)2 f(x, y)=(1-x)^{2}+100\\left(y-x^{2}\\right)^{2} f(x,y)=(1−x)​2​​+100(y−x​2​​)​2​​ 则雅可比矩阵为： [∂f(x,y)∂x,∂f(x,y)∂y] \\left[\\frac{\\partial f(x, y)}{\\partial x}, \\frac{\\partial f(x, y)}{\\partial y}\\right] [​∂x​​∂f(x,y)​​,​∂y​​∂f(x,y)​​] 则海森矩阵为： [∂2f(x,y)∂x2∂2f(x,y)∂x∂y∂2f(x,y)∂y∂x∂2f(x,y)∂y2] \\left[\\begin{array}{cc} \\frac{\\partial^{2} f(x, y)}{\\partial x^{2}} & \\frac{\\partial^{2} f(x, y)}{\\partial x \\partial y} \\\\ \\frac{\\partial^{2} f(x, y)}{\\partial y \\partial x} & \\frac{\\partial^{2} f(x, y)}{\\partial y^{2}} \\end{array}\\right] ​⎣​⎢​⎢​⎡​​​​∂x​2​​​​∂​2​​f(x,y)​​​​∂y∂x​​∂​2​​f(x,y)​​​​​​∂x∂y​​∂​2​​f(x,y)​​​​∂y​2​​​​∂​2​​f(x,y)​​​​​⎦​⎥​⎥​⎤​​ 于是有： def fun(p): x,y=p.tolist()#p 为数组，形状为 (2,) return f(x,y) def jac(p): x,y=p.tolist()#p 为数组，形状为 (2,) return np.array([df/dx,df/dy]) def hess(p): x,y=p.tolist()#p 为数组，形状为 (2,) return np.array([[ddf/dxx,ddf/dxdy],[ddf/dydx,ddf/dyy]]) 案例 0 计算 1/x+x1/x+x1/x+x 的最小值 from scipy.optimize import minimize import numpy as np #demo 1 #计算 1/x+x 的最小值 def fun(x): return 1/x + x x0 = np.asarray((2)) # 初始猜测值 res = minimize(fun, x0, method='SLSQP') print(res) # 结果： fun: 2.0000000815356342 jac: array([0.00057095]) message: 'Optimization terminated successfully.' nfev: 19 nit: 6 njev: 6 status: 0 success: True x: array([1.00028559]) 案例 1 计算函数： f(x,y)=(1−x)2+100(y−x2)2 f(x, y)=(1-x)^{2}+100\\left(y-x^{2}\\right)^{2} f(x,y)=(1−x)​2​​+100(y−x​2​​)​2​​ 的最小值 雅可比矩阵为: [400x(x2−y)+2(x−1)200(y−x2)] \\left[400 x\\left(x^{2}-y\\right)+2(x-1) \\quad 200\\left(y-x^{2}\\right)\\right] [400x(x​2​​−y)+2(x−1)200(y−x​2​​)] 海森矩阵为: [400(3x2−y)+2−400x−400x200] \\left[\\begin{array}{cc} 400\\left(3 x^{2}-y\\right)+2 & -400 x \\\\ -400 x & 200 \\end{array}\\right] [​400(3x​2​​−y)+2​−400x​​​−400x​200​​] def func(p): x, y = p return (1-x)**2 + 100*(y-x**2)**2 def jac(p): x, y = p return np.array([2*(x-1)+400*x*(x**2-y), 200*(y-x**2)]) def hess(p): x, y = p return np.array([ [400*(3*x**2-y)+2, -400*x], [-400*x, 200] ]) result = opt.minimize(func, x0=[10, 10], method='Newton-CG', jac=jac, hess=hess) result 结果： fun: 1.5507998929041444e-18 jac: array([ 4.09654832e-07, -2.05662731e-07]) message: 'Optimization terminated successfully.' nfev: 83 nhev: 51 nit: 51 njev: 133 status: 0 success: True x: array([1., 1.]) 案例 2 计算 (2+x1)/(1+x2)−3∗x1+4∗x3(2+x1)/(1+x2) - 3*x1+4*x3(2+x1)/(1+x2)−3∗x1+4∗x3 的最小值 x1,x2,x3x1,x2,x3x1,x2,x3 的范围都在0.1到0.9 之间 from scipy.optimize import minimize import numpy as np # demo 2 #计算 (2+x1)/(1+x2) - 3*x1+4*x3 的最小值 x1,x2,x3的范围都在0.1到0.9 之间 def fun(args): a,b,c,d=args v=lambda x: (a+x[0])/(b+x[1]) -c*x[0]+d*x[2] return v def con(args): # 约束条件 分为eq 和ineq #eq表示 函数结果等于0 ； ineq 表示 表达式大于等于0 x1min, x1max, x2min, x2max,x3min,x3max = args cons = ({'type': 'ineq', 'fun': lambda x: x[0] - x1min},\\ {'type': 'ineq', 'fun': lambda x: -x[0] + x1max},\\ {'type': 'ineq', 'fun': lambda x: x[1] - x2min},\\ {'type': 'ineq', 'fun': lambda x: -x[1] + x2max},\\ {'type': 'ineq', 'fun': lambda x: x[2] - x3min},\\ {'type': 'ineq', 'fun': lambda x: -x[2] + x3max}) return cons #定义常量值 args = (2,1,3,4) #a,b,c,d #设置参数范围/约束条件 args1 = (0.1,0.9,0.1, 0.9,0.1,0.9) #x1min, x1max, x2min, x2max,x3min, x3max cons = con(args1) #设置初始猜测值 x0 = np.asarray((0.5,0.5,0.5)) res = minimize(fun(args), x0, method='SLSQP',constraints=cons) 结果： fun: -0.773684210526435 jac: array([-2.47368421, -0.80332409, 4. ]) message: 'Optimization terminated successfully.' nfev: 10 nit: 2 njev: 2 status: 0 success: True x: array([0.9, 0.9, 0.1]) 参考 huaxiaozhuan: Scipy python 非线性规划（scipy.optimize.minimize） 最小化函数:minimize 函数 幻大米：最优化 Update time： 2020-07-05 "},"Chapter3/拟合与优化optimize/全局最优点basinhopping.html":{"url":"Chapter3/拟合与优化optimize/全局最优点basinhopping.html","title":"全局最优点basinhopping","keywords":"","body":"全局最优点basinhopping 常规的最优化算法很容易陷入局部极值点。basinhopping算法是一个寻找全局最优点的算法。 scipy.optimize.basinhopping(func, x0, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None) func：可调用函数。为待优化的目标函数。最开始的参数是待优化的自变量；后面的参数由minimizer_kwargs字典给出 x0：一个向量，设定迭代的初始值 niter：一个整数，指定迭代次数 T：一个浮点数，设定了“温度”参数。 stepsize：一个浮点数，指定了步长 minimizer_kwargs：一个字典，给出了传递给scipy.optimize.minimize的额外的关键字参数。 take_step：一个可调用对象，给出了游走策略 accept_step：一个可调用对象，用于判断是否接受这一步 callback：一个可调用对象，每当有一个极值点找到时，被调用 interval：一个整数，指定stepsize被更新的频率 disp：一个布尔值，如果为True，则打印状态信息 niter_success：一个整数。Stop the run if the global minimum candidate remains the same for this number of iterations. 返回值：一个OptimizeResult对象。其重要属性为： x：最优解向量 success：一个布尔值，表示是否优化成功 message：描述了迭代终止的原因 基本使用 假设我们要求解最小值的函数为： f(x,y)=(1−x)2+100(y−x2)2 f(x, y)=(1-x)^{2}+100\\left(y-x^{2}\\right)^{2} f(x,y)=(1−x)​2​​+100(y−x​2​​)​2​​ 于是有： def fun(p): x,y=p.tolist()#p 为数组，形状为 (2,) return f(x,y) 案例 1 求解函数: f(x,y)=(1−x)2+100(y−x2)2 f(x, y)=(1-x)^{2}+100\\left(y-x^{2}\\right)^{2} f(x,y)=(1−x)​2​​+100(y−x​2​​)​2​​ 的最小值。 import numpy as np from scipy import optimize as opt def func(p): x, y = p return (1-x)**2 + 100*(y-x**2)**2 result = opt.basinhopping(func,x0=np.array([10,10])) result 结果： fun: 6.098906415928738e-13 lowest_optimization_result: fun: 6.098906415928738e-13 hess_inv: array([[0.50162481, 1.0026807 ], [1.0026807 , 2.00930225]]) jac: array([ 2.91012054e-05, -1.05691264e-05]) message: 'Desired error not necessarily achieved due to precision loss.' nfev: 444 nit: 16 njev: 108 status: 2 success: False x: array([0.9999995 , 0.99999895]) message: ['requested number of basinhopping iterations completed successfully'] minimization_failures: 30 nfev: 17153 nit: 100 njev: 4199 x: array([0.9999995 , 0.99999895]) 参考 中文 Python 笔记 huaxiaozhuan: Scipy Update time： 2020-07-05 "},"Chapter3/插值interpolate/插值interpolate.html":{"url":"Chapter3/插值interpolate/插值interpolate.html","title":"插值interpolate","keywords":"","body":"插值interpolate Update time： 2020-07-05 "},"Chapter3/插值interpolate/一维插值interp1d.html":{"url":"Chapter3/插值interpolate/一维插值interp1d.html","title":"一维插值interp1d","keywords":"","body":"一维插值interp1d interp1d类进行一维插值 一维数据的插值运算可以通过函数interp1d()完成。 interp1d(x, y, kind='linear', ...) 其中，x和y参数是一系列已知的数据点，kind参数是插值类型，可以是字符串或整数，它给出插值的B样条曲线的阶数，候选值及作用下表所示： 候选值 作用 ‘zero’ 、'nearest' 阶梯插值，相当于0阶B样条曲线 ‘slinear’ 、'linear' 线性插值，用一条直线连接所有的取样点，相当于一阶B样条曲线 ‘quadratic’ 、'cubic' 二阶和三阶B样条曲线，更高阶的曲线可以直接使用整数值指定 interp1d比Matlab的interp有些优势，因为返回的是函数，不需要在事先设定需要求解的点，而是在需要使用时调用函数。 import numpy as np import pylab as pl from scipy import interpolate import matplotlib as mpl mpl.rcParams['font.sans-serif'] = ['SimHei'] x = np.linspace(0,10,11) y = np.sin(x) xnew = np.linspace(0,10,101) pl.plot(x,y,'ro') for kind in ['nearest', 'zero', 'slinear', 'quadratic']: f = interpolate.interp1d(x,y,kind=kind) ynew = f(xnew) pl.plot(xnew, ynew,label=str(kind)) pl.legend() UnivariateSpline interp1d不能外推运算（外插值）UnivariateSpline可以外插值 class scipy.interpolate.UnivariateSpline(x, y, w=None, bbox=[None, None], k=3, s=None ) x,y是X-Y坐标数组 w是每个数据点的权重值 k为样条曲线的阶数 s为平滑参数。 s=0，样条曲线强制通过所有数据点 s>0,满足 ∑(w(y−spline(x)))2≤s \\sum(w(y-spline(x)))^2 \\leq s∑(w(y−spline(x)))​2​​≤s s=0强制通过所有数据点的外插值 from scipy import interpolate import numpy as np x1=np.linspace(0,10,20) y1=np.sin(x1) sx1=np.linspace(0,12,100) func1=interpolate.UnivariateSpline(x1,y1,s=0)#强制通过所有点 sy1=func1(sx1) import matplotlib.pyplot as plt plt.plot(x1,y1,'o') plt.plot(sx1,sy1) plt.show() 也就插值到(0,12)，范围再大就不行了，毕竟插值的专长不在于预测 s>0：不强制通过所有点 import numpy as np from scipy import interpolate x2=np.linspace(0,20,200) y2=np.sin(x2)+np.random.normal(loc=0,scale=1,size=len(x2))*0.2 sx2=np.linspace(0,22,2000) func2=interpolate.UnivariateSpline(x2,y2,s=8) sy2=func2(sx2) import matplotlib.pyplot as plt plt.plot(x2,y2,'.') plt.plot(sx2,sy2) plt.show() __call__(x[, nu]) Evaluate spline (or its nu-th derivative) at positions x. antiderivative([n]) Construct a new spline representing the antiderivative of this spline. derivative([n]) Construct a new spline representing the derivative of this spline. derivatives(x) 返回点 x 处样条曲线的导数。 get_coeffs() 返回样条系数。 get_knots() Return positions of (boundary and interior) knots of the spline. get_residual() Return weighted sum of squared residuals of the spline approximation: sum((w[i] * (y[i]-s(x[i])))**2, axis=0). integral(a, b) 返回两个给定点之间样条曲线的定积分。. roots() 返回样条曲线的零点. set_smoothing_factor(s) Continue spline computation with the given smoothing factor s and with the knots found at the last call. print(np.array_str(func2.roots(), precision=3)) # [ 0.046 3.152 6.299 9.349 12.67 15.793 18.813] 参数插值 使用参数插值连接二维平面上的点 x = [ 4.913, 4.913, 4.918, 4.938, 4.955, 4.949, 4.911, 4.848, 4.864, 4.893, 4.935, 4.981, 5.01, 5.021 ] y = [ 5.2785, 5.2875, 5.291, 5.289, 5.28, 5.26, 5.245, 5.245, 5.2615, 5.278, 5.2775, 5.261, 5.245, 5.241 ] pl.plot(x, y, \"o\") for s in (0, 1e-4): tck, t = interpolate.splprep([x, y], s=s) #❶ xi, yi = interpolate.splev(np.linspace(t[0], t[-1], 200), tck) #❷ pl.plot(xi, yi, lw=2, label=u\"s=%g\" % s) pl.legend() scipy.interpolate包里提供了两个函数splev和splrep共同完成(B-样条:贝兹曲线(又称贝塞尔曲线))插值，和之前一元插值一步就能完成不同，样条插值需要两步完成，第一步先用splrep计算出b样条曲线的参数tck，第二步在第一步的基础上用splev计算出各取样点的插值结果。 一元函数的 Rbf 插值 lass scipy.interpolate.Rbf(*args) 参数： args：arrays x, y, z, …, d, where x, y, z, … are the coordinates of the nodes and d is the array of values at the nodes function：str or callable, optional 基于半径r的径向基函数，默认值为欧几里得距离； 默认值为“ multiquadric” 'multiquadric': sqrt((r/self.epsilon)**2 + 1) 'inverse': 1.0/sqrt((r/self.epsilon)**2 + 1) 'gaussian': exp(-(r/self.epsilon)**2) 'linear': r 'cubic': r**3 'quintic': r**5 'thin_plate': r**2 * log(r) 一维RBF插值 from scipy.interpolate import Rbf x1 = np.array([-1, 0, 2.0, 1.0]) y1 = np.array([1.0, 0.3, -0.5, 0.8]) funcs = ['multiquadric', 'gaussian', 'linear'] nx = np.linspace(-3, 4, 100) rbfs = [Rbf(x1, y1, function=fname) for fname in funcs] #❶ rbf_ys = [rbf(nx) for rbf in rbfs] #❷ pl.plot(x1, y1, \"o\") for fname, ny in zip(funcs, rbf_ys): pl.plot(nx, ny, label=fname, lw=2) pl.ylim(-1.0, 1.5) pl.legend() for fname, rbf in zip(funcs, rbfs): print (fname, rbf.nodes) # multiquadric [-0.88822885 2.17654513 1.42877511 -2.67919021] # gaussian [ 1.00321945 -0.02345964 -0.65441716 0.91375159] # linear [-0.26666667 0.6 0.73333333 -0.9 ] Update time： 2020-07-05 "},"Chapter3/插值interpolate/多维插值interp2d.html":{"url":"Chapter3/插值interpolate/多维插值interp2d.html","title":"多维插值interp2d","keywords":"","body":"多维插值interp2d interp2d class scipy.interpolate.interp2d(x, y, z, kind='linear', copy=True, bounds_error=False, fill_value=None ) 参数： x, y, z : array_like If the points lie on a regular grid, x can specify the column coordinates and y the row coordinates, for example: >>> x = [0,1,2]; y = [0,3]; z = [[1,2,3], [4,5,6]] If x and y are multi-dimensional, they are flattened before use. kind： {‘linear’, ‘cubic’, ‘quintic’}, optional # 生成数据 import numpy as np def func(x,y): return (x+y)*np.exp(-5*(x**2+y**2)) x,y=np.mgrid[-1:1:8j,-1:1:8j] z=func(x,y) # 插值 from scipy import interpolate func=interpolate.interp2d(x,y,z,kind='cubic') xnew=np.linspace(-1,1,100) ynew=np.linspace(-1,1,100) znew=func(xnew,ynew)#xnew, ynew是一维的，输出znew是二维的 xnew,ynew=np.mgrid[-1:1:100j,-1:1:100j]#统一变成二维，便于下一步画图 # 画图 import mpl_toolkits.mplot3d import matplotlib.pyplot as plt ax=plt.subplot(111,projection='3d') ax.plot_surface(xnew,ynew,znew) ax.scatter(x,y,z,c='r',marker='^') plt.show() 二维插值 Rbf 二维径向基函数插值 # 随机生成点，并计算函数值 import numpy as np def func(x,y): return (x+y)*np.exp(-5*(x**2+y**2)) x=np.random.uniform(low=-1,high=1,size=100) y=np.random.uniform(low=-1,high=1,size=100) z=func(x,y) # 插值 from scipy import interpolate func=interpolate.Rbf(x,y,z,function='multiquadric') xnew,ynew=np.mgrid[-1:1:100j,-1:1:100j] znew=func(xnew,ynew)#输入输出都是二维 # 画图 import mpl_toolkits.mplot3d import matplotlib.pyplot as plt ax=plt.subplot(111,projection='3d') ax.plot_surface(xnew,ynew,znew) ax.scatter(x,y,z,c='r',marker='^') plt.show() 参考 scipy.interpolate Scipy Tutorial-一元样条插值 scipy.interpolate.Rbf Update time： 2020-07-05 "},"Chapter3/插值interpolate/牛顿插值.html":{"url":"Chapter3/插值interpolate/牛顿插值.html","title":"牛顿插值","keywords":"","body":"牛顿插值 牛顿插值 import matplotlib.pyplot as plt import numpy as np import pylab as mpl mpl.rcParams['font.sans-serif'] = ['SimHei'] mpl.rcParams['axes.unicode_minus'] = False x = [0.4, 0.55, 0.65, 0.80, 0.90, 1.05] y = [0.41075, 0.57815, 0.69675, 0.88811, 1.02652, 1.25382] '''计算五次差商的值''' def five_order_difference_quotient(x, y): # i 记录计算差商的次数，这里计算五次差商 i = 0 quotient = np.zeros(6) while i i: if i == 0: quotient[j] = (y[j] - y[j - 1]) / (x[j] - x[j - 1]) else: quotient[j] = (quotient[j] - quotient[j - 1]) / (x[j] - x[j - 1 - i]) j -= 1 i += 1 # 返回差商值 return quotient def function(data): return x[0] + parameters[1] * (data - 0.4) + parameters[2] * (data - 0.4) * (data - 0.55) + \\ parameters[3] * (data - 0.4) * (data - 0.55) * (data - 0.65) \\ + parameters[4] * (data - 0.4) * (data - 0.55) * (data - 0.80) \"\"\"计算插值多项式的值和相应的误差\"\"\" def calculate_data(x, parameters): return_data = [] for data in x: return_data.append(function(data)) return return_data def draw(newData): plt.scatter(x, y, label=\"离散数据\", color=\"red\") plt.plot(x, newData, label=\"牛顿插值拟合曲线\", color=\"black\") plt.scatter(0.596, function(0.596), label=\"预测函数点\", color=\"blue\") plt.title(\"牛顿插值法\") plt.legend(loc=\"upper left\") plt.show() parameters = five_order_difference_quotient(x, y) yuanzu = calculate_data(x, parameters) draw(yuanzu) Update time： 2020-07-05 "}}